{"cells":[{"cell_type":"markdown","id":"fe63480a","metadata":{"id":"fe63480a"},"source":["# Chapter 15 — TFX: MLOps and deploying models with TensorFlow\n","\n","This notebook reproduces the workflow from **TensorFlow in Action (Thushan Ganegedara)** Chapter 15.  \n","The focus is not only training a model, but organizing the whole “model lifecycle” into a repeatable pipeline.\n","\n","---\n","\n","## Summary\n","\n","In earlier chapters, it was possible to train a model and evaluate it in a single notebook session. In production, that approach breaks down quickly because we also need to manage:\n","\n","- Data ingestion: where the data comes from, how it is split, and whether it keeps the same schema over time  \n","- Data validation: detecting anomalies (unexpected values, missing columns, distribution shifts) before training  \n","- Feature transformation: consistent preprocessing that is applied during both training and serving  \n","- Training and artifact tracking: saving models, metadata, and intermediate artifacts in a way that can be reproduced  \n","- Evaluation and validation: ensuring a newly trained model is “good enough” and does not regress compared to a baseline  \n","- Infrastructure checks + deployment: validating that a model can actually be served, then pushing it to a serving directory\n","\n","This chapter uses **TFX (TensorFlow Extended)** to connect those pieces. TFX is an ecosystem of standard pipeline components (ExampleGen, StatisticsGen, SchemaGen, Transform, Trainer, Evaluator, InfraValidator, Pusher, …) that communicate by passing **artifacts** (data, schemas, models, evaluation results) while tracking everything in **ML Metadata (MLMD)**.\n","\n","The dataset used here is the **Forest Fires dataset** from UCI. The target variable is `area` (burned area), and the goal is to build a **regression model** and then walk through a simplified end-to-end MLOps pipeline.\n","\n","---\n","\n","## Notes about running in Colab\n","\n","- TFX is a large dependency. After installing it, Colab often requires a runtime restart.\n","- Some parts of deployment (especially Docker-based TensorFlow Serving) depend on whether your environment supports Docker.  \n","  If Docker commands fail, the serving parts can be run on a local machine instead."]},{"cell_type":"code","execution_count":1,"id":"f41bfee5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f41bfee5","executionInfo":{"status":"ok","timestamp":1767984119319,"user_tz":-420,"elapsed":1387,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"24d70d91-d791-44f2-899a-d94a8a59c848"},"outputs":[{"output_type":"stream","name":"stdout","text":["  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["# (Colab) Install TFX\n","# After install, restart the runtime if you see import errors.\n","!pip -q install -U tfx"]},{"cell_type":"code","execution_count":9,"id":"19e83ef6","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"id":"19e83ef6","executionInfo":{"status":"error","timestamp":1767984139003,"user_tz":-420,"elapsed":5,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"cbae3392-086a-45ed-d2fb-226922759c5b"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tfx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3421942428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TensorFlow version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TFX version       :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tfx'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import json\n","import base64\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from absl import logging\n","logging.set_verbosity(logging.INFO)\n","\n","from tfx import v1 as tfx\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"TFX version       :\", tfx.__version__)"]},{"cell_type":"markdown","id":"54c6dcd1","metadata":{"id":"54c6dcd1"},"source":["## 1) Download and inspect the dataset\n","\n","The Forest Fires dataset is a small tabular dataset.  \n","It has both continuous features (e.g., `temp`, `wind`, `rain`) and categorical-like features (`month`, `day`).\n","\n","We will:\n","1. download the CSV\n","2. load into a pandas DataFrame\n","3. check shape, columns, and a few sample rows"]},{"cell_type":"code","execution_count":null,"id":"0af7525f","metadata":{"id":"0af7525f","executionInfo":{"status":"aborted","timestamp":1767984127739,"user_tz":-420,"elapsed":9894,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["import requests\n","\n","DATA_DIR = pathlib.Path(\"data\")\n","RAW_DIR = DATA_DIR / \"raw\"\n","CSV_DIR = DATA_DIR / \"csv\"\n","TRAIN_DIR = CSV_DIR / \"train\"\n","TEST_DIR = CSV_DIR / \"test\"\n","\n","for d in [RAW_DIR, TRAIN_DIR, TEST_DIR]:\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","csv_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\"\n","names_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.names\"\n","\n","csv_path = RAW_DIR / \"forestfires.csv\"\n","names_path = RAW_DIR / \"forestfires.names\"\n","\n","if not csv_path.exists():\n","    r = requests.get(csv_url, timeout=60)\n","    r.raise_for_status()\n","    csv_path.write_bytes(r.content)\n","\n","if not names_path.exists():\n","    r = requests.get(names_url, timeout=60)\n","    r.raise_for_status()\n","    names_path.write_bytes(r.content)\n","\n","print(\"Saved:\", csv_path)\n","print(\"Saved:\", names_path)"]},{"cell_type":"code","execution_count":null,"id":"b1c6b630","metadata":{"id":"b1c6b630","executionInfo":{"status":"aborted","timestamp":1767984127770,"user_tz":-420,"elapsed":9922,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["df = pd.read_csv(csv_path)\n","display(df.head())\n","print(\"\\nShape:\", df.shape)\n","print(\"Columns:\", list(df.columns))"]},{"cell_type":"markdown","id":"8b39ad15","metadata":{"id":"8b39ad15"},"source":["### Quick checks\n","\n","Before building any pipeline, it is useful to do small sanity checks:\n","\n","- Are there missing values?\n","- What is the target distribution?\n","- Are the categorical columns formatted consistently?"]},{"cell_type":"code","execution_count":null,"id":"12f42adc","metadata":{"id":"12f42adc","executionInfo":{"status":"aborted","timestamp":1767984127776,"user_tz":-420,"elapsed":9926,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["print(\"Missing values per column:\")\n","display(df.isna().sum())\n","\n","print(\"\\nTarget (area) summary:\")\n","display(df[\"area\"].describe())\n","\n","print(\"\\nUnique values (month):\", sorted(df[\"month\"].unique()))\n","print(\"Unique values (day)  :\", sorted(df[\"day\"].unique()))"]},{"cell_type":"markdown","id":"d634061b","metadata":{"id":"d634061b"},"source":["## 2) Create a train/test split and store as CSV\n","\n","The chapter prepares separate CSV files.  \n","This is mainly to make the pipeline input explicit (a folder with CSV files).\n","\n","Later, `CsvExampleGen` will create the train/eval splits that the Trainer uses internally."]},{"cell_type":"code","execution_count":null,"id":"e357d272","metadata":{"id":"e357d272","executionInfo":{"status":"aborted","timestamp":1767984127777,"user_tz":-420,"elapsed":9926,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# 95% train, 5% test\n","train_df = df.sample(frac=0.95, random_state=42)\n","test_df = df.drop(train_df.index)\n","\n","train_csv_path = TRAIN_DIR / \"forestfires.csv\"\n","test_csv_path = TEST_DIR / \"forestfires.csv\"\n","\n","train_df.to_csv(train_csv_path, index=False)\n","test_df.to_csv(test_csv_path, index=False)\n","\n","print(\"Train rows:\", len(train_df), \"->\", train_csv_path)\n","print(\"Test  rows:\", len(test_df),  \"->\", test_csv_path)"]},{"cell_type":"markdown","id":"85b9035a","metadata":{"id":"85b9035a"},"source":["## 3) Create an Interactive TFX context\n","\n","For learning and experimentation, **InteractiveContext** is convenient because we can run components step-by-step.\n","\n","Two key paths:\n","\n","- PIPELINE_ROOT: where artifacts are stored\n","- METADATA_PATH: a local ML Metadata (MLMD) SQLite database"]},{"cell_type":"code","execution_count":null,"id":"0435d0ed","metadata":{"id":"0435d0ed","executionInfo":{"status":"aborted","timestamp":1767984127778,"user_tz":-420,"elapsed":9925,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["PIPELINE_NAME = \"forest-fires-tfx\"\n","PIPELINE_ROOT = os.path.join(\"pipelines\", PIPELINE_NAME)\n","METADATA_PATH = os.path.join(\"metadata\", PIPELINE_NAME, \"metadata.db\")\n","SERVING_MODEL_DIR = os.path.join(\"serving_model\", PIPELINE_NAME)\n","\n","os.makedirs(PIPELINE_ROOT, exist_ok=True)\n","os.makedirs(os.path.dirname(METADATA_PATH), exist_ok=True)\n","os.makedirs(SERVING_MODEL_DIR, exist_ok=True)\n","\n","context = tfx.orchestration.experimental.interactive.InteractiveContext(\n","    pipeline_root=PIPELINE_ROOT,\n","    metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(METADATA_PATH),\n",")\n","\n","print(\"PIPELINE_ROOT     :\", PIPELINE_ROOT)\n","print(\"METADATA_PATH     :\", METADATA_PATH)\n","print(\"SERVING_MODEL_DIR :\", SERVING_MODEL_DIR)"]},{"cell_type":"markdown","id":"57ad8603","metadata":{"id":"57ad8603"},"source":["## 4) ExampleGen — ingest CSV and create Examples\n","\n","`CsvExampleGen` reads input data and outputs an Examples artifact in TFRecord format.\n","\n","Downstream components mainly work with Examples artifacts, not raw CSV files."]},{"cell_type":"code","execution_count":null,"id":"ba35b923","metadata":{"id":"ba35b923","executionInfo":{"status":"aborted","timestamp":1767984127779,"user_tz":-420,"elapsed":9925,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["example_gen = tfx.components.CsvExampleGen(input_base=str(TRAIN_DIR))\n","context.run(example_gen)"]},{"cell_type":"code","execution_count":null,"id":"bc9664d5","metadata":{"id":"bc9664d5","executionInfo":{"status":"aborted","timestamp":1767984127812,"user_tz":-420,"elapsed":9956,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(example_gen.outputs[\"examples\"])"]},{"cell_type":"markdown","id":"31e8034a","metadata":{"id":"31e8034a"},"source":["## 5) StatisticsGen — compute dataset statistics\n","\n","This step calculates summary statistics (counts, mean/std, histograms, etc.).\n","These statistics can be visualized and used for anomaly detection."]},{"cell_type":"code","execution_count":null,"id":"b9954523","metadata":{"id":"b9954523","executionInfo":{"status":"aborted","timestamp":1767984127820,"user_tz":-420,"elapsed":9963,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs[\"examples\"])\n","context.run(statistics_gen)"]},{"cell_type":"code","execution_count":null,"id":"6f0ec9a7","metadata":{"id":"6f0ec9a7","executionInfo":{"status":"aborted","timestamp":1767984127821,"user_tz":-420,"elapsed":9963,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(statistics_gen.outputs[\"statistics\"])"]},{"cell_type":"markdown","id":"d012d734","metadata":{"id":"d012d734"},"source":["## 6) SchemaGen — infer a data schema\n","\n","SchemaGen uses the computed statistics to infer feature types and constraints.\n","The schema becomes a contract between training data and future incoming data."]},{"cell_type":"code","execution_count":null,"id":"f483525f","metadata":{"id":"f483525f","executionInfo":{"status":"aborted","timestamp":1767984127822,"user_tz":-420,"elapsed":9962,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["schema_gen = tfx.components.SchemaGen(\n","    statistics=statistics_gen.outputs[\"statistics\"],\n","    infer_feature_shape=False,\n",")\n","context.run(schema_gen)"]},{"cell_type":"code","execution_count":null,"id":"48805050","metadata":{"id":"48805050","executionInfo":{"status":"aborted","timestamp":1767984127822,"user_tz":-420,"elapsed":9961,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(schema_gen.outputs[\"schema\"])"]},{"cell_type":"markdown","id":"e35d3a06","metadata":{"id":"e35d3a06"},"source":["## 7) ExampleValidator — detect anomalies against the schema\n","\n","ExampleValidator compares dataset statistics against the schema and reports anomalies."]},{"cell_type":"code","execution_count":null,"id":"460ac421","metadata":{"id":"460ac421","executionInfo":{"status":"aborted","timestamp":1767984127823,"user_tz":-420,"elapsed":9961,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["example_validator = tfx.components.ExampleValidator(\n","    statistics=statistics_gen.outputs[\"statistics\"],\n","    schema=schema_gen.outputs[\"schema\"],\n",")\n","context.run(example_validator)"]},{"cell_type":"code","execution_count":null,"id":"10d826da","metadata":{"id":"10d826da","executionInfo":{"status":"aborted","timestamp":1767984127824,"user_tz":-420,"elapsed":9960,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(example_validator.outputs[\"anomalies\"])"]},{"cell_type":"markdown","id":"09e989e3","metadata":{"id":"09e989e3"},"source":["## 8) Transform — define feature engineering with tf.Transform\n","\n","Transform ensures preprocessing is applied consistently in training and serving.\n","\n","We will create:\n","- `forest_fires_constants.py`\n","- `forest_fires_transform.py` containing `preprocessing_fn(inputs)`"]},{"cell_type":"code","execution_count":null,"id":"83929216","metadata":{"id":"83929216","executionInfo":{"status":"aborted","timestamp":1767984127824,"user_tz":-420,"elapsed":9959,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from pathlib import Path\n","\n","constants_code = \"\"\"\n","VOCAB_FEATURE_KEYS = [\"day\", \"month\"]\n","MAX_CATEGORICAL_FEATURE_VALUES = [7, 12]\n","\n","DENSE_FLOAT_FEATURE_KEYS = [\"DC\", \"DMC\", \"FFMC\", \"ISI\", \"rain\", \"temp\", \"wind\", \"X\", \"Y\"]\n","\n","BUCKET_FEATURE_KEYS = [\"RH\"]\n","BUCKET_FEATURE_BOUNDARIES = [[33.0, 66.0]]\n","\n","LABEL_KEY = \"area\"\n","\n","def transformed_name(key: str) -> str:\n","    return key + \"_xf\"\n","\"\"\".strip() + \"\\n\"\n","\n","Path(\"forest_fires_constants.py\").write_text(constants_code)\n","print(\"Wrote forest_fires_constants.py\")"]},{"cell_type":"code","execution_count":null,"id":"c3116f9d","metadata":{"id":"c3116f9d","executionInfo":{"status":"aborted","timestamp":1767984127825,"user_tz":-420,"elapsed":9959,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["transform_code = \"\"\"\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","import forest_fires_constants as const\n","\n","_DENSE_FLOAT_FEATURE_KEYS = const.DENSE_FLOAT_FEATURE_KEYS\n","_VOCAB_FEATURE_KEYS = const.VOCAB_FEATURE_KEYS\n","_BUCKET_FEATURE_KEYS = const.BUCKET_FEATURE_KEYS\n","_BUCKET_FEATURE_BOUNDARIES = const.BUCKET_FEATURE_BOUNDARIES\n","_LABEL_KEY = const.LABEL_KEY\n","\n","def _transformed_name(key: str) -> str:\n","    return const.transformed_name(key)\n","\n","def _sparse_to_dense(x):\n","    if isinstance(x, tf.SparseTensor):\n","        x = tf.sparse.to_dense(x)\n","    return tf.squeeze(x, axis=1)\n","\n","def preprocessing_fn(inputs):\n","    outputs = {}\n","\n","    for key in _DENSE_FLOAT_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.scale_to_z_score(_sparse_to_dense(inputs[key]))\n","\n","    for key in _VOCAB_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n","            _sparse_to_dense(inputs[key]),\n","            num_oov_buckets=1,\n","            vocab_filename=key,\n","        )\n","\n","    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n","        outputs[_transformed_name(key)] = tft.bucketize(_sparse_to_dense(inputs[key]), boundaries=boundaries)\n","\n","    outputs[_transformed_name(_LABEL_KEY)] = tf.cast(_sparse_to_dense(inputs[_LABEL_KEY]), tf.float32)\n","\n","    return outputs\n","\"\"\".strip() + \"\\n\"\n","\n","Path(\"forest_fires_transform.py\").write_text(transform_code)\n","print(\"Wrote forest_fires_transform.py\")"]},{"cell_type":"code","execution_count":null,"id":"5b83223c","metadata":{"id":"5b83223c","executionInfo":{"status":"aborted","timestamp":1767984127825,"user_tz":-420,"elapsed":9958,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["transform = tfx.components.Transform(\n","    examples=example_gen.outputs[\"examples\"],\n","    schema=schema_gen.outputs[\"schema\"],\n","    module_file=os.path.abspath(\"forest_fires_transform.py\"),\n",")\n","context.run(transform)"]},{"cell_type":"code","execution_count":null,"id":"586d5c40","metadata":{"id":"586d5c40","executionInfo":{"status":"aborted","timestamp":1767984127826,"user_tz":-420,"elapsed":9958,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(transform.outputs[\"transform_graph\"])\n","context.show(transform.outputs[\"transformed_examples\"])"]},{"cell_type":"markdown","id":"e6005afd","metadata":{"id":"e6005afd"},"source":["## 9) Trainer — train a regression model using transformed features\n","\n","The Trainer component runs model-building code from a module file."]},{"cell_type":"code","execution_count":null,"id":"cad59678","metadata":{"id":"cad59678","executionInfo":{"status":"aborted","timestamp":1767984127826,"user_tz":-420,"elapsed":9956,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["trainer_code = \"\"\"\n","import os\n","from typing import List, Text\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","from tfx_bsl.public import tfxio\n","from absl import logging\n","\n","import forest_fires_constants as const\n","\n","_DENSE_FLOAT_FEATURE_KEYS = const.DENSE_FLOAT_FEATURE_KEYS\n","_VOCAB_FEATURE_KEYS = const.VOCAB_FEATURE_KEYS\n","_MAX_CATEGORICAL_FEATURE_VALUES = const.MAX_CATEGORICAL_FEATURE_VALUES\n","_BUCKET_FEATURE_KEYS = const.BUCKET_FEATURE_KEYS\n","_BUCKET_FEATURE_BOUNDARIES = const.BUCKET_FEATURE_BOUNDARIES\n","_LABEL_KEY = const.LABEL_KEY\n","\n","def _transformed_name(key: str) -> str:\n","    return const.transformed_name(key)\n","\n","def _transformed_names(keys):\n","    return [_transformed_name(k) for k in keys]\n","\n","def _input_fn(file_pattern: List[Text], data_accessor, tf_transform_output: tft.TFTransformOutput, batch_size: int = 64):\n","    dataset = data_accessor.tf_dataset_factory(\n","        file_pattern,\n","        tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n","        tf_transform_output.transformed_metadata.schema,\n","    )\n","\n","    def _cast(features, label):\n","        for k in list(features.keys()):\n","            if features[k].dtype == tf.int64:\n","                features[k] = tf.cast(features[k], tf.int32)\n","        return features, label\n","\n","    return dataset.map(_cast)\n","\n","def _build_keras_model():\n","    numeric_cols = [tf.feature_column.numeric_column(k) for k in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)]\n","\n","    bucket_cols = []\n","    for key, boundaries in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n","        num_buckets = len(boundaries) + 1\n","        cat_col = tf.feature_column.categorical_column_with_identity(_transformed_name(key), num_buckets=num_buckets)\n","        bucket_cols.append(tf.feature_column.indicator_column(cat_col))\n","\n","    vocab_cols = []\n","    for key, max_val in zip(_VOCAB_FEATURE_KEYS, _MAX_CATEGORICAL_FEATURE_VALUES):\n","        cat_col = tf.feature_column.categorical_column_with_identity(_transformed_name(key), num_buckets=max_val + 1)\n","        vocab_cols.append(tf.feature_column.indicator_column(cat_col))\n","\n","    feature_columns = numeric_cols + bucket_cols + vocab_cols\n","\n","    inputs = {}\n","    for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS):\n","        inputs[key] = tf.keras.Input(shape=(1,), name=key, dtype=tf.float32)\n","    for key in _transformed_names(_VOCAB_FEATURE_KEYS + _BUCKET_FEATURE_KEYS):\n","        inputs[key] = tf.keras.Input(shape=(1,), name=key, dtype=tf.int32)\n","\n","    x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)\n","    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n","    outputs = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","        loss=\"mse\",\n","        metrics=[tf.keras.metrics.MeanSquaredError(name=\"mse\")],\n","    )\n","    return model\n","\n","def _get_serve_tf_examples_fn(model, tf_transform_output):\n","    transform_layer = tf_transform_output.transform_features_layer()\n","\n","    raw_feature_spec = tf_transform_output.raw_feature_spec()\n","    raw_feature_spec.pop(_LABEL_KEY)\n","\n","    @tf.function\n","    def serve_tf_examples_fn(serialized_tf_examples):\n","        parsed_features = tf.io.parse_example(serialized_tf_examples, raw_feature_spec)\n","        transformed_features = transform_layer(parsed_features)\n","\n","        model_inputs = {}\n","        for k, v in transformed_features.items():\n","            if k.endswith(\"_xf\") and k != _transformed_name(_LABEL_KEY):\n","                if v.dtype == tf.int64:\n","                    v = tf.cast(v, tf.int32)\n","                model_inputs[k] = tf.expand_dims(v, -1) if len(v.shape) == 1 else v\n","\n","        return {\"predictions\": model(model_inputs)}\n","\n","    return serve_tf_examples_fn\n","\n","def run_fn(fn_args):\n","    logging.info(\"Trainer run_fn started\")\n","\n","    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n","    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, tf_transform_output, batch_size=64)\n","    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, tf_transform_output, batch_size=64)\n","\n","    model = _build_keras_model()\n","    model.summary(print_fn=logging.info)\n","\n","    log_dir = os.path.join(fn_args.model_run_dir, \"logs\")\n","    os.makedirs(log_dir, exist_ok=True)\n","    callbacks = [\n","        tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n","        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n","    ]\n","\n","    model.fit(\n","        train_dataset,\n","        steps_per_epoch=fn_args.train_steps,\n","        validation_data=eval_dataset,\n","        validation_steps=fn_args.eval_steps,\n","        epochs=20,\n","        callbacks=callbacks,\n","        verbose=1,\n","    )\n","\n","    serving_fn = _get_serve_tf_examples_fn(model, tf_transform_output)\n","    signatures = {\n","        \"serving_default\": serving_fn.get_concrete_function(\n","            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n","        )\n","    }\n","\n","    model.save(fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures)\n","    logging.info(\"Saved model to %s\", fn_args.serving_model_dir)\n","\"\"\".strip() + \"\\n\"\n","\n","Path(\"forest_fires_trainer.py\").write_text(trainer_code)\n","print(\"Wrote forest_fires_trainer.py\")"]},{"cell_type":"markdown","id":"526384f8","metadata":{"id":"526384f8"},"source":["### Train/Eval steps\n","\n","TFX uses train/eval splits internally.  \n","Here we set explicit step counts using a simple approximation."]},{"cell_type":"code","execution_count":null,"id":"cb20519c","metadata":{"id":"cb20519c","executionInfo":{"status":"aborted","timestamp":1767984127827,"user_tz":-420,"elapsed":9956,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from tfx.proto import trainer_pb2\n","\n","BATCH_SIZE = 64\n","n_rows = len(train_df)\n","\n","train_steps = max(1, int((2.0/3.0) * n_rows / BATCH_SIZE))\n","eval_steps  = max(1, int((1.0/3.0) * n_rows / BATCH_SIZE))\n","\n","print(\"Rows (train CSV):\", n_rows)\n","print(\"Train steps     :\", train_steps)\n","print(\"Eval steps      :\", eval_steps)"]},{"cell_type":"code","execution_count":null,"id":"07d90d3f","metadata":{"id":"07d90d3f","executionInfo":{"status":"aborted","timestamp":1767984127828,"user_tz":-420,"elapsed":9956,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["trainer = tfx.components.Trainer(\n","    module_file=os.path.abspath(\"forest_fires_trainer.py\"),\n","    examples=transform.outputs[\"transformed_examples\"],\n","    transform_graph=transform.outputs[\"transform_graph\"],\n","    schema=schema_gen.outputs[\"schema\"],\n","    train_args=trainer_pb2.TrainArgs(num_steps=train_steps),\n","    eval_args=trainer_pb2.EvalArgs(num_steps=eval_steps),\n",")\n","context.run(trainer)"]},{"cell_type":"code","execution_count":null,"id":"41dc023a","metadata":{"id":"41dc023a","executionInfo":{"status":"aborted","timestamp":1767984127828,"user_tz":-420,"elapsed":9955,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(trainer.outputs[\"model\"])"]},{"cell_type":"markdown","id":"62cbc5a2","metadata":{"id":"62cbc5a2"},"source":["## 10) Resolver + Evaluator — compare candidate model against a baseline\n","\n","Evaluator uses TFMA to compute metrics and (optionally) validate the model with thresholds."]},{"cell_type":"code","execution_count":null,"id":"2428d088","metadata":{"id":"2428d088","executionInfo":{"status":"aborted","timestamp":1767984127829,"user_tz":-420,"elapsed":9955,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["import tensorflow_model_analysis as tfma\n","from google.protobuf import text_format\n","\n","eval_config = text_format.Parse(\n","    r\"\"\"\n","    model_specs { label_key: \"area\" }\n","    slicing_specs {}\n","    slicing_specs { feature_keys: [\"month\"] }\n","    metrics_specs {\n","      metrics { class_name: \"ExampleCount\" }\n","      metrics {\n","        class_name: \"MeanSquaredError\"\n","        threshold {\n","          value_threshold { upper_bound { value: 300.0 } }\n","          change_threshold { direction: LOWER_IS_BETTER absolute { value: 0.0 } }\n","        }\n","      }\n","    }\n","    \"\"\",\n","    tfma.EvalConfig(),\n",")\n","\n","eval_config"]},{"cell_type":"code","execution_count":null,"id":"dd2c7878","metadata":{"id":"dd2c7878","executionInfo":{"status":"aborted","timestamp":1767984127837,"user_tz":-420,"elapsed":9962,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["model_resolver = tfx.dsl.Resolver(\n","    strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n","    model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n","    model_blessing=tfx.dsl.Channel(type=tfx.types.standard_artifacts.ModelBlessing),\n",").with_id(\"latest_blessed_model_resolver\")\n","\n","context.run(model_resolver)"]},{"cell_type":"code","execution_count":null,"id":"6a1c7fa5","metadata":{"id":"6a1c7fa5","executionInfo":{"status":"aborted","timestamp":1767984127838,"user_tz":-420,"elapsed":9962,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["evaluator = tfx.components.Evaluator(\n","    examples=example_gen.outputs[\"examples\"],\n","    model=trainer.outputs[\"model\"],\n","    baseline_model=model_resolver.outputs[\"model\"],\n","    eval_config=eval_config,\n",")\n","\n","context.run(evaluator)"]},{"cell_type":"code","execution_count":null,"id":"6e84b061","metadata":{"id":"6e84b061","executionInfo":{"status":"aborted","timestamp":1767984127838,"user_tz":-420,"elapsed":9961,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(evaluator.outputs[\"evaluation\"])\n","context.show(evaluator.outputs.get(\"blessing\"))"]},{"cell_type":"code","execution_count":null,"id":"83d7b38d","metadata":{"id":"83d7b38d","executionInfo":{"status":"aborted","timestamp":1767984127840,"user_tz":-420,"elapsed":9961,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["eval_result = tfma.load_eval_result(evaluator.outputs[\"evaluation\"].get()[0].uri)\n","tfma.view.render_slicing_metrics(eval_result)"]},{"cell_type":"markdown","id":"bec1d4a9","metadata":{"id":"bec1d4a9"},"source":["## 11) InfraValidator — check serving infrastructure (Docker-based TF Serving)\n","\n","InfraValidator launches a serving environment and validates that the model can be loaded and queried.\n","This step depends on Docker availability."]},{"cell_type":"code","execution_count":null,"id":"2862b134","metadata":{"id":"2862b134","executionInfo":{"status":"aborted","timestamp":1767984127840,"user_tz":-420,"elapsed":9960,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from tfx.proto import infra_validator_pb2\n","\n","infra_validator = tfx.components.InfraValidator(\n","    model=trainer.outputs[\"model\"],\n","    examples=example_gen.outputs[\"examples\"],\n","    serving_spec=infra_validator_pb2.ServingSpec(\n","        tensorflow_serving=infra_validator_pb2.TensorFlowServing(tags=[\"latest\"]),\n","        local_docker=infra_validator_pb2.LocalDockerConfig(),\n","    ),\n","    validation_spec=infra_validator_pb2.ValidationSpec(\n","        max_loading_time_seconds=60,\n","        num_tries=3,\n","    ),\n","    request_spec=infra_validator_pb2.RequestSpec(\n","        tensorflow_serving=infra_validator_pb2.TensorFlowServingRequestSpec(),\n","        num_examples=1,\n","    ),\n",")\n","\n","context.run(infra_validator)"]},{"cell_type":"code","execution_count":null,"id":"97217ccd","metadata":{"id":"97217ccd","executionInfo":{"status":"aborted","timestamp":1767984127841,"user_tz":-420,"elapsed":9960,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(infra_validator.outputs[\"blessing\"])"]},{"cell_type":"markdown","id":"dfe32b5d","metadata":{"id":"dfe32b5d"},"source":["## 12) Pusher — push the validated model to a serving directory\n","\n","Pusher copies the model to a deployment destination only if it is blessed."]},{"cell_type":"code","execution_count":null,"id":"ea2e3cfa","metadata":{"id":"ea2e3cfa","executionInfo":{"status":"aborted","timestamp":1767984127841,"user_tz":-420,"elapsed":9959,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from tfx.proto import pusher_pb2\n","\n","pusher = tfx.components.Pusher(\n","    model=trainer.outputs[\"model\"],\n","    model_blessing=evaluator.outputs.get(\"blessing\"),\n","    infra_blessing=infra_validator.outputs.get(\"blessing\"),\n","    push_destination=pusher_pb2.PushDestination(\n","        filesystem=pusher_pb2.PushDestination.Filesystem(base_directory=SERVING_MODEL_DIR)\n","    ),\n",")\n","\n","context.run(pusher)"]},{"cell_type":"code","execution_count":null,"id":"df0d7de8","metadata":{"id":"df0d7de8","executionInfo":{"status":"aborted","timestamp":1767984127842,"user_tz":-420,"elapsed":9959,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["context.show(pusher.outputs[\"pushed_model\"])\n","print(\"Serving model directory:\", SERVING_MODEL_DIR)"]},{"cell_type":"markdown","id":"301b5e00","metadata":{"id":"301b5e00"},"source":["## 13) Serving the pushed model with TensorFlow Serving (Docker)\n","\n","A typical TensorFlow Serving command (run where Docker is available):\n","\n","```bash\n","docker run -p 8501:8501 \\\n","  --mount type=bind,source=$(pwd)/serving_model/forest-fires-tfx,target=/models/forest-fires-tfx \\\n","  -e MODEL_NAME=forest-fires-tfx \\\n","  tensorflow/serving:latest\n","```\n","\n","The model exported by this pipeline accepts serialized `tf.Example` records.  \n","The next cells build a sample `tf.Example` request and show the REST payload format."]},{"cell_type":"code","execution_count":null,"id":"c63ddc2b","metadata":{"id":"c63ddc2b","executionInfo":{"status":"aborted","timestamp":1767984127843,"user_tz":-420,"elapsed":9958,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def _bytes_feature(v: str):\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[v.encode(\"utf-8\")]))\n","\n","def _float_feature(v: float):\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v)]))\n","\n","sample = test_df.iloc[0].to_dict()\n","\n","features = {}\n","for k, v in sample.items():\n","    if k in [\"month\", \"day\"]:\n","        features[k] = _bytes_feature(str(v))\n","    else:\n","        features[k] = _float_feature(float(v))\n","\n","example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n","serialized = example_proto.SerializeToString()\n","\n","b64 = base64.b64encode(serialized).decode(\"utf-8\")\n","print(\"Base64 example length:\", len(b64))"]},{"cell_type":"code","execution_count":null,"id":"c2c471a4","metadata":{"id":"c2c471a4","executionInfo":{"status":"aborted","timestamp":1767984127843,"user_tz":-420,"elapsed":9957,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["import requests\n","\n","TF_SERVING_URL = \"http://localhost:8501/v1/models/forest-fires-tfx:predict\"\n","\n","payload = {\n","    \"signature_name\": \"serving_default\",\n","    \"instances\": [{\"b64\": b64}],\n","}\n","\n","print(\"POST ->\", TF_SERVING_URL)\n","print(\"Payload keys:\", list(payload.keys()))\n","\n","# Uncomment if TF Serving is running\n","# r = requests.post(TF_SERVING_URL, json=payload, timeout=10)\n","# print(\"Status:\", r.status_code)\n","# print(\"Response:\", r.text[:500])"]},{"cell_type":"markdown","id":"e8bb4147","metadata":{"id":"e8bb4147"},"source":["## Closing notes\n","\n","This chapter shifts from “training in isolation” to a pipeline mindset:\n","\n","- data checks and schema become part of the workflow\n","- preprocessing is exported and reused consistently\n","- evaluation can compare models over time using ML Metadata\n","- deployment is conditional on passing validations"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}