{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e62928",
   "metadata": {},
   "source": [
    "# Chapter 12 — Sequence-to-sequence learning: Part 2 (Attention)\n",
    "\n",
    "Chapter 11 built an encoder–decoder translator that predicts German tokens from an English sentence.  \n",
    "The key limitation of the plain encoder–decoder setup is that the decoder often relies too heavily on a *single* compressed representation of the source sentence. When sentences get longer or contain multiple clauses, that compression becomes a bottleneck.\n",
    "\n",
    "This chapter introduces **Bahdanau attention** (additive attention). Instead of treating the encoder as a one-shot summary, attention lets the decoder access *all* encoder time steps and select what is most relevant while generating each output token.\n",
    "\n",
    "In this notebook, I reproduce the chapter workflow using the same English → German dataset and the same preprocessing steps as Chapter 11:\n",
    "- download and prepare the dataset,\n",
    "- build tokenizers and aligned input/target sequences with teacher forcing,\n",
    "- implement Bahdanau attention with the Keras subclassing API,\n",
    "- integrate attention into the encoder–decoder model,\n",
    "- train and evaluate using accuracy and BLEU score,\n",
    "- and finally inspect the model by **visualizing attention weights** (which words the model “looked at” while producing each German token).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba11987",
   "metadata": {},
   "source": [
    "## 1) Summary\n",
    "\n",
    "### 1.1 Why attention improves seq2seq translation\n",
    "In a plain encoder–decoder model, the encoder produces a fixed-size vector, and the decoder conditions on that vector to generate the translation.  \n",
    "This works for short sentences, but for longer inputs the fixed vector can lose information.\n",
    "\n",
    "Attention changes the information flow:\n",
    "- the encoder produces a sequence of hidden states \\(h_1, h_2, ..., h_T\\),\n",
    "- at each decoder step \\(t\\), the model computes attention weights \\(\\alpha_{t, i}\\) over encoder states,\n",
    "- the decoder receives a context vector \\(c_t = \\sum_i \\alpha_{t,i} h_i\\), which is a weighted summary of the source sentence that is specific to the current output step.\n",
    "\n",
    "This helps the model focus on relevant source words during generation.\n",
    "\n",
    "### 1.2 Bahdanau attention (additive attention) in one place\n",
    "Bahdanau attention uses a small feed-forward network to compute alignment scores (“energies”):\n",
    "\n",
    "\\[\n",
    "e_{t,i} = v^\\top \\tanh(W h_i + U s_t)\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(h_i\\) is encoder output at position \\(i\\),\n",
    "- \\(s_t\\) is decoder hidden state at time \\(t\\),\n",
    "- \\(W, U, v\\) are learned parameters.\n",
    "\n",
    "Then attention weights are:\n",
    "\n",
    "\\[\n",
    "\\alpha_{t,i} = \\text{softmax}(e_{t,i})\n",
    "\\]\n",
    "\n",
    "and the context vector is:\n",
    "\n",
    "\\[\n",
    "c_t = \\sum_i \\alpha_{t,i} h_i\n",
    "\\]\n",
    "\n",
    "In this notebook, I implement this mechanism as a reusable Keras layer.\n",
    "\n",
    "### 1.3 What I evaluate (and why BLEU matters)\n",
    "Token-level accuracy is useful for debugging, but it does not fully capture translation quality because:\n",
    "- multiple translations can be valid,\n",
    "- a single early mistake can shift the whole output.\n",
    "\n",
    "BLEU is an n-gram overlap metric that correlates better with translation quality than raw accuracy for this kind of task.  \n",
    "I keep the BLEU implementation from Chapter 11 so results are comparable across models.\n",
    "\n",
    "### 1.4 Attention visualization as a sanity check\n",
    "A major practical benefit of attention is interpretability:  \n",
    "I can visualize the attention matrix (decoder steps × encoder steps) to see whether the model aligns German tokens to relevant English words (e.g., nouns → nouns, verbs → verbs, phrase boundaries).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd969912",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "\n",
    "Imports and reproducibility settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa52bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 4321\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d565e7",
   "metadata": {},
   "source": [
    "## 3) Download and load the English–German sentence pairs\n",
    "\n",
    "This chapter uses the same dataset as Chapter 11:\n",
    "- `http://www.manythings.org/anki/deu-eng.zip`\n",
    "\n",
    "The extracted text file is `deu.txt`, which contains tab-separated lines:\n",
    "`English<TAB>German<TAB>...`\n",
    "\n",
    "I keep only the English and German columns for machine translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7843351",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ZIP_PATH = DATA_DIR / \"deu-eng.zip\"\n",
    "TXT_PATH = DATA_DIR / \"deu.txt\"\n",
    "\n",
    "URL = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "\n",
    "def download_and_extract(url=URL, zip_path=ZIP_PATH, txt_path=TXT_PATH):\n",
    "    if not zip_path.exists():\n",
    "        print(\"Downloading:\", url)\n",
    "        tf.keras.utils.get_file(\n",
    "            fname=str(zip_path),\n",
    "            origin=url,\n",
    "            cache_dir=\".\",\n",
    "            cache_subdir=\"\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"Zip already exists:\", zip_path)\n",
    "\n",
    "    if not txt_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extractall(DATA_DIR)\n",
    "        print(\"Extracted to:\", DATA_DIR)\n",
    "    else:\n",
    "        print(\"Text file already exists:\", txt_path)\n",
    "\n",
    "download_and_extract()\n",
    "\n",
    "# Load to DataFrame\n",
    "df = pd.read_csv(\n",
    "    TXT_PATH,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"EN\", \"DE\", \"META\"],\n",
    "    usecols=[0, 1, 2],\n",
    ")\n",
    "\n",
    "print(\"Total pairs:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd04e8",
   "metadata": {},
   "source": [
    "### 3.1 Basic cleaning and sampling\n",
    "\n",
    "To keep runtime manageable in Colab, I sample a fixed number of sentence pairs.\n",
    "I also add **start** and **end** tokens to German sequences because:\n",
    "- the decoder needs an explicit start signal (`sos`),\n",
    "- and we need a clean stopping criterion (`eos`) during inference.\n",
    "\n",
    "I keep the same tokens as Chapter 11: `sos` and `eos`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8897de",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 50000\n",
    "df_sample = df.sample(n=min(N_SAMPLES, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "start_token = \"sos\"\n",
    "end_token = \"eos\"\n",
    "df_sample[\"DE\"] = start_token + \" \" + df_sample[\"DE\"].astype(str) + \" \" + end_token\n",
    "\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a15795",
   "metadata": {},
   "source": [
    "## 4) Train/validation/test split and sequence length inspection\n",
    "\n",
    "I split 80/10/10 so validation and test are both large enough to be informative.\n",
    "\n",
    "I also inspect sentence lengths to choose a fixed maximum length for vectorization:\n",
    "- this helps avoid extreme outliers dominating memory and compute,\n",
    "- and it keeps encoder/decoder tensors consistent for training.\n",
    "\n",
    "The length choice is a modeling decision: keeping longer sequences captures more information but increases cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/valid/test split (80/10/10)\n",
    "n = len(df_sample)\n",
    "train_end = int(0.8 * n)\n",
    "valid_end = int(0.9 * n)\n",
    "\n",
    "train_df = df_sample.iloc[:train_end].reset_index(drop=True)\n",
    "valid_df = df_sample.iloc[train_end:valid_end].reset_index(drop=True)\n",
    "test_df  = df_sample.iloc[valid_end:].reset_index(drop=True)\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Valid:\", len(valid_df), \"Test:\", len(test_df))\n",
    "\n",
    "def sequence_length_stats(series, name, q_low=0.01, q_high=0.99):\n",
    "    lengths = series.astype(str).apply(lambda s: len(s.split()))\n",
    "    print(\"\\n===\", name, \"===\")\n",
    "    print(\"Min:\", int(lengths.min()), \"Max:\", int(lengths.max()))\n",
    "    print(\"Mean:\", float(lengths.mean()))\n",
    "    print(\"Median:\", float(lengths.median()))\n",
    "    print(lengths.describe())\n",
    "\n",
    "    lo = int(lengths.quantile(q_low))\n",
    "    hi = int(lengths.quantile(q_high))\n",
    "    trimmed = lengths[(lengths >= lo) & (lengths <= hi)]\n",
    "    print(f\"Between {int(q_low*100)}% and {int(q_high*100)}% quantiles (ignore outliers)\")\n",
    "    print(trimmed.describe())\n",
    "    return lengths, lo, hi\n",
    "\n",
    "en_lens, en_lo, en_hi = sequence_length_stats(train_df[\"EN\"], \"English (EN)\")\n",
    "de_lens, de_lo, de_hi = sequence_length_stats(train_df[\"DE\"], \"German (DE) with sos/eos\")\n",
    "\n",
    "# Conservative max lengths based on high quantiles (cap to keep runtime reasonable)\n",
    "en_seq_length = int(min(en_hi, 20))\n",
    "de_seq_length = int(min(de_hi, 22))\n",
    "\n",
    "print(\"\\nChosen EN max length:\", en_seq_length)\n",
    "print(\"Chosen DE max length:\", de_seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c8248",
   "metadata": {},
   "source": [
    "## 5) Vectorizers and shifted decoder sequences (teacher forcing)\n",
    "\n",
    "The decoder is trained with teacher forcing:\n",
    "- input sequence is the German sentence shifted right (starts with `sos`),\n",
    "- label sequence is the German sentence shifted left (ends with `eos`).\n",
    "\n",
    "Example:\n",
    "- original: `sos ich bin müde eos`\n",
    "- decoder input : `sos ich bin müde`\n",
    "- decoder label : `ich bin müde eos`\n",
    "\n",
    "This shift makes the learning problem well-defined: the decoder predicts the next token given previous ground-truth tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65486ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(corpus: np.ndarray, n_vocab: int, max_length=None, name=\"vectorizer\"):\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=n_vocab + 2,   # +2 for '' and [UNK]\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_length,\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "    )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    return tf.keras.models.Model(inp, vectorized_out, name=name), vectorize_layer.get_vocabulary(), vectorize_layer\n",
    "\n",
    "# Vocabulary sizes (can be tuned)\n",
    "en_vocab = 15000\n",
    "de_vocab = 15000\n",
    "\n",
    "en_vectorizer, en_vocabulary, en_vec_layer = get_vectorizer(\n",
    "    corpus=np.array(train_df[\"EN\"].tolist()),\n",
    "    n_vocab=en_vocab,\n",
    "    max_length=en_seq_length,\n",
    "    name=\"en_vectorizer\",\n",
    ")\n",
    "\n",
    "# Decoder inputs/labels are length de_seq_length-1 after shifting\n",
    "de_vectorizer, de_vocabulary, de_vec_layer = get_vectorizer(\n",
    "    corpus=np.array(train_df[\"DE\"].tolist()),\n",
    "    n_vocab=de_vocab,\n",
    "    max_length=de_seq_length - 1,\n",
    "    name=\"de_vectorizer\",\n",
    ")\n",
    "\n",
    "print(\"EN vocab size (with special tokens):\", len(en_vocabulary))\n",
    "print(\"DE vocab size (with special tokens):\", len(de_vocabulary))\n",
    "print(\"Special tokens (start of vocab):\", de_vocabulary[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_target_sequence(s: str):\n",
    "    tokens = s.split()\n",
    "    de_in = \" \".join(tokens[:-1])   # drop last token\n",
    "    de_out = \" \".join(tokens[1:])   # drop first token\n",
    "    return de_in, de_out\n",
    "\n",
    "def prepare_data(train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    data = {}\n",
    "    for name, dfi in [(\"train\", train_df), (\"valid\", valid_df), (\"test\", test_df)]:\n",
    "        en_inputs = dfi[\"EN\"].astype(str).values.reshape(-1, 1)\n",
    "\n",
    "        de_in_list = []\n",
    "        de_out_list = []\n",
    "        for s in dfi[\"DE\"].astype(str).tolist():\n",
    "            de_in, de_out = shift_target_sequence(s)\n",
    "            de_in_list.append(de_in)\n",
    "            de_out_list.append(de_out)\n",
    "\n",
    "        de_inputs = np.array(de_in_list, dtype=object).reshape(-1, 1)\n",
    "        de_labels = np.array(de_out_list, dtype=object).reshape(-1, 1)\n",
    "\n",
    "        data[name] = {\n",
    "            \"encoder_inputs\": en_inputs,\n",
    "            \"decoder_inputs\": de_inputs,\n",
    "            \"decoder_labels\": de_labels,\n",
    "        }\n",
    "    return data\n",
    "\n",
    "data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "{k: {kk: v.shape for kk, v in data_dict[k].items()} for k in data_dict}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8b0ff",
   "metadata": {},
   "source": [
    "## 6) BLEU implementation (same as Chapter 11)\n",
    "\n",
    "To keep evaluation consistent with Chapter 11, I use the same lightweight BLEU implementation:\n",
    "- modified n-gram precision (up to 4-grams),\n",
    "- brevity penalty,\n",
    "- smoothing to avoid zero scores when a higher-order n-gram is missing.\n",
    "\n",
    "This BLEU is not a full official corpus BLEU implementation, but it is sufficient for relative comparisons while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def _modified_precision(reference, candidate, n):\n",
    "    ref_ngrams = Counter(_ngrams(reference, n))\n",
    "    cand_ngrams = Counter(_ngrams(candidate, n))\n",
    "    if len(cand_ngrams) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    clipped = 0\n",
    "    total = 0\n",
    "    for ng, count in cand_ngrams.items():\n",
    "        clipped += min(count, ref_ngrams.get(ng, 0))\n",
    "        total += count\n",
    "    return clipped / total if total > 0 else 0.0\n",
    "\n",
    "def sentence_bleu(reference_tokens, candidate_tokens, max_n=4, smooth=1e-9):\n",
    "    ref_len = len(reference_tokens)\n",
    "    cand_len = len(candidate_tokens)\n",
    "    if cand_len == 0:\n",
    "        return 0.0\n",
    "\n",
    "    bp = 1.0 if cand_len > ref_len else np.exp(1.0 - (ref_len / (cand_len + smooth)))\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        p = _modified_precision(reference_tokens, candidate_tokens, n)\n",
    "        precisions.append(max(p, smooth))\n",
    "\n",
    "    log_p = sum((1.0/max_n) * np.log(p) for p in precisions)\n",
    "    return float(bp * np.exp(log_p))\n",
    "\n",
    "def clean_text(token_tensor, end_token=\"eos\"):\n",
    "    # token_tensor: (batch, time) int\n",
    "    out = []\n",
    "    for row in token_tensor:\n",
    "        toks = []\n",
    "        for tid in row:\n",
    "            if tid == 0:\n",
    "                continue\n",
    "            tok = de_vocabulary[int(tid)] if int(tid) < len(de_vocabulary) else \"[UNK]\"\n",
    "            if tok == end_token:\n",
    "                break\n",
    "            toks.append(tok)\n",
    "        out.append(toks)\n",
    "    return out\n",
    "\n",
    "class BleuMetric:\n",
    "    def __init__(self, end_token=\"eos\"):\n",
    "        self.end_token = end_token\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, y_true_ids, y_pred_probs):\n",
    "        # y_true_ids: (batch, time)\n",
    "        # y_pred_probs: (batch, time, vocab)\n",
    "        y_pred_ids = np.argmax(y_pred_probs, axis=-1)\n",
    "\n",
    "        refs = clean_text(y_true_ids, end_token=self.end_token)\n",
    "        cands = clean_text(y_pred_ids, end_token=self.end_token)\n",
    "\n",
    "        scores = []\n",
    "        for r, c in zip(refs, cands):\n",
    "            scores.append(sentence_bleu(r, c))\n",
    "        return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "bleu_metric = BleuMetric(end_token=\"eos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575123f",
   "metadata": {},
   "source": [
    "## 7) Model with Bahdanau attention\n",
    "\n",
    "### 7.1 Encoder output changes from Part 1\n",
    "Attention needs **all encoder time steps**, not only a final vector.  \n",
    "So the encoder must output:\n",
    "- `enc_outputs`: (batch, enc_time, hidden_dim)\n",
    "- forward and backward states to initialize the decoder.\n",
    "\n",
    "### 7.2 Attention layer implementation detail\n",
    "I implement Bahdanau attention as a Keras layer:\n",
    "- it takes `enc_outputs` and `dec_outputs`,\n",
    "- computes alignment scores and a softmax distribution over encoder steps,\n",
    "- returns a context sequence aligned to decoder time steps and the attention weights.\n",
    "\n",
    "The attention weights are kept accessible so I can build a second model later that outputs them for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.W = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.U = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.V = tf.keras.layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs = [enc_outputs, dec_outputs]\n",
    "        enc_outputs, dec_outputs = inputs  # (B, Te, H), (B, Td, H)\n",
    "\n",
    "        # Transform\n",
    "        # W(enc): (B, Te, units)\n",
    "        # U(dec): (B, Td, units)\n",
    "        w_enc = self.W(enc_outputs)\n",
    "        u_dec = self.U(dec_outputs)\n",
    "\n",
    "        # Broadcast addition: (B, Td, Te, units)\n",
    "        w_enc = tf.expand_dims(w_enc, axis=1)         # (B, 1, Te, units)\n",
    "        u_dec = tf.expand_dims(u_dec, axis=2)         # (B, Td, 1, units)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(w_enc + u_dec))     # (B, Td, Te, 1)\n",
    "        attn_weights = tf.nn.softmax(score, axis=2)   # softmax over encoder steps\n",
    "\n",
    "        # Context: weighted sum over encoder outputs\n",
    "        enc_exp = tf.expand_dims(enc_outputs, axis=1)         # (B, 1, Te, H)\n",
    "        context = tf.reduce_sum(attn_weights * enc_exp, axis=2)  # (B, Td, H)\n",
    "\n",
    "        return context, tf.squeeze(attn_weights, axis=-1)     # weights: (B, Td, Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_with_states(n_vocab: int, vectorizer: tf.keras.Model, enc_len: int):\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"e_input\")\n",
    "    x = vectorizer(inp)  # (B, enc_len)\n",
    "\n",
    "    emb = tf.keras.layers.Embedding(n_vocab + 2, 128, mask_zero=True, name=\"e_embedding\")\n",
    "    x = emb(x)  # (B, enc_len, 128)\n",
    "\n",
    "    bi_gru = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, return_sequences=True, return_state=True),\n",
    "        name=\"e_bi_gru\",\n",
    "    )\n",
    "    # output: (B, enc_len, 256), states: (B, 128), (B, 128)\n",
    "    enc_outputs, fwd_state, bwd_state = bi_gru(x)\n",
    "\n",
    "    encoder = tf.keras.models.Model(\n",
    "        inputs=inp,\n",
    "        outputs=[fwd_state, bwd_state, enc_outputs],\n",
    "        name=\"encoder_with_states\",\n",
    "    )\n",
    "    return encoder\n",
    "\n",
    "encoder = get_encoder_with_states(n_vocab=en_vocab, vectorizer=en_vectorizer, enc_len=en_seq_length)\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_seq2seq_model(n_vocab: int, encoder: tf.keras.Model, de_vectorizer: tf.keras.Model):\n",
    "    # Encoder inputs\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"e_input_final\")\n",
    "    fwd_state, bwd_state, enc_outputs = encoder(e_inp)\n",
    "    d_init_state = tf.concat([fwd_state, bwd_state], axis=-1)  # (B, 256)\n",
    "\n",
    "    # Decoder inputs (teacher forcing)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"d_input\")\n",
    "    d_vec = de_vectorizer(d_inp)  # (B, dec_len)\n",
    "\n",
    "    d_emb = tf.keras.layers.Embedding(n_vocab + 2, 128, mask_zero=True, name=\"d_embedding\")\n",
    "    d_emb_out = d_emb(d_vec)\n",
    "\n",
    "    d_gru = tf.keras.layers.GRU(256, return_sequences=True, name=\"d_gru\")\n",
    "    d_gru_out = d_gru(d_emb_out, initial_state=d_init_state)  # (B, dec_len, 256)\n",
    "\n",
    "    # Attention\n",
    "    attn_layer = BahdanauAttention(256, name=\"bahdanau_attention\")\n",
    "    context, attn_weights = attn_layer([enc_outputs, d_gru_out])  # (B, dec_len, 256), (B, dec_len, enc_len)\n",
    "\n",
    "    # Combine context + decoder output\n",
    "    concat = tf.concat([context, d_gru_out], axis=-1)  # (B, dec_len, 512)\n",
    "    combine = tf.keras.layers.Dense(256, activation=\"tanh\", name=\"attn_combine\")(concat)\n",
    "\n",
    "    # Classifier head\n",
    "    d_dense_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"d_dense_1\")\n",
    "    x = d_dense_1(combine)\n",
    "\n",
    "    d_final = tf.keras.layers.Dense(n_vocab + 2, activation=\"softmax\", name=\"d_dense_final\")\n",
    "    out = d_final(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[e_inp, d_inp], outputs=out, name=\"seq2seq_with_attention\")\n",
    "    return model\n",
    "\n",
    "attn_model = get_attention_seq2seq_model(n_vocab=de_vocab, encoder=encoder, de_vectorizer=de_vectorizer)\n",
    "attn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b20493",
   "metadata": {},
   "source": [
    "## 8) Training helpers (batch evaluation + BLEU)\n",
    "\n",
    "I keep the same approach as Chapter 11:\n",
    "- vectorize labels to token IDs,\n",
    "- train using sparse categorical cross-entropy,\n",
    "- report mean loss and accuracy,\n",
    "- compute BLEU from model predictions for a more translation-relevant metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafe7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, vectorizer, data_split, batch_size=128, bleu_metric=None):\n",
    "    en_inputs_raw = data_split[\"encoder_inputs\"]\n",
    "    de_inputs_raw = data_split[\"decoder_inputs\"]\n",
    "    de_labels_raw = data_split[\"decoder_labels\"]\n",
    "\n",
    "    loss_log, acc_log, bleu_log = [], [], []\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "    if n_batches == 0:\n",
    "        n_batches = 1\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        x = [\n",
    "            en_inputs_raw[i*batch_size:(i+1)*batch_size],\n",
    "            de_inputs_raw[i*batch_size:(i+1)*batch_size],\n",
    "        ]\n",
    "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        eval_out = model.evaluate(x, y, verbose=0)\n",
    "        loss_log.append(eval_out[0])\n",
    "        acc_log.append(eval_out[1])\n",
    "\n",
    "        if bleu_metric is not None:\n",
    "            pred_y = model.predict(x, verbose=0)\n",
    "            bleu = bleu_metric.calculate_bleu_from_predictions(y.numpy(), pred_y)\n",
    "            bleu_log.append(bleu)\n",
    "\n",
    "    mean_loss = float(np.mean(loss_log))\n",
    "    mean_acc = float(np.mean(acc_log))\n",
    "    mean_bleu = float(np.mean(bleu_log)) if bleu_log else 0.0\n",
    "    return mean_loss, mean_acc, mean_bleu\n",
    "\n",
    "def train_model(model, vectorizer, data_dict, epochs=5, batch_size=128, bleu_metric=None):\n",
    "    train_split = data_dict[\"train\"]\n",
    "    valid_split = data_dict[\"valid\"]\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\nEpoch {ep}/{epochs}\")\n",
    "\n",
    "        # Train in mini-batches\n",
    "        en_inputs_raw = train_split[\"encoder_inputs\"]\n",
    "        de_inputs_raw = train_split[\"decoder_inputs\"]\n",
    "        de_labels_raw = train_split[\"decoder_labels\"]\n",
    "\n",
    "        idx = np.arange(len(en_inputs_raw))\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        n_batches = len(idx) // batch_size\n",
    "        if n_batches == 0:\n",
    "            n_batches = 1\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            b = idx[i*batch_size:(i+1)*batch_size]\n",
    "            x = [en_inputs_raw[b], de_inputs_raw[b]]\n",
    "            y = vectorizer(de_labels_raw[b])\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "        tr_loss, tr_acc, tr_bleu = evaluate_model(model, vectorizer, train_split, batch_size=batch_size, bleu_metric=bleu_metric)\n",
    "        va_loss, va_acc, va_bleu = evaluate_model(model, vectorizer, valid_split, batch_size=batch_size, bleu_metric=bleu_metric)\n",
    "\n",
    "        print(f\"(train) loss: {tr_loss:.4f} - accuracy: {tr_acc:.4f} - bleu: {tr_bleu:.4f}\")\n",
    "        print(f\"(valid) loss: {va_loss:.4f} - accuracy: {va_acc:.4f} - bleu: {va_bleu:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248eaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "attn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "attn_model = train_model(\n",
    "    attn_model,\n",
    "    vectorizer=de_vectorizer,\n",
    "    data_dict=data_dict,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bleu_metric=bleu_metric,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ba0d7",
   "metadata": {},
   "source": [
    "## 9) Test evaluation\n",
    "\n",
    "I evaluate on the test split after training.  \n",
    "The BLEU score here is computed from model predictions on the same batches used for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c21fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_loss, te_acc, te_bleu = evaluate_model(\n",
    "    attn_model,\n",
    "    vectorizer=de_vectorizer,\n",
    "    data_split=data_dict[\"test\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bleu_metric=bleu_metric,\n",
    ")\n",
    "\n",
    "print(f\"(test) loss: {te_loss:.4f} - accuracy: {te_acc:.4f} - bleu: {te_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b0f71",
   "metadata": {},
   "source": [
    "## 10) Save the trained model and vocabularies\n",
    "\n",
    "Saving the model and vocabularies makes the notebook reproducible:\n",
    "- the same token-to-id mapping is necessary for consistent inference,\n",
    "- and it avoids needing to re-adapt vectorizers when testing translations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d85ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "attn_model.save(MODEL_DIR / \"ch12_seq2seq_attention.keras\")\n",
    "\n",
    "# Save vocabularies\n",
    "with open(MODEL_DIR / \"ch12_en_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in en_vocabulary:\n",
    "        f.write(tok + \"\\n\")\n",
    "\n",
    "with open(MODEL_DIR / \"ch12_de_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in de_vocabulary:\n",
    "        f.write(tok + \"\\n\")\n",
    "\n",
    "print(\"Saved model and vocabularies in:\", MODEL_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ce1b1",
   "metadata": {},
   "source": [
    "## 11) Attention visualization\n",
    "\n",
    "To visualize attention, I build a second model that shares the same trained weights but also outputs the attention weights tensor.\n",
    "\n",
    "Because the attention layer is part of the computation graph, I can retrieve its second output (weights) without changing the training model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39be5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model that outputs both translation probabilities and attention weights\n",
    "attn_layer = attn_model.get_layer(\"bahdanau_attention\")\n",
    "\n",
    "# Layer outputs: (context, weights)\n",
    "context_tensor, weights_tensor = attn_layer.output\n",
    "\n",
    "viz_model = tf.keras.Model(\n",
    "    inputs=attn_model.inputs,\n",
    "    outputs=[attn_model.output, weights_tensor],\n",
    "    name=\"seq2seq_with_attention_viz\",\n",
    ")\n",
    "\n",
    "viz_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0627b",
   "metadata": {},
   "source": [
    "### 11.1 Greedy decoding (simple inference) + attention matrix\n",
    "\n",
    "For a qualitative check, I implement a small greedy decoder:\n",
    "- start with `sos`,\n",
    "- repeatedly predict the next token by argmax,\n",
    "- stop when `eos` is produced or when max steps is reached.\n",
    "\n",
    "At the same time, I record the attention weights at each decoding step so I can visualize a heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build token id mappings from vocabulary lists\n",
    "en_word_index = {w: i for i, w in enumerate(en_vocabulary)}\n",
    "de_word_index = {w: i for i, w in enumerate(de_vocabulary)}\n",
    "de_index_word = {i: w for i, w in enumerate(de_vocabulary)}\n",
    "\n",
    "sos_id = de_word_index.get(\"sos\", None)\n",
    "eos_id = de_word_index.get(\"eos\", None)\n",
    "print(\"sos id:\", sos_id, \"| eos id:\", eos_id)\n",
    "\n",
    "def standardize_text(s: str):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-zA-Zäöüß0-9\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def greedy_translate_with_attention(sentence: str, max_len=30):\n",
    "    sentence = standardize_text(sentence)\n",
    "    enc_inp = np.array([[sentence]], dtype=object)\n",
    "\n",
    "    # Start decoder with \"sos\"\n",
    "    dec_tokens = [\"sos\"]\n",
    "    attn_rows = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        dec_inp = np.array([[\" \".join(dec_tokens)]], dtype=object)\n",
    "\n",
    "        probs, weights = viz_model.predict([enc_inp, dec_inp], verbose=0)\n",
    "        # probs: (1, dec_len, vocab), weights: (1, dec_len, enc_len)\n",
    "\n",
    "        # Next token is last time step prediction\n",
    "        next_id = int(np.argmax(probs[0, len(dec_tokens)-1]))\n",
    "        next_tok = de_index_word.get(next_id, \"[UNK]\")\n",
    "\n",
    "        # Save attention row for this step (use same time index)\n",
    "        attn_row = weights[0, len(dec_tokens)-1]  # (enc_len,)\n",
    "        attn_rows.append(attn_row)\n",
    "\n",
    "        dec_tokens.append(next_tok)\n",
    "        if next_tok == \"eos\":\n",
    "            break\n",
    "\n",
    "    # Remove sos/eos for display\n",
    "    out_tokens = [t for t in dec_tokens[1:] if t != \"eos\"]\n",
    "    return out_tokens, np.array(attn_rows), sentence\n",
    "\n",
    "def decode_en_tokens(sentence: str):\n",
    "    # Approximate token display for x-axis\n",
    "    toks = standardize_text(sentence).split()\n",
    "    return toks[:en_seq_length]\n",
    "\n",
    "sample_sentence = \"I am a student.\"\n",
    "out_tokens, attn_mat, cleaned_en = greedy_translate_with_attention(sample_sentence, max_len=25)\n",
    "\n",
    "print(\"EN:\", cleaned_en)\n",
    "print(\"DE:\", \" \".join(out_tokens))\n",
    "print(\"Attention matrix shape:\", attn_mat.shape)  # (dec_steps, enc_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28966f4d",
   "metadata": {},
   "source": [
    "### 11.2 Plot the attention heatmap\n",
    "\n",
    "Rows: generated German tokens  \n",
    "Columns: English tokens\n",
    "\n",
    "Even if translation is not perfect, I expect to see some structured alignment pattern (rather than uniform or random weights).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f027fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attn_mat, en_tokens, de_tokens):\n",
    "    # attn_mat: (T_dec, enc_len). Limit to real token counts for readability.\n",
    "    enc_len = min(len(en_tokens), attn_mat.shape[1])\n",
    "    dec_len = min(len(de_tokens), attn_mat.shape[0])\n",
    "\n",
    "    mat = attn_mat[:dec_len, :enc_len]\n",
    "\n",
    "    plt.figure(figsize=(min(12, 0.6 * enc_len + 2), min(8, 0.6 * dec_len + 2)))\n",
    "    plt.imshow(mat, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(enc_len), en_tokens[:enc_len], rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(dec_len), de_tokens[:dec_len])\n",
    "    plt.xlabel(\"English tokens\")\n",
    "    plt.ylabel(\"Generated German tokens\")\n",
    "    plt.title(\"Bahdanau attention heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "en_tokens = decode_en_tokens(cleaned_en)\n",
    "plot_attention(attn_mat, en_tokens, out_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24027e7c",
   "metadata": {},
   "source": [
    "## 12) Takeaways\n",
    "\n",
    "- Attention improves seq2seq translation by reducing the compression bottleneck of plain encoder–decoder models.\n",
    "- Bahdanau attention provides a learnable alignment mechanism that selects relevant encoder time steps per decoder step.\n",
    "- BLEU complements token accuracy for translation evaluation and gives a more task-relevant signal.\n",
    "- Visualizing attention weights is a practical way to validate that the model is using source context in a structured way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7466a",
   "metadata": {},
   "source": [
    "## 13) References\n",
    "\n",
    "- Thushan Ganegedara, *TensorFlow in Action* (Chapter 12).\n",
    "- Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate.”\n",
    "- ManyThings.org English–German sentence pairs (Anki dataset).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
