{"cells":[{"cell_type":"markdown","id":"c7327c78","metadata":{"id":"c7327c78"},"source":["# Chapter 10 — Natural Language Processing with TensorFlow: Language Modeling\n","\n","In the previous chapter, I treated text as an input for a *classification* problem (sentiment analysis).  \n","This chapter changes the objective: instead of predicting a label for a whole text, I train a model to **predict the next piece of text**.\n","\n","The core task is **language modeling**:\n","> Given a sequence of previous tokens, predict the next token.\n","\n","Language models are important because they force the network to learn structure in text:\n","- frequent patterns (common phrases),\n","- grammar-like constraints (what tends to follow what),\n","- and longer dependencies (topics, entities, references across sentences).\n","\n","In the book, the workflow is built around a practical dataset of children’s stories (CBTest) and a GRU-based model that learns to generate the next token.  \n","A key design choice in this chapter is to represent text using **character n-grams** (in particular, bigrams), which keeps vocabulary size manageable.\n","\n","This notebook reproduces the end-to-end process:\n","1) download and inspect the dataset,\n","2) read stories into Python,\n","3) convert stories → n-grams → token IDs,\n","4) define a `tf.data` pipeline that creates fixed-length training windows,\n","5) implement and train a GRU language model,\n","6) evaluate using **perplexity**,\n","7) generate new text using greedy decoding and beam search.\n"]},{"cell_type":"markdown","id":"61f04e8a","metadata":{"id":"61f04e8a"},"source":["## 1) Summary\n","\n","### 1.1 What is a language model in practice?\n","A language model is trained to approximate:\n","\n","\\[\n","P(w_{t} \\mid w_{1}, w_{2}, \\dots, w_{t-1})\n","\\]\n","\n","In a neural setting, this means:\n","- take a sequence of tokens,\n","- compress context into a hidden state (RNN/GRU/LSTM),\n","- output a probability distribution over the vocabulary for the next token.\n","\n","### 1.2 Why character n-grams help here\n","If I tokenize at the word level, vocabulary can explode quickly:\n","- unusual spellings,\n","- rare names,\n","- punctuation and formatting artifacts.\n","\n","The chapter uses **n-grams** as subword tokens. With small *n*, the token set is limited and OOV (out-of-vocabulary) becomes less frequent, because unseen words can often be constructed from known n-grams.\n","\n","In this notebook I use the book’s `get_ngrams(text, n)` function that splits the string into chunks with stride `n`:\n","- for `n=2`, tokens are character bigrams like `\"ch\"`, `\"ap\"`, `\"te\"`, `\"r \"`, etc.\n","\n","This reduces vocabulary size substantially compared with word tokens.\n","\n","### 1.3 How the dataset becomes training examples\n","Stories are variable-length. A model needs fixed-length sequences, so I create training windows:\n","\n","- take a long token ID sequence,\n","- create windows of length `n_seq + 1`,\n","- use first `n_seq` IDs as inputs,\n","- use the same sequence shifted by 1 as targets.\n","\n","This matches the “next-token prediction” objective.\n","\n","### 1.4 How I judge model quality (perplexity)\n","Accuracy can be misleading in language modeling because:\n","- predicting common tokens can inflate accuracy,\n","- but the probability assigned to correct tokens still matters a lot.\n","\n","Perplexity is the standard metric:\n","\n","\\[\n","\\text{perplexity} = \\exp(\\text{cross-entropy})\n","\\]\n","\n","Lower perplexity indicates the model is assigning higher probability to the correct next tokens on average.\n","\n","### 1.5 How text is generated\n","After training, I convert the model into an inference setup that supports:\n","- taking an initial prompt,\n","- predicting the next token,\n","- feeding the prediction back recursively.\n","\n","I implement two decoding methods from the chapter:\n","- **greedy decoding**: always pick the highest-probability next token,\n","- **beam search**: keep the top-*k* candidate sequences to reduce the chance of early greedy mistakes.\n"]},{"cell_type":"markdown","id":"35f0f23f","metadata":{"id":"35f0f23f"},"source":["## 2) Setup\n","\n","Imports, seeds, and a few helper utilities."]},{"cell_type":"code","execution_count":1,"id":"a9eb5919","metadata":{"id":"a9eb5919","executionInfo":{"status":"ok","timestamp":1767982195054,"user_tz":-420,"elapsed":9319,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"065dd93a-4e3c-4b86-edd4-ca36ad1f36ee","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow: 2.19.0\n"]}],"source":["import os\n","import tarfile\n","import random\n","import pickle\n","from pathlib import Path\n","from collections import Counter\n","\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","SEED = 4321\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","\n","print(\"TensorFlow:\", tf.__version__)\n"]},{"cell_type":"markdown","id":"b3f07424","metadata":{"id":"b3f07424"},"source":["## 3) Download and extract the CBTest dataset\n","\n","The chapter uses the CBTest dataset (a set of children’s book stories).\n","\n","Download URL used in the book (tgz archive):\n","- `http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz`\n","\n","This section:\n","- downloads the archive if it does not exist,\n","- extracts it to a local directory,\n","- and then lists the available files so I can pick train/valid/test files.\n"]},{"cell_type":"code","execution_count":3,"id":"88e51541","metadata":{"id":"88e51541","executionInfo":{"status":"error","timestamp":1767982198711,"user_tz":-420,"elapsed":444,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"291747a9-9121-42de-ceea-81a94eae82bd","colab":{"base_uri":"https://localhost:8080/","height":369}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n"]},{"output_type":"error","ename":"HTTPError","evalue":"404 Client Error: Not Found for url: http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-798760516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz"]}],"source":["import requests\n","\n","DATA_DIR = Path(\"data\") / \"lm\"\n","DATA_DIR.mkdir(parents=True, exist_ok=True)\n","\n","tgz_path = DATA_DIR / \"CBTest.tgz\"\n","cbtest_dir = DATA_DIR / \"CBTest\"\n","\n","url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n","\n","if not tgz_path.exists():\n","    print(\"Downloading:\", url)\n","    r = requests.get(url, stream=True)\n","    r.raise_for_status()\n","    with open(tgz_path, \"wb\") as f:\n","        for chunk in r.iter_content(chunk_size=1024 * 1024):\n","            if chunk:\n","                f.write(chunk)\n","    print(\"Saved:\", tgz_path)\n","else:\n","    print(\"Archive already exists:\", tgz_path)\n","\n","if not cbtest_dir.exists():\n","    print(\"Extracting archive...\")\n","    with tarfile.open(tgz_path) as tarf:\n","        tarf.extractall(DATA_DIR)\n","    print(\"Extracted to:\", cbtest_dir)\n","else:\n","    print(\"Extracted folder already exists:\", cbtest_dir)\n","\n","# List available text files\n","all_txt = sorted([p for p in cbtest_dir.rglob(\"*.txt\")])\n","print(\"Number of .txt files:\", len(all_txt))\n","for p in all_txt[:20]:\n","    print(\" -\", p.relative_to(cbtest_dir))\n"]},{"cell_type":"markdown","id":"d5b8927a","metadata":{"id":"d5b8927a"},"source":["## 4) Reading stories into Python\n","\n","The CBTest files contain multiple stories.  \n","In the chapter, stories are separated by a line that begins with `_BOOK_TITLE_`.\n","\n","I follow the same idea:\n","- maintain a list `s` for the current story lines,\n","- when `_BOOK_TITLE_` appears, close out the previous story (if any) and start a new one,\n","- join story lines into a single long string.\n","\n","I also include a fallback: if `_BOOK_TITLE_` markers are not present (dataset variants can differ), the code treats the entire file as a single “story”.\n"]},{"cell_type":"code","execution_count":null,"id":"4cddd70a","metadata":{"id":"4cddd70a","executionInfo":{"status":"aborted","timestamp":1767982195871,"user_tz":-420,"elapsed":10168,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def read_stories(path):\n","    stories = []\n","    current = []\n","\n","    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","        for row in f:\n","            row = row.rstrip(\"\\n\")\n","\n","            if row.startswith(\"_BOOK_TITLE_\"):\n","                if len(current) > 0:\n","                    stories.append(\" \".join(current).strip())\n","                current = []\n","                continue\n","\n","            # Remove leading line numbers if present (common in bAbI/CBTest formats)\n","            # Example: \"1 Mary went to the bathroom.\"\n","            parts = row.split(\" \", 1)\n","            if len(parts) == 2 and parts[0].isdigit():\n","                row = parts[1]\n","\n","            if row.strip():\n","                current.append(row)\n","\n","    if len(current) > 0:\n","        stories.append(\" \".join(current).strip())\n","\n","    # Fallback: if no markers and nothing collected, return empty list\n","    return stories\n","\n","def pick_split_files(cbtest_dir):\n","    txts = sorted([p for p in cbtest_dir.rglob(\"*.txt\")])\n","    # Heuristic: pick files with train/valid/test in name if present\n","    train = next((p for p in txts if \"train\" in p.name.lower()), None)\n","    valid = next((p for p in txts if \"valid\" in p.name.lower() or \"val\" in p.name.lower()), None)\n","    test  = next((p for p in txts if \"test\" in p.name.lower()), None)\n","\n","    # If not found, fallback to first three text files\n","    if train is None or valid is None or test is None:\n","        if len(txts) >= 3:\n","            train = train or txts[0]\n","            valid = valid or txts[1]\n","            test  = test  or txts[2]\n","\n","    return train, valid, test\n","\n","train_file, valid_file, test_file = pick_split_files(cbtest_dir)\n","print(\"Train file:\", train_file)\n","print(\"Valid file:\", valid_file)\n","print(\"Test file :\", test_file)\n","\n","train_stories = read_stories(train_file)\n","valid_stories = read_stories(valid_file)\n","test_stories  = read_stories(test_file)\n","\n","print(f\"Collected {len(train_stories)} stories (train)\")\n","print(f\"Collected {len(valid_stories)} stories (valid)\")\n","print(f\"Collected {len(test_stories)} stories (test)\")\n","\n","# Peek at one story\n","if len(train_stories) > 0:\n","    print(\"\\nSample story snippet:\")\n","    print(train_stories[min(10, len(train_stories)-1)][:500])\n"]},{"cell_type":"markdown","id":"5a89749c","metadata":{"id":"5a89749c"},"source":["## 5) N-grams (character chunks)\n","\n","The chapter defines a very simple n-gram function:\n","\n","```python\n","def get_ngrams(text, n):\n","    return [text[i:i+n] for i in range(0, len(text), n)]\n","```\n","\n","This is not a sliding window; it is a chunking operation with stride `n`.  \n","For `n=2`, it splits the string into non-overlapping bigrams.\n","\n","I reproduce the function exactly and test it on a small example.\n"]},{"cell_type":"code","execution_count":null,"id":"cf540755","metadata":{"id":"cf540755","executionInfo":{"status":"aborted","timestamp":1767982195874,"user_tz":-420,"elapsed":10171,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def get_ngrams(text, n):\n","    return [text[i:i+n] for i in range(0, len(text), n)]\n","\n","test_string = \"I like chocolates\"\n","print(\"Original:\", test_string)\n","for n in [1, 2, 3]:\n","    print(f\"{n}-grams:\", get_ngrams(test_string, n)[:30], \"...\")\n"]},{"cell_type":"markdown","id":"7a6fc0cf","metadata":{"id":"7a6fc0cf"},"source":["### 5.1 Vocabulary comparison: words vs n-grams (quick sanity check)\n","\n","This is a small measurement step to confirm the motivation:\n","- word vocabulary size can be very large,\n","- character n-gram vocabulary size is much smaller for small *n*.\n"]},{"cell_type":"code","execution_count":null,"id":"4cc0d1f5","metadata":{"id":"4cc0d1f5","executionInfo":{"status":"aborted","timestamp":1767982195933,"user_tz":-420,"elapsed":10230,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def word_vocab_size(texts):\n","    vocab = set()\n","    for t in texts:\n","        for w in t.lower().split():\n","            vocab.add(w)\n","    return len(vocab)\n","\n","def ngram_vocab_size(texts, n):\n","    vocab = set()\n","    for t in texts:\n","        for g in get_ngrams(t.lower(), n):\n","            vocab.add(g)\n","    return len(vocab)\n","\n","ngrams = 2\n","wv = word_vocab_size(train_stories[:50])  # sample to keep this quick\n","ngv = ngram_vocab_size(train_stories[:50], ngrams)\n","\n","print(\"Word vocab size (sample):\", wv)\n","print(f\"{ngrams}-gram vocab size (sample):\", ngv)\n"]},{"cell_type":"markdown","id":"98c76f50","metadata":{"id":"98c76f50"},"source":["## 6) Tokenization: n-grams → token IDs\n","\n","The chapter uses `Tokenizer` from Keras.\n","\n","A practical issue is that n-grams can contain spaces and punctuation, so default splitting (by space) can break tokens.  \n","To avoid that, I join tokens using a delimiter (tab) and configure the tokenizer with `split=\"\\t\"` and `filters=\"\"`.\n","\n","I also apply a minimum frequency cutoff (`MIN_FREQ=10`) similar to the chapter’s goal:\n","- n-grams that appear fewer than `MIN_FREQ` times are mapped to an OOV token.\n"]},{"cell_type":"code","execution_count":null,"id":"a7b014ce","metadata":{"id":"a7b014ce","executionInfo":{"status":"aborted","timestamp":1767982195934,"user_tz":-420,"elapsed":10230,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","MIN_FREQ = 10\n","OOV_TOKEN = \"[UNK]\"\n","DELIM = \"\\t\"\n","\n","def stories_to_ngram_docs(stories, n):\n","    # Lowercase for consistency; keep punctuation as characters.\n","    docs = []\n","    for s in stories:\n","        grams = get_ngrams(s.lower(), n)\n","        docs.append(DELIM.join(grams))\n","    return docs\n","\n","train_docs = stories_to_ngram_docs(train_stories, ngrams)\n","valid_docs = stories_to_ngram_docs(valid_stories, ngrams)\n","test_docs  = stories_to_ngram_docs(test_stories,  ngrams)\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, split=DELIM, oov_token=OOV_TOKEN)\n","tokenizer.fit_on_texts(train_docs)\n","\n","# Build a frequency map\n","word_counts = tokenizer.word_counts  # token -> count\n","\n","# Identify tokens that meet the minimum frequency (excluding OOV token)\n","kept_tokens = [tok for tok, cnt in word_counts.items() if cnt >= MIN_FREQ and tok != OOV_TOKEN]\n","\n","print(\"Total unique n-grams:\", len(word_counts))\n","print(f\"Kept tokens (freq >= {MIN_FREQ}):\", len(kept_tokens))\n","\n","# Define a max vocab size based on kept tokens (plus OOV + padding)\n","# Tokenizer indices start at 1; index 0 is reserved for padding.\n","oov_id = tokenizer.word_index[OOV_TOKEN]\n","print(\"OOV token id:\", oov_id)\n","\n","# Convert documents to sequences, then remap rare tokens to OOV id\n","def docs_to_sequences_with_minfreq(docs, tokenizer, word_counts, min_freq, oov_id):\n","    seqs = []\n","    for d in docs:\n","        ids = tokenizer.texts_to_sequences([d])[0]\n","        toks = d.split(DELIM)\n","        # toks and ids should align\n","        out = []\n","        for tok, idx in zip(toks, ids):\n","            if word_counts.get(tok, 0) < min_freq:\n","                out.append(oov_id)\n","            else:\n","                out.append(idx)\n","        seqs.append(out)\n","    return seqs\n","\n","train_seqs = docs_to_sequences_with_minfreq(train_docs, tokenizer, word_counts, MIN_FREQ, oov_id)\n","valid_seqs = docs_to_sequences_with_minfreq(valid_docs, tokenizer, word_counts, MIN_FREQ, oov_id)\n","test_seqs  = docs_to_sequences_with_minfreq(test_docs,  tokenizer, word_counts, MIN_FREQ, oov_id)\n","\n","# Vocabulary size for the model (max id + 1 to include padding=0)\n","vocab_size = max(max(s) for s in train_seqs if len(s) > 0) + 1\n","print(\"Model vocab_size (including padding):\", vocab_size)\n","\n","# Show a quick example\n","example = train_stories[min(10, len(train_stories)-1)][:120]\n","eg_grams = get_ngrams(example.lower(), ngrams)\n","eg_doc = DELIM.join(eg_grams)\n","eg_ids = docs_to_sequences_with_minfreq([eg_doc], tokenizer, word_counts, MIN_FREQ, oov_id)[0]\n","\n","print(\"\\nOriginal snippet:\", example)\n","print(\"n-grams:\", eg_grams[:30], \"...\")\n","print(\"IDs   :\", eg_ids[:30], \"...\")\n"]},{"cell_type":"markdown","id":"239eef5d","metadata":{"id":"239eef5d"},"source":["## 7) `tf.data` pipeline: fixed-length windows for next-token prediction\n","\n","Stories have different lengths, so I create a dataset of fixed-length training windows.\n","\n","For each story sequence:\n","- create sliding windows of length `n_seq + 1` (where the extra 1 is the next-token target),\n","- split window into `(inputs, targets)` where targets are inputs shifted by 1.\n","\n","This reproduces the idea shown in the chapter listing.\n","\n","I keep the shift at 1 (classic next-token prediction), and I use `drop_remainder=True` so every example has the same length.\n"]},{"cell_type":"code","execution_count":null,"id":"d4b26b22","metadata":{"id":"d4b26b22","executionInfo":{"status":"aborted","timestamp":1767982195935,"user_tz":-420,"elapsed":10231,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n","    \"\"\"Create a tf.data pipeline from a list of variable-length integer sequences.\"\"\"\n","    ragged = tf.ragged.constant(data_seq, dtype=tf.int32)\n","    ds = tf.data.Dataset.from_tensor_slices(ragged)\n","\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=min(len(data_seq), 1024), seed=SEED, reshuffle_each_iteration=True)\n","\n","    # Convert each ragged sequence into many fixed-length windows\n","    ds = ds.flat_map(\n","        lambda x: tf.data.Dataset.from_tensor_slices(x)\n","            .window(n_seq + 1, shift=shift, drop_remainder=True)\n","            .flat_map(lambda w: w.batch(n_seq + 1, drop_remainder=True))\n","    )\n","\n","    # Split into inputs and targets\n","    ds = ds.map(lambda w: (w[:-1], w[1:]), num_parallel_calls=AUTOTUNE)\n","    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n","    return ds\n","\n","n_seq = 100\n","BATCH_SIZE = 64\n","\n","train_ds = get_tf_pipeline(train_seqs, n_seq=n_seq, batch_size=BATCH_SIZE, shift=1, shuffle=True)\n","valid_ds = get_tf_pipeline(valid_seqs, n_seq=n_seq, batch_size=BATCH_SIZE, shift=1, shuffle=False)\n","test_ds  = get_tf_pipeline(test_seqs,  n_seq=n_seq, batch_size=BATCH_SIZE, shift=1, shuffle=False)\n","\n","# Sanity check one batch\n","x0, y0 = next(iter(train_ds))\n","print(\"inputs :\", x0.shape, x0.dtype)\n","print(\"targets:\", y0.shape, y0.dtype)\n","print(\"First input sequence (first 20 ids):\", x0[0, :20].numpy().tolist())\n","print(\"First target sequence (first 20 ids):\", y0[0, :20].numpy().tolist())\n"]},{"cell_type":"markdown","id":"926bcfb8","metadata":{"id":"926bcfb8"},"source":["### 7.1 Save hyperparameters (for reproducibility)\n","\n","The chapter explicitly saves key preprocessing hyperparameters:\n","- n in n-grams,\n","- vocabulary size,\n","- sequence length.\n","\n","I do the same so that generation notebooks or later reuse can stay consistent.\n"]},{"cell_type":"code","execution_count":null,"id":"44c0e28e","metadata":{"id":"44c0e28e","executionInfo":{"status":"aborted","timestamp":1767982195936,"user_tz":-420,"elapsed":10232,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["MODEL_DIR = Path(\"models\")\n","MODEL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","hyperparams = {\n","    \"ngrams\": ngrams,\n","    \"vocab_size\": vocab_size,\n","    \"n_seq\": n_seq,\n","    \"min_freq\": MIN_FREQ,\n","    \"oov_token\": OOV_TOKEN,\n","}\n","\n","print(\"n_grams uses n={}\".format(hyperparams[\"ngrams\"]))\n","print(\"Vocabulary size: {}\".format(hyperparams[\"vocab_size\"]))\n","print(\"Sequence length for model: {}\".format(hyperparams[\"n_seq\"]))\n","\n","with open(MODEL_DIR / \"text_hyperparams.pkl\", \"wb\") as f:\n","    pickle.dump(hyperparams, f)\n"]},{"cell_type":"markdown","id":"6b7bbd04","metadata":{"id":"6b7bbd04"},"source":["## 8) Model: GRU-based language model\n","\n","The chapter builds a GRU language model using:\n","- an embedding layer (learn token vectors),\n","- a GRU with `return_sequences=True` (predict at every time step),\n","- a dense head that produces vocabulary-sized logits/probabilities.\n","\n","The output has shape `(batch, n_seq, vocab_size)`.\n","\n","Because this is a multi-class next-token problem, I use:\n","- `SparseCategoricalCrossentropy`,\n","- and track perplexity as an additional metric.\n"]},{"cell_type":"code","execution_count":null,"id":"5b69cab8","metadata":{"id":"5b69cab8","executionInfo":{"status":"aborted","timestamp":1767982195937,"user_tz":-420,"elapsed":10232,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["import tensorflow.keras.backend as K\n","\n","class PerplexityMetric(tf.keras.metrics.Mean):\n","    def __init__(self, name=\"perplexity\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=False, reduction=\"none\"\n","        )\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        # y_true: (B, T), y_pred: (B, T, V)\n","        ce = self.cross_entropy(y_true, y_pred)  # (B, T) -> reduced per element by loss impl\n","        # Ensure we reduce over time steps\n","        ce = tf.reduce_mean(ce, axis=-1)  # (B,)\n","        return super().update_state(ce, sample_weight=sample_weight)\n","\n","    def result(self):\n","        return tf.exp(super().result())\n","\n","def build_language_model(vocab_size):\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=512, input_shape=(None,)),\n","        tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n","        tf.keras.layers.Dense(512, activation=\"relu\"),\n","        tf.keras.layers.Dense(vocab_size, name=\"final_out\"),\n","        tf.keras.layers.Activation(\"softmax\"),\n","    ])\n","    return model\n","\n","lm = build_language_model(vocab_size)\n","lm.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","    metrics=[\"accuracy\", PerplexityMetric()],\n",")\n","\n","lm.summary()\n"]},{"cell_type":"markdown","id":"b4c0c6b1","metadata":{"id":"b4c0c6b1"},"source":["## 9) Training and evaluation\n","\n","This is a compute-heavy model (embedding 512 + GRU 1024), and the dataset creates many windows.\n","\n","To keep this notebook runnable in Colab:\n","- I compute an approximate number of training windows,\n","- then set `steps_per_epoch` and `validation_steps`.\n","\n","If you want a stronger result, increasing epochs and steps will usually help.\n"]},{"cell_type":"code","execution_count":null,"id":"cf5a229f","metadata":{"id":"cf5a229f","executionInfo":{"status":"aborted","timestamp":1767982195938,"user_tz":-420,"elapsed":10233,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def count_windows(seqs, n_seq, shift=1):\n","    total = 0\n","    for s in seqs:\n","        L = len(s)\n","        if L >= n_seq + 1:\n","            total += 1 + (L - (n_seq + 1)) // shift\n","    return total\n","\n","train_windows = count_windows(train_seqs, n_seq, shift=1)\n","valid_windows = count_windows(valid_seqs, n_seq, shift=1)\n","test_windows  = count_windows(test_seqs,  n_seq, shift=1)\n","\n","print(\"Approx window counts\")\n","print(\"train:\", train_windows)\n","print(\"valid:\", valid_windows)\n","print(\"test :\", test_windows)\n","\n","steps_per_epoch = max(1, train_windows // BATCH_SIZE)\n","validation_steps = max(1, valid_windows // BATCH_SIZE)\n","test_steps = max(1, test_windows // BATCH_SIZE)\n","\n","print(\"steps_per_epoch:\", steps_per_epoch)\n","print(\"validation_steps:\", validation_steps)\n","print(\"test_steps:\", test_steps)\n"]},{"cell_type":"code","execution_count":null,"id":"1c6a534f","metadata":{"id":"1c6a534f","executionInfo":{"status":"aborted","timestamp":1767982195939,"user_tz":-420,"elapsed":10234,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","EPOCHS = 5\n","ckpt_path = MODEL_DIR / \"ch10_language_model_best.keras\"\n","\n","callbacks = [\n","    ModelCheckpoint(str(ckpt_path), monitor=\"val_perplexity\", mode=\"min\", save_best_only=True),\n","    EarlyStopping(monitor=\"val_perplexity\", mode=\"min\", patience=2, restore_best_weights=True),\n","]\n","\n","history = lm.fit(\n","    train_ds,\n","    validation_data=valid_ds,\n","    epochs=EPOCHS,\n","    steps_per_epoch=min(steps_per_epoch, 2000),      # cap for runtime\n","    validation_steps=min(validation_steps, 500),\n","    callbacks=callbacks,\n","    verbose=1,\n",")\n"]},{"cell_type":"markdown","id":"44f86187","metadata":{"id":"44f86187"},"source":["### 9.1 Plot learning curves"]},{"cell_type":"code","execution_count":null,"id":"1f24d9b2","metadata":{"id":"1f24d9b2","executionInfo":{"status":"aborted","timestamp":1767982195952,"user_tz":-420,"elapsed":10247,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def plot_history(hist, keys):\n","    plt.figure(figsize=(10, 4))\n","    for k in keys:\n","        if k in hist.history:\n","            plt.plot(hist.history[k], label=k)\n","    plt.xlabel(\"Epoch\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","plot_history(history, [\"loss\", \"val_loss\"])\n","plot_history(history, [\"perplexity\", \"val_perplexity\"])\n","plot_history(history, [\"accuracy\", \"val_accuracy\"])\n"]},{"cell_type":"markdown","id":"4d8b65d0","metadata":{"id":"4d8b65d0"},"source":["### 9.2 Evaluate on the test split\n","\n","The test split is built from a separate CBTest file (selected automatically by filename heuristics).  \n","I report loss, accuracy, and perplexity.\n"]},{"cell_type":"code","execution_count":null,"id":"f0d24817","metadata":{"id":"f0d24817","executionInfo":{"status":"aborted","timestamp":1767982195953,"user_tz":-420,"elapsed":10247,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["test_metrics = lm.evaluate(test_ds, steps=min(test_steps, 500), verbose=1)\n","for name, value in zip(lm.metrics_names, test_metrics):\n","    print(f\"{name:12s}: {value:.4f}\")\n"]},{"cell_type":"markdown","id":"12e01496","metadata":{"id":"12e01496"},"source":["## 10) Inference model (stateful decoding)\n","\n","During training, the GRU processes a whole sequence and returns predictions for every time step.\n","\n","For generation, I want a model that supports:\n","- feeding the previous hidden state,\n","- predicting the next token,\n","- returning the updated state.\n","\n","So I build a small inference graph that reuses the learned weights:\n","- `Embedding` weights,\n","- `GRU` weights,\n","- output head weights.\n","\n","This is the model I will call recursively for greedy decoding and beam search.\n"]},{"cell_type":"code","execution_count":null,"id":"930b4c5a","metadata":{"id":"930b4c5a","executionInfo":{"status":"aborted","timestamp":1767982195954,"user_tz":-420,"elapsed":10248,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def build_inference_model_from_trained(lm_model, vocab_size):\n","    # Extract layers by order (as built in build_language_model)\n","    emb_layer = lm_model.layers[0]\n","    gru_layer = lm_model.layers[1]\n","    dense1 = lm_model.layers[2]\n","    dense_out = lm_model.layers[3]\n","    act = lm_model.layers[4]\n","\n","    inp_tokens = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"inp_tokens\")\n","    inp_state = tf.keras.layers.Input(shape=(gru_layer.units,), dtype=tf.float32, name=\"inp_state\")\n","\n","    x = emb_layer(inp_tokens)\n","    x, out_state = gru_layer(x, initial_state=inp_state, training=False, return_state=True)\n","    x = dense1(x)\n","    x = dense_out(x)\n","    probs = act(x)\n","\n","    inf_model = tf.keras.Model(inputs=[inp_tokens, inp_state], outputs=[probs, out_state], name=\"lm_inference\")\n","    return inf_model\n","\n","lm_infer = build_inference_model_from_trained(lm, vocab_size)\n","lm_infer.summary()\n"]},{"cell_type":"markdown","id":"c0b44592","metadata":{"id":"c0b44592"},"source":["## 11) Text generation utilities\n","\n","Because tokens are character n-grams, decoding back to text means:\n","- convert token IDs → token strings,\n","- join the token strings directly.\n","\n","I keep a mapping from ID → token using the tokenizer’s `index_word`.  \n","Padding ID 0 is ignored in decoding.\n"]},{"cell_type":"code","execution_count":null,"id":"17efa148","metadata":{"id":"17efa148","executionInfo":{"status":"aborted","timestamp":1767982195955,"user_tz":-420,"elapsed":10249,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["index_word = tokenizer.index_word  # id -> token (includes OOV token)\n","word_index = tokenizer.word_index  # token -> id\n","\n","def ids_to_text(ids):\n","    parts = []\n","    for i in ids:\n","        if i == 0:\n","            continue\n","        tok = index_word.get(int(i), OOV_TOKEN)\n","        parts.append(tok)\n","    return \"\".join(parts)\n","\n","def text_to_ids(text, n=ngrams):\n","    grams = get_ngrams(text.lower(), n)\n","    doc = DELIM.join(grams)\n","    ids = docs_to_sequences_with_minfreq([doc], tokenizer, word_counts, MIN_FREQ, oov_id)[0]\n","    return ids\n","\n","# Quick sanity check: encode -> decode\n","prompt = \"chapter i. \"\n","ids = text_to_ids(prompt)\n","print(\"Prompt:\", prompt)\n","print(\"IDs:\", ids[:20])\n","print(\"Decoded (approx):\", ids_to_text(ids[:20]))\n"]},{"cell_type":"markdown","id":"b8a8869f","metadata":{"id":"b8a8869f"},"source":["## 12) Greedy decoding (next-token recursion)\n","\n","Greedy decoding is straightforward:\n","- start with a prompt,\n","- run the prompt through the inference model to get an initial state,\n","- then repeatedly:\n","  - predict next token distribution,\n","  - pick the highest probability token,\n","  - feed it back as the next input.\n","\n","This is fast, but it can get stuck in repetitive loops if early choices are suboptimal.\n"]},{"cell_type":"code","execution_count":null,"id":"482566cd","metadata":{"id":"482566cd","executionInfo":{"status":"aborted","timestamp":1767982195956,"user_tz":-420,"elapsed":10250,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def greedy_generate(lm_infer, prompt_text, n_steps=200):\n","    ids = text_to_ids(prompt_text)\n","    if len(ids) == 0:\n","        ids = [oov_id]\n","\n","    state = tf.zeros((1, 1024), dtype=tf.float32)\n","\n","    # Prime the model with the whole prompt\n","    inp = tf.constant([ids], dtype=tf.int32)\n","    probs, state = lm_infer([inp, state])\n","    next_id = int(tf.argmax(probs[:, -1, :], axis=-1).numpy()[0])\n","    out_ids = ids + [next_id]\n","\n","    for _ in range(n_steps - 1):\n","        inp = tf.constant([[out_ids[-1]]], dtype=tf.int32)\n","        probs, state = lm_infer([inp, state])\n","        next_id = int(tf.argmax(probs[:, -1, :], axis=-1).numpy()[0])\n","        out_ids.append(next_id)\n","\n","    return ids_to_text(out_ids)\n","\n","generated = greedy_generate(lm_infer, \"chapter i. \", n_steps=250)\n","print(generated[:1500])\n"]},{"cell_type":"markdown","id":"b39d01c3","metadata":{"id":"b39d01c3"},"source":["## 13) Beam search decoding\n","\n","Beam search keeps several candidate continuations instead of only one.  \n","At each step, it expands each candidate by the top-*k* tokens, then keeps the best beams by log-probability.\n","\n","This typically produces more coherent text than greedy decoding, especially when early steps are ambiguous.\n"]},{"cell_type":"code","execution_count":null,"id":"3f6dee93","metadata":{"id":"3f6dee93","executionInfo":{"status":"aborted","timestamp":1767982195956,"user_tz":-420,"elapsed":10249,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def top_k_from_probs(prob_vec, k):\n","    # prob_vec: (V,) numpy\n","    idx = np.argpartition(-prob_vec, k)[:k]\n","    idx = idx[np.argsort(-prob_vec[idx])]\n","    return idx, prob_vec[idx]\n","\n","def beam_search_generate(lm_infer, prompt_text, n_steps=200, beam_width=3):\n","    prompt_ids = text_to_ids(prompt_text)\n","    if len(prompt_ids) == 0:\n","        prompt_ids = [oov_id]\n","\n","    init_state = tf.zeros((1, 1024), dtype=tf.float32)\n","\n","    # Prime the model with the prompt\n","    probs, state = lm_infer([tf.constant([prompt_ids], dtype=tf.int32), init_state])\n","    last_probs = probs[:, -1, :].numpy()[0]\n","\n","    top_ids, top_ps = top_k_from_probs(last_probs, beam_width)\n","\n","    beams = []\n","    for tid, tp in zip(top_ids, top_ps):\n","        beams.append({\n","            \"ids\": prompt_ids + [int(tid)],\n","            \"state\": state,  # same primed state for first step\n","            \"logp\": float(np.log(tp + 1e-12)),\n","        })\n","\n","    for _ in range(n_steps - 1):\n","        candidates = []\n","        for b in beams:\n","            last_id = b[\"ids\"][-1]\n","            probs, new_state = lm_infer([tf.constant([[last_id]], dtype=tf.int32), b[\"state\"]])\n","            p = probs[:, -1, :].numpy()[0]\n","\n","            top_ids, top_ps = top_k_from_probs(p, beam_width)\n","            for tid, tp in zip(top_ids, top_ps):\n","                candidates.append({\n","                    \"ids\": b[\"ids\"] + [int(tid)],\n","                    \"state\": new_state,\n","                    \"logp\": b[\"logp\"] + float(np.log(tp + 1e-12)),\n","                })\n","\n","        # Keep best beams by average log prob (length-normalized)\n","        candidates.sort(key=lambda d: d[\"logp\"] / len(d[\"ids\"]), reverse=True)\n","        beams = candidates[:beam_width]\n","\n","    best = max(beams, key=lambda d: d[\"logp\"] / len(d[\"ids\"]))\n","    return ids_to_text(best[\"ids\"])\n","\n","beam_text = beam_search_generate(lm_infer, \"chapter i. \", n_steps=250, beam_width=3)\n","print(beam_text[:1500])\n"]},{"cell_type":"markdown","id":"1365cee5","metadata":{"id":"1365cee5"},"source":["## 14) Takeaways\n","\n","- Language modeling reframes NLP as a next-token prediction task, which forces the network to learn patterns in sequences.\n","- Character n-grams keep vocabulary manageable and reduce OOV issues, at the cost of expressivity compared with word-level modeling.\n","- A `tf.data` pipeline that creates fixed-length windows is the practical bridge between raw stories and training tensors.\n","- Perplexity is a more informative metric than accuracy for language models because it measures probability quality.\n","- Greedy decoding is simple and fast but can be brittle; beam search often produces more coherent continuations.\n"]},{"cell_type":"markdown","id":"9eeec8c8","metadata":{"id":"9eeec8c8"},"source":["## 15) References\n","\n","- Thushan Ganegedara, *TensorFlow in Action* (Chapter 10).\n","- CBTest dataset archive used in the chapter.\n","- Keras/TensorFlow: `Tokenizer`, `tf.data`, `Embedding`, `GRU`.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}