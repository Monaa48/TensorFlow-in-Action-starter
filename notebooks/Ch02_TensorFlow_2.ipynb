{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monaa48/TensorFlow-in-Action-starter/blob/main/notebooks/Ch02_TensorFlow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4ca705",
      "metadata": {
        "id": "2f4ca705"
      },
      "source": [
        "# Chapter 02 — TensorFlow 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f213ef90",
      "metadata": {
        "id": "f213ef90"
      },
      "source": [
        "## 1) Summary\n",
        "\n",
        "In this chapter I’m trying to get comfortable with **how TensorFlow 2 actually feels to use**:\n",
        "\n",
        "- TF2 runs **eagerly** by default (so it behaves like normal Python).\n",
        "- When I want speed, I can wrap code with `@tf.function` to get a **graph**.\n",
        "- The core “building blocks” that keep showing up are **tensors**, **variables**, and **ops**.\n",
        "- Most deep learning computations are combinations of a few patterns: **matrix multiply**, **convolution**, and **pooling**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352cfeb0",
      "metadata": {
        "id": "352cfeb0"
      },
      "source": [
        "## 2) Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d66add5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d66add5c",
        "outputId": "64958d61-c9c8-42bc-ab99-9d88f147ab0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Eager execution: True\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Eager execution:\", tf.executing_eagerly())\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33971ba7",
      "metadata": {
        "id": "33971ba7"
      },
      "source": [
        "## 3) First steps in TF2 — a small MLP forward pass\n",
        "\n",
        "A basic MLP layer is primarily:\n",
        "\n",
        "\\[\n",
        "h = xW + b\n",
        "\\]\n",
        "\n",
        "and then we apply a non-linearity, like ReLU:\n",
        "\n",
        "\\[\n",
        "\\mathrm{ReLU}(h) = \\max(0, h)\n",
        "\\]\n",
        "\n",
        "Below I’m only doing a **forward pass** . I just want to see shapes and outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "56652df8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56652df8",
        "outputId": "2a95e6f8-0609-4f37-e82c-19275fc0bbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape     : (4, 3)\n",
            "hidden shape: (4, 5)\n",
            "logits shape: (4, 2)\n",
            "probabilities:\n",
            " [[0.4962676  0.50373244]\n",
            " [0.5004868  0.49951315]\n",
            " [0.50065905 0.49934104]\n",
            " [0.5        0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "# A small batch: batch_size=4, input_dim=3\n",
        "x = tf.constant([[0.2, 0.8, -0.5],\n",
        "                 [1.0, -0.2, 0.1],\n",
        "                 [-0.3, 0.4, 0.9],\n",
        "                 [0.0, 0.0, 1.0]], dtype=tf.float32)\n",
        "\n",
        "input_dim = 3\n",
        "hidden_dim = 5\n",
        "num_classes = 2\n",
        "\n",
        "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim], stddev=0.1), name=\"W1\")\n",
        "b1 = tf.Variable(tf.zeros([hidden_dim]), name=\"b1\")\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal([hidden_dim, num_classes], stddev=0.1), name=\"W2\")\n",
        "b2 = tf.Variable(tf.zeros([num_classes]), name=\"b2\")\n",
        "\n",
        "h = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "logits = tf.matmul(h, W2) + b2\n",
        "probs = tf.nn.softmax(logits)\n",
        "\n",
        "print(\"x shape     :\", x.shape)\n",
        "print(\"hidden shape:\", h.shape)\n",
        "print(\"logits shape:\", logits.shape)\n",
        "print(\"probabilities:\\n\", probs.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e00107",
      "metadata": {
        "id": "d1e00107"
      },
      "source": [
        "## 4) Eager vs Graph — `tf.function`\n",
        "\n",
        "In TF2, the key observation is:\n",
        "\n",
        "- In eager mode, I can print intermediate tensors right away.\n",
        "- In graph mode (via `tf.function`), TF traces the function and builds a graph, which can run faster.\n",
        "\n",
        "I will wrap the same forward pass into `tf.function` and peek at the graph ops.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4b7392f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7392f0",
        "outputId": "0a178392-0d4a-4871-8719-1763bf5c55d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits (tf.function):\n",
            " [[-0.01093552  0.00399447]\n",
            " [-0.00042657 -0.00237386]\n",
            " [-0.00115932 -0.00379534]\n",
            " [ 0.          0.        ]]\n",
            "Number of ops in traced graph: 16\n",
            "First ~15 ops: ['x', 'MatMul/ReadVariableOp/resource', 'MatMul/ReadVariableOp', 'MatMul', 'add/ReadVariableOp/resource', 'add/ReadVariableOp', 'add', 'Relu', 'MatMul_1/ReadVariableOp/resource', 'MatMul_1/ReadVariableOp', 'MatMul_1', 'add_1/ReadVariableOp/resource', 'add_1/ReadVariableOp', 'add_1', 'Identity']\n"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def forward(x):\n",
        "    h = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    logits = tf.matmul(h, W2) + b2\n",
        "    return logits\n",
        "\n",
        "logits_graph = forward(x)\n",
        "print(\"Logits (tf.function):\\n\", logits_graph.numpy())\n",
        "\n",
        "concrete = forward.get_concrete_function(tf.TensorSpec([None, input_dim], tf.float32))\n",
        "ops = [op.name for op in concrete.graph.get_operations()]\n",
        "print(\"Number of ops in traced graph:\", len(ops))\n",
        "print(\"First ~15 ops:\", ops[:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24ddb67",
      "metadata": {
        "id": "b24ddb67"
      },
      "source": [
        "## 5) Core TF2 components\n",
        "\n",
        "### 5.1 Tensors vs Variables\n",
        "\n",
        "- `tf.Tensor` = an **immutable value** produced by ops. It’s like “the result”.\n",
        "- `tf.Variable` = **mutable state** (usually weights/biases). Optimizers update variables.\n",
        "\n",
        "### 5.2 Automatic differentiation with `GradientTape`\n",
        "\n",
        "To train, I need gradients. `tf.GradientTape()` records ops so TF can compute derivatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e523e886",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e523e886",
        "outputId": "1fd1292e-3e98-4ae6-cd33-231c3bb9684d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: 25.0\n",
            "dy/dw: 20.0\n",
            "dy/db: 10.0\n",
            "Updated w, b: 1.0 -2.0\n"
          ]
        }
      ],
      "source": [
        "w = tf.Variable(3.0)\n",
        "b = tf.Variable(-1.0)\n",
        "x_scalar = tf.constant(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = (w * x_scalar + b) ** 2\n",
        "\n",
        "dw, db = tape.gradient(y, [w, b])\n",
        "print(\"y:\", float(y.numpy()))\n",
        "print(\"dy/dw:\", float(dw.numpy()))\n",
        "print(\"dy/db:\", float(db.numpy()))\n",
        "\n",
        "# One manual gradient descent step\n",
        "lr = 0.1\n",
        "w.assign_sub(lr * dw)\n",
        "b.assign_sub(lr * db)\n",
        "print(\"Updated w, b:\", float(w.numpy()), float(b.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6eb319",
      "metadata": {
        "id": "2f6eb319"
      },
      "source": [
        "## 6) Common neural network computations\n",
        "\n",
        "### 6.1 Matrix multiplication (Dense layers)\n",
        "\n",
        "Dense layers are primarily `matmul + bias + activation`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "373e9bc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "373e9bc0",
        "outputId": "53bf2531-6496-4a64-b80d-d4897028d36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            " [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "B:\n",
            " [[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 1.]]\n",
            "A @ B:\n",
            " [[ 4.  5.]\n",
            " [10. 11.]]\n"
          ]
        }
      ],
      "source": [
        "A = tf.constant([[1., 2., 3.],\n",
        "                 [4., 5., 6.]], dtype=tf.float32)  # (2, 3)\n",
        "B = tf.constant([[1., 0.],\n",
        "                 [0., 1.],\n",
        "                 [1., 1.]], dtype=tf.float32)      # (3, 2)\n",
        "\n",
        "C = tf.matmul(A, B)  # (2, 2)\n",
        "print(\"A:\\n\", A.numpy())\n",
        "print(\"B:\\n\", B.numpy())\n",
        "print(\"A @ B:\\n\", C.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397619b2",
      "metadata": {
        "id": "397619b2"
      },
      "source": [
        "### 6.2 Convolution (toy example)\n",
        "\n",
        "A convolution slides a small filter across an image-like grid and produces feature maps.\n",
        "Here I use a 5×5 “image” and a 3×3 filter so I can literally see the numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3b3af546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b3af546",
        "outputId": "af15c42e-875c-4e33-a321-8f481105364c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image (5x5):\n",
            " [[ 0.  1.  2.  3.  4.]\n",
            " [ 5.  6.  7.  8.  9.]\n",
            " [10. 11. 12. 13. 14.]\n",
            " [15. 16. 17. 18. 19.]\n",
            " [20. 21. 22. 23. 24.]]\n",
            "Kernel (3x3):\n",
            " [[ 1.  0. -1.]\n",
            " [ 1.  0. -1.]\n",
            " [ 1.  0. -1.]]\n",
            "Conv output (3x3):\n",
            " [[-6. -6. -6.]\n",
            " [-6. -6. -6.]\n",
            " [-6. -6. -6.]]\n"
          ]
        }
      ],
      "source": [
        "# Conv2D input format: [batch, height, width, channels]\n",
        "img = tf.reshape(tf.range(25, dtype=tf.float32), [1, 5, 5, 1])\n",
        "\n",
        "# 3x3 filter: simple left-right edge-ish pattern\n",
        "kernel = tf.constant([[[[1.]], [[0.]], [[-1.]]],\n",
        "                      [[[1.]], [[0.]], [[-1.]]],\n",
        "                      [[[1.]], [[0.]], [[-1.]]]], dtype=tf.float32)  # [3,3,1,1]\n",
        "\n",
        "out = tf.nn.conv2d(img, kernel, strides=1, padding=\"VALID\")\n",
        "\n",
        "print(\"Input image (5x5):\\n\", tf.squeeze(img).numpy())\n",
        "print(\"Kernel (3x3):\\n\", tf.squeeze(kernel).numpy())\n",
        "print(\"Conv output (3x3):\\n\", tf.squeeze(out).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf17c09",
      "metadata": {
        "id": "faf17c09"
      },
      "source": [
        "### 6.3 Pooling (downsampling)\n",
        "\n",
        "Pooling reduces spatial size. Max pooling keeps the maximum value in each window.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a845a919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a845a919",
        "outputId": "9ee4d742-de76-4c1a-8037-de53af2d754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaxPool output shape: (1, 2, 2, 1)\n",
            "MaxPool output:\n",
            " [[ 6.  8.]\n",
            " [16. 18.]]\n"
          ]
        }
      ],
      "source": [
        "pooled = tf.nn.max_pool2d(img, ksize=2, strides=2, padding=\"VALID\")\n",
        "print(\"MaxPool output shape:\", pooled.shape)\n",
        "print(\"MaxPool output:\\n\", tf.squeeze(pooled).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1457722a",
      "metadata": {
        "id": "1457722a"
      },
      "source": [
        "## 7) Takeaways\n",
        "\n",
        "- TF2 feels “Python-first” because of eager execution.\n",
        "- `tf.function` is useful once code is stable and I want performance.\n",
        "- Variables are the trainable parts; tensors are the values flowing through the graph.\n",
        "- GradientTape is the core tool that makes training possible.\n",
        "- The math patterns I keep seeing: matmul (MLP), conv + pooling (CNN).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}