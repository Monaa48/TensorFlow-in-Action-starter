{"cells":[{"cell_type":"markdown","id":"2494153a","metadata":{"id":"2494153a"},"source":["# Chapter 11 — Sequence-to-sequence learning: Part 1\n","\n","This chapter moves from *single-output* NLP tasks (classification and language modeling) into **sequence-to-sequence (seq2seq)** learning, where:\n","\n","- the input is a sequence (e.g., an English sentence),\n","- the output is another sequence (e.g., a German sentence),\n","- and input/output lengths can differ.\n","\n","The main example is **English → German machine translation** using an **encoder–decoder** architecture with GRU layers. I reproduce the workflow from the book:\n","\n","1. Download and inspect a translation dataset (`deu.txt` from `deu-eng.zip`).\n","2. Prepare the data for teacher forcing (decoder inputs vs. decoder labels).\n","3. Build an end-to-end model that accepts **raw strings** and performs tokenization internally using `TextVectorization`.\n","4. Train and evaluate using **masked accuracy** and a **BLEU-style** metric.\n","5. Repurpose the trained model into an **inference model** that generates tokens recursively (starting from `sos` until `eos`).\n","\n","> Notes for Colab: This notebook is written to run in Google Colab (CPU or GPU).  \n","> If training is slow, reduce `N_SAMPLES` and/or `EPOCHS` in the training section.\n"]},{"cell_type":"markdown","id":"0a0464db","metadata":{"id":"0a0464db"},"source":["## 0) Setup\n","\n","I keep the setup minimal: TensorFlow 2.x + standard Python libraries.\n","\n","The dataset is small enough to download directly inside the notebook.\n"]},{"cell_type":"code","execution_count":1,"id":"ddabf684","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddabf684","executionInfo":{"status":"ok","timestamp":1767982271712,"user_tz":-420,"elapsed":9290,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"fd0d58b9-c12b-47f3-9e77-c98733d1530a"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow: 2.19.0\n"]}],"source":["# Core\n","import os\n","import re\n","import zipfile\n","from pathlib import Path\n","\n","# Data / math\n","import numpy as np\n","import pandas as pd\n","\n","# TensorFlow\n","import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# Reproducibility\n","np.random.seed(4321)\n","tf.random.set_seed(4321)\n","\n","print(\"TensorFlow:\", tf.__version__)\n"]},{"cell_type":"markdown","id":"2740b282","metadata":{"id":"2740b282"},"source":["## 1) Download and load the English–German dataset\n","\n","The chapter uses a tab-separated text file (`deu.txt`) where each row has:\n","\n","- English sentence (source)\n","- German sentence (target)\n","- Attribution metadata (we ignore this for modeling)\n","\n","The file is distributed inside `deu-eng.zip`.\n"]},{"cell_type":"code","execution_count":3,"id":"4e6652fd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"4e6652fd","executionInfo":{"status":"error","timestamp":1767982274260,"user_tz":-420,"elapsed":7,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"a175b2a0-edc5-410a-e000-a4aa836b83d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: http://www.manythings.org/anki/deu-eng.zip\n"]},{"output_type":"error","ename":"ValueError","evalue":"Paths are no longer accepted as the `fname` argument. To specify the file's parent directory, use the `cache_dir` argument. Received: fname=data/deu-eng.zip","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-968548440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text file already exists:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exists?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTXT_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-968548440.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(url, zip_path, txt_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mzip_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         tf.keras.utils.get_file(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/file_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"Paths are no longer accepted as the `fname` argument. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"To specify the file's parent directory, use \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Paths are no longer accepted as the `fname` argument. To specify the file's parent directory, use the `cache_dir` argument. Received: fname=data/deu-eng.zip"]}],"source":["DATA_DIR = Path(\"data\")\n","DATA_DIR.mkdir(exist_ok=True)\n","\n","ZIP_PATH = DATA_DIR / \"deu-eng.zip\"\n","TXT_PATH = DATA_DIR / \"deu.txt\"\n","\n","URL = \"http://www.manythings.org/anki/deu-eng.zip\"\n","\n","def download_and_extract(url=URL, zip_path=ZIP_PATH, txt_path=TXT_PATH):\n","    if not zip_path.exists():\n","        print(\"Downloading:\", url)\n","        tf.keras.utils.get_file(\n","            fname=str(zip_path),\n","            origin=url,\n","            cache_dir=\".\",\n","            cache_subdir=\"\",\n","        )\n","    else:\n","        print(\"Zip already exists:\", zip_path)\n","\n","    if not txt_path.exists():\n","        print(\"Extracting zip...\")\n","        with zipfile.ZipFile(zip_path, \"r\") as zf:\n","            zf.extractall(DATA_DIR)\n","    else:\n","        print(\"Text file already exists:\", txt_path)\n","\n","download_and_extract()\n","\n","print(\"Exists?\", TXT_PATH.exists())\n"]},{"cell_type":"code","execution_count":null,"id":"1be4099e","metadata":{"id":"1be4099e","executionInfo":{"status":"aborted","timestamp":1767982271738,"user_tz":-420,"elapsed":7,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# Load the raw file into a DataFrame\n","df = pd.read_csv(TXT_PATH, delimiter=\"\\t\", header=None)\n","df.columns = [\"EN\", \"DE\", \"Attribution\"]\n","df = df[[\"EN\", \"DE\"]]\n","\n","print(\"df.shape =\", df.shape)\n","df.head()\n"]},{"cell_type":"markdown","id":"e8995938","metadata":{"id":"e8995938"},"source":["## 2) Cleaning, sampling, and adding `sos` / `eos`\n","\n","### Why `sos` and `eos`?\n","For sequence generation, it helps to mark:\n","\n","- `sos`: *start of sequence* (the first input token to the decoder during inference)\n","- `eos`: *end of sequence* (stopping condition during inference)\n","\n","During training, I insert these tokens into every German sentence so that the model learns the same “format” it will face at inference time.\n","\n","### Sampling\n","To keep training practical, the chapter uses a subset of the full dataset. I do the same using `N_SAMPLES`.\n"]},{"cell_type":"code","execution_count":null,"id":"de284412","metadata":{"id":"de284412","executionInfo":{"status":"aborted","timestamp":1767982271740,"user_tz":-420,"elapsed":3,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# Basic filtering: remove rows that can cause odd encoding artifacts\n","# (The book mentions filtering out entries containing certain problematic byte patterns.)\n","def has_bad_bytes(s: str) -> bool:\n","    try:\n","        return b\"\\xc2\" in s.encode(\"utf-8\")\n","    except Exception:\n","        return True\n","\n","mask = ~df[\"DE\"].apply(has_bad_bytes)\n","df = df[mask].reset_index(drop=True)\n","\n","print(\"After filtering:\", df.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"17f6a8f7","metadata":{"id":"17f6a8f7","executionInfo":{"status":"aborted","timestamp":1767982271742,"user_tz":-420,"elapsed":3,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["N_SAMPLES = 50000\n","df_sample = df.sample(n=min(N_SAMPLES, len(df)), random_state=4321).reset_index(drop=True)\n","\n","start_token = \"sos\"\n","end_token = \"eos\"\n","\n","df_sample[\"DE\"] = start_token + \" \" + df_sample[\"DE\"].astype(str) + \" \" + end_token\n","\n","df_sample.head()\n"]},{"cell_type":"code","execution_count":null,"id":"ab0afff5","metadata":{"id":"ab0afff5","executionInfo":{"status":"aborted","timestamp":1767982271744,"user_tz":-420,"elapsed":1,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# Train/valid/test split (80/10/10)\n","n = len(df_sample)\n","n_train = int(0.8 * n)\n","n_valid = int(0.1 * n)\n","\n","train_df = df_sample.iloc[:n_train].reset_index(drop=True)\n","valid_df = df_sample.iloc[n_train:n_train+n_valid].reset_index(drop=True)\n","test_df  = df_sample.iloc[n_train+n_valid:].reset_index(drop=True)\n","\n","print(\"train:\", train_df.shape, \"valid:\", valid_df.shape, \"test:\", test_df.shape)\n"]},{"cell_type":"markdown","id":"8597f899","metadata":{"id":"8597f899"},"source":["## 3) Quick EDA: vocabulary size and sequence lengths\n","\n","The book uses two practical checks:\n","\n","1. **Vocabulary size above a frequency threshold**  \n","   This gives a sense of how many tokens the model will need to handle.\n","\n","2. **Sequence length statistics**  \n","   This helps pick a reasonable maximum sequence length for padding/truncation.\n","\n","These are not strict rules, but they are useful for selecting hyperparameters in a controlled way.\n"]},{"cell_type":"code","execution_count":null,"id":"a8469b81","metadata":{"id":"a8469b81","executionInfo":{"status":"aborted","timestamp":1767982271751,"user_tz":-420,"elapsed":9362,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from collections import Counter\n","\n","def vocabulary_stats(series: pd.Series, min_freq: int = 10, top_k: int = 10):\n","    words = series.str.split().sum()\n","    counter = Counter(words)\n","    freq = pd.Series(counter).sort_values(ascending=False)\n","\n","    print(\"=\"*50)\n","    print(\"Top tokens\")\n","    print(freq.head(top_k))\n","    vocab_size = int((freq >= min_freq).sum())\n","    print(f\"Vocabulary size (>= {min_freq} frequent): {vocab_size}\")\n","    return vocab_size, freq\n","\n","en_vocab, en_freq = vocabulary_stats(train_df[\"EN\"], min_freq=10, top_k=10)\n","de_vocab, de_freq = vocabulary_stats(train_df[\"DE\"], min_freq=10, top_k=10)\n"]},{"cell_type":"code","execution_count":null,"id":"3b2a9f8d","metadata":{"id":"3b2a9f8d","executionInfo":{"status":"aborted","timestamp":1767982271753,"user_tz":-420,"elapsed":9363,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def sequence_length_stats(series: pd.Series, name: str, q_low=0.01, q_high=0.99):\n","    lengths = series.str.split().apply(len)\n","    print(\"=\"*50)\n","    print(name)\n","    print(\"Median length:\", float(lengths.median()))\n","    print(lengths.describe())\n","\n","    lo = int(lengths.quantile(q_low))\n","    hi = int(lengths.quantile(q_high))\n","    trimmed = lengths[(lengths >= lo) & (lengths <= hi)]\n","    print(f\"Between {int(q_low*100)}% and {int(q_high*100)}% quantiles (ignore outliers)\")\n","    print(trimmed.describe())\n","    return lengths, lo, hi\n","\n","en_lens, en_lo, en_hi = sequence_length_stats(train_df[\"EN\"], \"English (EN)\")\n","de_lens, de_lo, de_hi = sequence_length_stats(train_df[\"DE\"], \"German (DE) with sos/eos\")\n"]},{"cell_type":"markdown","id":"07ab3a5d","metadata":{"id":"07ab3a5d"},"source":["### Choosing maximum sequence lengths\n","\n","In the chapter, the chosen maximum lengths are:\n","\n","- English max length: **19**\n","- German max length: **21**\n","\n","German includes `sos` and `eos`, so it is slightly longer.\n","\n","I keep these values because they work well with the statistics above and match the book’s workflow.\n"]},{"cell_type":"code","execution_count":null,"id":"aa9dbe37","metadata":{"id":"aa9dbe37","executionInfo":{"status":"aborted","timestamp":1767982271755,"user_tz":-420,"elapsed":9365,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["en_seq_length = 19\n","de_seq_length = 21\n","\n","print(\"EN vocabulary size:\", en_vocab)\n","print(\"DE vocabulary size:\", de_vocab)\n","print(\"EN max sequence length:\", en_seq_length)\n","print(\"DE max sequence length:\", de_seq_length)\n"]},{"cell_type":"markdown","id":"7b08df19","metadata":{"id":"7b08df19"},"source":["## 4) TextVectorization inside the model\n","\n","Previously, tokenization was treated as a preprocessing step. Here the model is more end-to-end:\n","\n","- Input: raw strings\n","- Inside the model: `TextVectorization` converts strings → integer token IDs\n","- Then: embedding layer converts IDs → trainable word vectors\n","\n","This makes the model easier to use at inference time, because it accepts raw text directly.\n"]},{"cell_type":"code","execution_count":null,"id":"66648d8f","metadata":{"id":"66648d8f","executionInfo":{"status":"aborted","timestamp":1767982271757,"user_tz":-420,"elapsed":9366,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def get_vectorizer(corpus: np.ndarray, n_vocab: int, max_length=None, name=\"vectorizer\"):\n","    \"\"\"Return a (string -> token IDs) model and optionally its vocabulary.\"\"\"\n","    inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n","    vectorize_layer = TextVectorization(\n","        max_tokens=n_vocab + 2,  # +2 for '' and [UNK]\n","        output_mode=\"int\",\n","        output_sequence_length=max_length,\n","    )\n","    vectorize_layer.adapt(corpus)\n","    vectorized_out = vectorize_layer(inp)\n","\n","    return tf.keras.models.Model(inp, vectorized_out, name=name), vectorize_layer.get_vocabulary()\n","\n","# English vectorizer (encoder input)\n","en_vectorizer, en_vocabulary = get_vectorizer(\n","    corpus=np.array(train_df[\"EN\"].tolist()),\n","    n_vocab=en_vocab,\n","    max_length=en_seq_length,\n","    name=\"en_vectorizer\",\n",")\n","\n","# German vectorizer (decoder input / labels are length de_seq_length-1 after shifting)\n","de_vectorizer, de_vocabulary = get_vectorizer(\n","    corpus=np.array(train_df[\"DE\"].tolist()),\n","    n_vocab=de_vocab,\n","    max_length=de_seq_length - 1,\n","    name=\"d_vectorizer\",\n",")\n","\n","print(\"EN vocab size (with special tokens):\", len(en_vocabulary))\n","print(\"DE vocab size (with special tokens):\", len(de_vocabulary))\n","print(\"Special tokens (start):\", de_vocabulary[:5])\n"]},{"cell_type":"markdown","id":"a07fc52a","metadata":{"id":"a07fc52a"},"source":["## 5) Building the encoder\n","\n","The encoder consumes the **English** sequence and produces a single vector representation (context vector).\n","\n","Architecture (matching the chapter):\n","- `TextVectorization` (English)\n","- `Embedding` (128-dim)\n","- `Bidirectional(GRU(128))`\n","\n","Because the bidirectional GRU concatenates forward and backward representations, the final context vector has size **256**.\n"]},{"cell_type":"code","execution_count":null,"id":"9bc81e5a","metadata":{"id":"9bc81e5a","executionInfo":{"status":"aborted","timestamp":1767982271758,"user_tz":-420,"elapsed":9367,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def get_encoder(n_vocab: int, vectorizer: tf.keras.Model):\n","    \"\"\"Define the encoder of the seq2seq model.\"\"\"\n","    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"e_input\")\n","    vectorized_out = vectorizer(inp)\n","\n","    emb_layer = tf.keras.layers.Embedding(\n","        n_vocab + 2, 128, mask_zero=True, name=\"e_embedding\"\n","    )\n","    emb_out = emb_layer(vectorized_out)\n","\n","    gru_layer = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.GRU(128), name=\"e_bi_gru\"\n","    )\n","    gru_out = gru_layer(emb_out)\n","\n","    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name=\"encoder\")\n","    return encoder\n","\n","encoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n","encoder.summary()\n"]},{"cell_type":"markdown","id":"86048665","metadata":{"id":"86048665"},"source":["## 6) Building the decoder + final seq2seq model (teacher forcing)\n","\n","During training, the decoder receives **ground-truth target tokens** (teacher forcing).  \n","It learns to predict the **next** token at each step.\n","\n","So I construct:\n","- decoder inputs: target sentence **without the last token**\n","- decoder labels: target sentence **without the first token**\n","\n","Example target sentence:\n","`\"sos ich möchte ... eos\"`\n","\n","- decoder inputs: `\"sos ich möchte ...\"`\n","- decoder labels: `\"ich möchte ... eos\"`\n","\n","Decoder architecture:\n","- `TextVectorization` (German)\n","- `Embedding` (128-dim)\n","- `GRU(256, return_sequences=True)` initialized with the encoder context vector\n","- `Dense(512, relu)`\n","- `Dense(vocab_size, softmax)` producing a probability distribution at each time step\n"]},{"cell_type":"code","execution_count":null,"id":"fefefca9","metadata":{"id":"fefefca9","executionInfo":{"status":"aborted","timestamp":1767982271759,"user_tz":-420,"elapsed":9367,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def get_final_seq2seq_model(n_vocab: int, encoder: tf.keras.Model, vectorizer: tf.keras.Model):\n","    \"\"\"Define the final encoder–decoder model.\"\"\"\n","    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"e_input_final\")\n","    d_init_state = encoder(e_inp)\n","\n","    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"d_input\")\n","    d_vectorized_out = vectorizer(d_inp)\n","\n","    d_emb_layer = tf.keras.layers.Embedding(\n","        n_vocab + 2, 128, mask_zero=True, name=\"d_embedding\"\n","    )\n","    d_emb_out = d_emb_layer(d_vectorized_out)\n","\n","    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name=\"d_gru\")\n","    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n","\n","    d_dense_layer_1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"d_dense_1\")\n","    d_dense1_out = d_dense_layer_1(d_gru_out)\n","\n","    d_dense_layer_final = tf.keras.layers.Dense(\n","        n_vocab + 2, activation=\"softmax\", name=\"d_dense_final\"\n","    )\n","    d_final_out = d_dense_layer_final(d_dense1_out)\n","\n","    seq2seq = tf.keras.models.Model(\n","        inputs=[e_inp, d_inp], outputs=d_final_out, name=\"final_seq2seq\"\n","    )\n","    return seq2seq\n","\n","final_model = get_final_seq2seq_model(n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer)\n","final_model.summary()\n"]},{"cell_type":"markdown","id":"b935039e","metadata":{"id":"b935039e"},"source":["## 7) Compiling with masked loss / masked accuracy\n","\n","Both decoder labels and predictions are padded to a fixed length (`de_seq_length-1`).\n","\n","Padding tokens should not contribute to:\n","- cross-entropy loss\n","- accuracy\n","\n","So I use masked versions of both.\n"]},{"cell_type":"code","execution_count":null,"id":"40135bf4","metadata":{"id":"40135bf4","executionInfo":{"status":"aborted","timestamp":1767982271760,"user_tz":-420,"elapsed":9368,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n","\n","def masked_loss(y_true, y_pred):\n","    # y_true: (batch, time), y_pred: (batch, time, vocab)\n","    loss = loss_obj(y_true, y_pred)\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)  # 0 is padding\n","    loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n","    return loss\n","\n","def masked_accuracy(y_true, y_pred):\n","    y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n","    matches = tf.cast(tf.equal(y_true, y_pred_ids), tf.float32)\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","    return tf.reduce_sum(matches * mask) / tf.reduce_sum(mask)\n","\n","final_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=masked_loss,\n","    metrics=[masked_accuracy],\n",")\n"]},{"cell_type":"markdown","id":"40fb11dc","metadata":{"id":"40fb11dc"},"source":["## 8) Preparing data for teacher forcing\n","\n","The model inputs are raw strings shaped like `(N, 1)`:\n","\n","- encoder input: English sentence\n","- decoder input: German sentence **without the last token**\n","- decoder label: German sentence **without the first token** (vectorized during training/eval)\n","\n","I keep these as raw strings and let `TextVectorization` inside the model handle the tokenization.\n"]},{"cell_type":"code","execution_count":null,"id":"37d1cbb7","metadata":{"id":"37d1cbb7","executionInfo":{"status":"aborted","timestamp":1767982271761,"user_tz":-420,"elapsed":9369,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def shift_target_sequence(s: str):\n","    tokens = s.split()\n","    # input: drop last token, label: drop first token\n","    de_in = \" \".join(tokens[:-1])\n","    de_out = \" \".join(tokens[1:])\n","    return de_in, de_out\n","\n","def prepare_data(train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame):\n","    data = {}\n","    for name, dfi in [(\"train\", train_df), (\"valid\", valid_df), (\"test\", test_df)]:\n","        en_inputs = dfi[\"EN\"].astype(str).values.reshape(-1, 1)\n","        de_in_list = []\n","        de_out_list = []\n","        for s in dfi[\"DE\"].astype(str).tolist():\n","            de_in, de_out = shift_target_sequence(s)\n","            de_in_list.append(de_in)\n","            de_out_list.append(de_out)\n","\n","        de_inputs = np.array(de_in_list, dtype=object).reshape(-1, 1)\n","        de_labels = np.array(de_out_list, dtype=object).reshape(-1, 1)\n","\n","        data[name] = {\n","            \"encoder_inputs\": en_inputs,\n","            \"decoder_inputs\": de_inputs,\n","            \"decoder_labels\": de_labels,\n","        }\n","    return data\n","\n","data_dict = prepare_data(train_df, valid_df, test_df)\n","{k: {kk: v.shape for kk, v in data_dict[k].items()} for k in data_dict}\n"]},{"cell_type":"code","execution_count":null,"id":"b900f1c9","metadata":{"id":"b900f1c9","executionInfo":{"status":"aborted","timestamp":1767982271762,"user_tz":-420,"elapsed":9369,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None):\n","    if shuffle_indices is None:\n","        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))\n","    else:\n","        shuffle_indices = np.random.permutation(shuffle_indices)\n","\n","    return (\n","        en_inputs[shuffle_indices],\n","        de_inputs[shuffle_indices],\n","        de_labels[shuffle_indices],\n","        shuffle_indices,\n","    )\n"]},{"cell_type":"markdown","id":"79e09c9d","metadata":{"id":"79e09c9d"},"source":["## 9) BLEU-style metric for translation quality\n","\n","Accuracy checks whether each predicted token equals the target token.  \n","That is useful, but it can be misleading because translation quality depends on **phrases**, not just isolated words.\n","\n","BLEU addresses this by measuring overlap of n-grams between candidate and reference translations (plus a brevity penalty).\n","\n","Here, I compute a lightweight BLEU-style score:\n","\n","- convert predictions to token sequences\n","- remove padding and cut off at `eos`\n","- compute BLEU-4 with modified n-gram precision and brevity penalty\n"]},{"cell_type":"code","execution_count":null,"id":"1dca8a57","metadata":{"id":"1dca8a57","executionInfo":{"status":"aborted","timestamp":1767982271763,"user_tz":-420,"elapsed":9370,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def _ngrams(tokens, n):\n","    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n","\n","def _modified_precision(reference, candidate, n):\n","    ref_ngrams = Counter(_ngrams(reference, n))\n","    cand_ngrams = Counter(_ngrams(candidate, n))\n","    if len(cand_ngrams) == 0:\n","        return 0.0\n","\n","    clipped = 0\n","    total = 0\n","    for ng, count in cand_ngrams.items():\n","        clipped += min(count, ref_ngrams.get(ng, 0))\n","        total += count\n","    return clipped / total if total > 0 else 0.0\n","\n","def bleu_score_single(reference_tokens, candidate_tokens, max_n=4, smooth=1e-9):\n","    # Brevity penalty\n","    ref_len = len(reference_tokens)\n","    cand_len = len(candidate_tokens)\n","    if cand_len == 0:\n","        return 0.0\n","\n","    bp = 1.0 if cand_len > ref_len else np.exp(1.0 - (ref_len / (cand_len + smooth)))\n","\n","    precisions = []\n","    for n in range(1, max_n+1):\n","        p = _modified_precision(reference_tokens, candidate_tokens, n)\n","        precisions.append(max(p, smooth))\n","\n","    # geometric mean\n","    log_p = sum((1.0/max_n) * np.log(p) for p in precisions)\n","    return float(bp * np.exp(log_p))\n","\n","def clean_text(token_tensor, end_token=\"eos\"):\n","    \"\"\"Convert a (batch, time) token tensor to a list of token lists, cut at eos, drop padding and sos.\"\"\"\n","    # token_tensor is usually dtype=string/bytes\n","    tokens = token_tensor.numpy()\n","    out = []\n","    for row in tokens:\n","        row_tokens = []\n","        for tok in row:\n","            if isinstance(tok, bytes):\n","                tok = tok.decode(\"utf-8\")\n","            tok = tok.strip()\n","            if tok == \"\" or tok == start_token:\n","                continue\n","            if tok == end_token:\n","                break\n","            row_tokens.append(tok)\n","        out.append(row_tokens)\n","    return out\n","\n","class BLEUMetric:\n","    def __init__(self, vocabulary):\n","        self.vocab = vocabulary\n","        self.id_to_token = tf.keras.layers.StringLookup(\n","            vocabulary=self.vocab, invert=True, num_oov_indices=0\n","        )\n","\n","    def calculate_bleu_from_predictions(self, y_true_ids, y_pred_probs):\n","        # Argmax IDs\n","        pred_ids = tf.argmax(y_pred_probs, axis=-1, output_type=tf.int32)\n","\n","        # IDs -> tokens\n","        pred_tokens = self.id_to_token(pred_ids)\n","        true_tokens = self.id_to_token(y_true_ids)\n","\n","        pred_clean = clean_text(pred_tokens)\n","        true_clean = clean_text(true_tokens)\n","\n","        scores = []\n","        for ref, cand in zip(true_clean, pred_clean):\n","            scores.append(bleu_score_single(ref, cand))\n","        return float(np.mean(scores)) if scores else 0.0\n","\n","bleu_metric = BLEUMetric(de_vocabulary)\n"]},{"cell_type":"markdown","id":"f49297a3","metadata":{"id":"f49297a3"},"source":["## 10) Training and evaluation loop\n","\n","The book uses a custom loop to make evaluation explicit and to include BLEU in the reporting.\n","\n","I follow the same idea:\n","\n","- `evaluate_model(...)`: loops over batches and returns mean loss / masked accuracy / BLEU\n","- `train_model(...)`: trains epoch-by-epoch, shuffles the data each epoch, and reports train + validation metrics\n"]},{"cell_type":"code","execution_count":null,"id":"ec61fe18","metadata":{"id":"ec61fe18","executionInfo":{"status":"aborted","timestamp":1767982271776,"user_tz":-420,"elapsed":9383,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def evaluate_model(model, vectorizer, data_split, batch_size=128, bleu_metric=None):\n","    en_inputs_raw = data_split[\"encoder_inputs\"]\n","    de_inputs_raw = data_split[\"decoder_inputs\"]\n","    de_labels_raw = data_split[\"decoder_labels\"]\n","\n","    loss_log, acc_log, bleu_log = [], [], []\n","    n_batches = en_inputs_raw.shape[0] // batch_size\n","    if n_batches == 0:\n","        n_batches = 1\n","\n","    for i in range(n_batches):\n","        x = [\n","            en_inputs_raw[i*batch_size:(i+1)*batch_size],\n","            de_inputs_raw[i*batch_size:(i+1)*batch_size],\n","        ]\n","        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n","\n","        # Keras returns [loss, metric...]\n","        eval_out = model.evaluate(x, y, verbose=0)\n","        loss_log.append(eval_out[0])\n","        acc_log.append(eval_out[1])\n","\n","        if bleu_metric is not None:\n","            pred_y = model.predict(x, verbose=0)\n","            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n","            bleu_log.append(bleu)\n","\n","    mean_loss = float(np.mean(loss_log))\n","    mean_acc = float(np.mean(acc_log))\n","    mean_bleu = float(np.mean(bleu_log)) if bleu_log else 0.0\n","    return mean_loss, mean_acc, mean_bleu\n","\n","def train_model(model, vectorizer, data_dict, epochs=5, batch_size=128):\n","    train_split = data_dict[\"train\"]\n","    valid_split = data_dict[\"valid\"]\n","\n","    en_inputs_raw = train_split[\"encoder_inputs\"]\n","    de_inputs_raw = train_split[\"decoder_inputs\"]\n","    de_labels_raw = train_split[\"decoder_labels\"]\n","\n","    prev_shuffle = None\n","\n","    for epoch in range(1, epochs+1):\n","        # Shuffle at the beginning of each epoch\n","        en_inputs_raw, de_inputs_raw, de_labels_raw, prev_shuffle = shuffle_data(\n","            en_inputs_raw, de_inputs_raw, de_labels_raw, prev_shuffle\n","        )\n","\n","        n_batches = en_inputs_raw.shape[0] // batch_size\n","        if n_batches == 0:\n","            n_batches = 1\n","\n","        for i in range(n_batches):\n","            x = [\n","                en_inputs_raw[i*batch_size:(i+1)*batch_size],\n","                de_inputs_raw[i*batch_size:(i+1)*batch_size],\n","            ]\n","            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n","\n","            model.train_on_batch(x, y)\n","\n","        # End-of-epoch evaluation (train + valid)\n","        train_loss, train_acc, train_bleu = evaluate_model(\n","            model, vectorizer, {\"encoder_inputs\": en_inputs_raw, \"decoder_inputs\": de_inputs_raw, \"decoder_labels\": de_labels_raw},\n","            batch_size=batch_size,\n","            bleu_metric=bleu_metric\n","        )\n","        valid_loss, valid_acc, valid_bleu = evaluate_model(\n","            model, vectorizer, valid_split,\n","            batch_size=batch_size,\n","            bleu_metric=bleu_metric\n","        )\n","\n","        print(f\"Epoch {epoch}/{epochs}\")\n","        print(f\"  (train) loss: {train_loss:.4f} - acc: {train_acc:.4f} - bleu: {train_bleu:.6f}\")\n","        print(f\"  (valid) loss: {valid_loss:.4f} - acc: {valid_acc:.4f} - bleu: {valid_bleu:.6f}\")\n","\n","# Default hyperparameters from the chapter\n","EPOCHS = 5\n","BATCH_SIZE = 128\n","\n","# Uncomment to train:\n","# train_model(final_model, de_vectorizer, data_dict, epochs=EPOCHS, batch_size=BATCH_SIZE)\n"]},{"cell_type":"markdown","id":"7f0cfba4","metadata":{"id":"7f0cfba4"},"source":["## 11) From training to inference: a recursive decoder\n","\n","Teacher forcing trains the decoder using the full target sequence.  \n","At inference time, the full German sentence is unknown — that is what the model must generate.\n","\n","So I create an **inference decoder** that:\n","- takes a single token (string) and the previous GRU state,\n","- outputs the next-token distribution and the updated state.\n","\n","Then translation works like:\n","1. Encode the English sentence → context vector (initial state).\n","2. Start decoder input with `sos`.\n","3. Predict next token, feed it back in, repeat until `eos` or max steps.\n"]},{"cell_type":"code","execution_count":null,"id":"3427eee5","metadata":{"id":"3427eee5","executionInfo":{"status":"aborted","timestamp":1767982271777,"user_tz":-420,"elapsed":9384,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def build_inference_decoder(trained_model, de_vocabulary, de_vocab_size):\n","    # 1) A vectorizer that maps a *single token* -> a *single ID*\n","    token_vectorizer = TextVectorization(\n","        max_tokens=de_vocab_size + 2,\n","        output_mode=\"int\",\n","        output_sequence_length=1,\n","    )\n","    token_vectorizer.set_vocabulary(de_vocabulary)\n","\n","    # 2) Pull trained layers to copy weights\n","    trained_emb = trained_model.get_layer(\"d_embedding\")\n","    trained_dense1 = trained_model.get_layer(\"d_dense_1\")\n","    trained_dense_final = trained_model.get_layer(\"d_dense_final\")\n","    trained_gru = trained_model.get_layer(\"d_gru\")\n","\n","    # 3) New layers for inference (single step)\n","    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"d_infer_input\")\n","    d_state_inp = tf.keras.Input(shape=(256,), name=\"d_infer_state\")\n","\n","    d_vec = token_vectorizer(d_inp)  # (batch, 1)\n","    emb_layer = tf.keras.layers.Embedding(de_vocab_size + 2, 128, mask_zero=True, name=\"d_infer_embedding\")\n","    emb_out = emb_layer(d_vec)  # (batch, 1, 128)\n","\n","    gru_layer = tf.keras.layers.GRU(256, name=\"d_infer_gru\")  # returns (batch, 256)\n","    gru_out = gru_layer(emb_out, initial_state=d_state_inp)\n","\n","    dense1 = tf.keras.layers.Dense(512, activation=\"relu\", name=\"d_infer_dense_1\")\n","    dense1_out = dense1(gru_out)\n","\n","    dense_final = tf.keras.layers.Dense(de_vocab_size + 2, activation=\"softmax\", name=\"d_infer_dense_final\")\n","    final_out = dense_final(dense1_out)\n","\n","    de_infer = tf.keras.Model(inputs=[d_inp, d_state_inp], outputs=[final_out, gru_out], name=\"decoder_inference\")\n","\n","    # 4) Transfer weights\n","    emb_layer.set_weights(trained_emb.get_weights())\n","    dense1.set_weights(trained_dense1.get_weights())\n","    dense_final.set_weights(trained_dense_final.get_weights())\n","    gru_layer.set_weights(trained_gru.get_weights())\n","\n","    return de_infer, token_vectorizer\n","\n","# Build inference pieces\n","en_infer = final_model.get_layer(\"encoder\")\n","de_infer, de_token_vectorizer = build_inference_decoder(final_model, de_vocabulary, de_vocab)\n","\n","id_to_token = tf.keras.layers.StringLookup(\n","    vocabulary=de_vocabulary, invert=True, num_oov_indices=0\n",")\n","\n","def translate(sentence: str, max_steps=30):\n","    # encoder expects shape (batch, 1)\n","    enc_in = np.array([[sentence]], dtype=object)\n","    state = en_infer(enc_in)  # (1, 256)\n","\n","    token = start_token\n","    out_tokens = []\n","\n","    for _ in range(max_steps):\n","        dec_in = np.array([[token]], dtype=object)\n","        probs, state = de_infer([dec_in, state], training=False)\n","\n","        next_id = int(tf.argmax(probs, axis=-1).numpy()[0])\n","        next_tok = id_to_token([next_id]).numpy()[0].decode(\"utf-8\")\n","\n","        if next_tok == \"\" or next_tok == end_token:\n","            break\n","\n","        out_tokens.append(next_tok)\n","        token = next_tok\n","\n","    return \" \".join(out_tokens)\n","\n","# Example usage (after training):\n","# print(translate(\"I love you.\"))\n"]},{"cell_type":"markdown","id":"e4c13299","metadata":{"id":"e4c13299"},"source":["## 12) Testing on the held-out test set (after training)\n","\n","After training, I typically test by:\n","- translating a few random English sentences from the test split,\n","- comparing the generated German output with the ground-truth German sentence.\n","\n","This is not a rigorous evaluation, but it is a good sanity check that the inference loop works.\n"]},{"cell_type":"code","execution_count":null,"id":"d5b4ac98","metadata":{"id":"d5b4ac98","executionInfo":{"status":"aborted","timestamp":1767982271777,"user_tz":-420,"elapsed":9383,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# Uncomment after training:\n","# for i in np.random.choice(len(test_df), size=5, replace=False):\n","#     en = test_df.loc[i, \"EN\"]\n","#     de_true = test_df.loc[i, \"DE\"]\n","#     de_pred = translate(en)\n","#     print(\"=\"*80)\n","#     print(\"EN      :\", en)\n","#     print(\"DE true :\", de_true)\n","#     print(\"DE pred :\", de_pred)\n"]},{"cell_type":"markdown","id":"e7ca060e","metadata":{"id":"e7ca060e"},"source":["## 13) Chapter wrap-up\n","\n","What I take away from this chapter:\n","\n","- **Seq2seq problems** require models that can map sequences to sequences, often with different lengths.\n","- The **encoder–decoder** structure is a natural fit: the encoder builds a context representation, and the decoder generates the output sequence.\n","- **Teacher forcing** makes training much easier by feeding the decoder the ground-truth previous token.\n","- Plain accuracy is helpful but not sufficient; **BLEU-style** scores give a more sequence-aware signal.\n","- Using `TextVectorization` inside the model makes the workflow closer to an end-to-end translation system: raw strings in, translation out.\n","- To actually translate, the trained model must be adapted into an **inference version** where the decoder runs recursively and consumes its own previous predictions.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.x"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}