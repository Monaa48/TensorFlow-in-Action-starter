{"cells":[{"cell_type":"markdown","id":"d79f82bf","metadata":{"id":"d79f82bf"},"source":["# Chapter 13 — Transformers (BERT + Question Answering)\n","\n","**Primary reference:** *TensorFlow in Action* (Thushan Ganegedara), Chapter 13  \n","This notebook reproduces the main workflows from the chapter and adds a structured explanation of the ideas used in the code.\n","\n","---\n","\n","## 1) Summary\n","\n","This chapter focuses on *Transformer-based* NLP workflows, especially when we do not train a model from scratch:\n","\n","- A short recap of why **self-attention** is the core computation behind Transformers, and how **positional encoding** compensates for the lack of recurrence.\n","- **Fine-tuning a pretrained BERT encoder** for a downstream classification task (spam vs ham SMS messages).\n","- **Question answering (extractive QA)** using a pretrained Transformer (DistilBERT) with the SQuAD v1 dataset: predicting the *start* and *end* token positions of the answer span.\n","\n","In practice, the chapter highlights a modern pattern:\n","\n","1. Download a pretrained Transformer + matching tokenizer\n","2. Format inputs the way the model expects\n","3. Add a small task-specific head (classification head or span heads)\n","4. Fine-tune with a relatively small labeled dataset\n","\n","---\n","\n","## 2) Transformers recap (core concepts used in this chapter)\n","\n","### 2.1 Self-attention in one paragraph\n","\n","Given token representations \\(X\\in\\mathbb{R}^{T\\times d}\\), self-attention learns three projections:\n","\n","- \\(Q=XW_Q\\) (queries)\n","- \\(K=XW_K\\) (keys)\n","- \\(V=XW_V\\) (values)\n","\n","Then each token attends to every other token by\n","\n","\\[\n","\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V.\n","\\]\n","\n","The softmax weights decide “which tokens matter” when building a new representation for each position.\n","\n","### 2.2 Why positional encoding exists\n","\n","Self-attention alone does **not** know token order. Transformers inject order information by adding a position-dependent vector to each token embedding (either sinusoidal or learned). The book discusses the classic sinusoidal version.\n","\n","### 2.3 BERT vs the original Transformer\n","\n","BERT is essentially the **encoder stack** of the original Transformer architecture, pretrained on large text corpora. During fine-tuning, we reuse the encoder weights and only learn a small head (plus optionally update the encoder).\n","\n","### 2.4 Extractive question answering\n","\n","In extractive QA (like SQuAD), the answer is a span inside the context paragraph. A Transformer model produces token-level representations, then we add two token-wise classifiers:\n","\n","- one predicts the **start token index**\n","- one predicts the **end token index**\n","\n","The final predicted answer is the substring between those token indices.\n"]},{"cell_type":"code","execution_count":1,"id":"dcdea6e6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcdea6e6","executionInfo":{"status":"ok","timestamp":1767982983055,"user_tz":-420,"elapsed":8837,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"687ca707-2ef1-492f-cea3-d62278a00650"},"outputs":[{"output_type":"stream","name":"stdout","text":["python: 3.12.12\n","platform: Linux-6.6.105+-x86_64-with-glibc2.35\n","tf: 2.19.0\n","GPU available: True\n"]}],"source":["# Environment check (Colab-friendly)\n","import os, sys, platform\n","import tensorflow as tf\n","\n","print(\"python:\", sys.version.split()[0])\n","print(\"platform:\", platform.platform())\n","print(\"tf:\", tf.__version__)\n","print(\"GPU available:\", bool(tf.config.list_physical_devices(\"GPU\")))\n"]},{"cell_type":"markdown","id":"d5411a40","metadata":{"id":"d5411a40"},"source":["## 3) Quick sanity-check: scaled dot-product attention (toy example)\n","\n","This section is not meant to be a full Transformer implementation. The goal is to verify the *shape logic* behind attention:\n","\n","- Inputs: a sequence of token vectors \\([batch, T, d]\\)\n","- Outputs: same shape \\([batch, T, d_v]\\) after attention\n"]},{"cell_type":"code","execution_count":2,"id":"adb3c231","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adb3c231","executionInfo":{"status":"ok","timestamp":1767982984145,"user_tz":-420,"elapsed":1091,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"da80b054-308e-4a3b-c102-2e0a427b0528"},"outputs":[{"output_type":"stream","name":"stdout","text":["x: (2, 5, 8)\n","attention output: (2, 5, 8)\n","attention weights: (2, 5, 5)\n","weights row sums (should be 1): [1.         1.         0.99999994 0.99999994 0.99999994]\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","def scaled_dot_product_attention(q, k, v, mask=None):\n","    \"\"\"Compute Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\"\"\"\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(dk)  # [B, Tq, Tk]\n","    if mask is not None:\n","        scores += (mask * -1e9)\n","    weights = tf.nn.softmax(scores, axis=-1)\n","    output = tf.matmul(weights, v)  # [B, Tq, dv]\n","    return output, weights\n","\n","# toy batch: 2 sequences, length 5, model dim 8\n","B, T, d = 2, 5, 8\n","x = tf.random.normal([B, T, d])\n","\n","# simple linear projections\n","Wq = tf.keras.layers.Dense(d, use_bias=False)\n","Wk = tf.keras.layers.Dense(d, use_bias=False)\n","Wv = tf.keras.layers.Dense(d, use_bias=False)\n","\n","q, k, v = Wq(x), Wk(x), Wv(x)\n","out, w = scaled_dot_product_attention(q, k, v)\n","\n","print(\"x:\", x.shape)\n","print(\"attention output:\", out.shape)\n","print(\"attention weights:\", w.shape)\n","print(\"weights row sums (should be 1):\", tf.reduce_sum(w, axis=-1)[0].numpy())\n"]},{"cell_type":"markdown","id":"a40a7afc","metadata":{"id":"a40a7afc"},"source":["## 4) Project 1 — Spam classification with a pretrained BERT encoder (TensorFlow Hub)\n","\n","### 4.1 Goal\n","\n","Build a spam classifier for SMS messages with minimal feature engineering by fine-tuning a pretrained BERT encoder.\n","\n","### 4.2 Dataset\n","\n","The book uses the **SMS Spam Collection** dataset (ham/spam labeled SMS). Each line contains a label and the message text.\n","\n","### 4.3 Approach\n","\n","1. Download + parse the dataset  \n","2. Create **balanced** validation and test sets (same number of ham and spam)  \n","3. Build a BERT-based model:\n","   - BERT preprocessing layer (tokenization + packing)\n","   - BERT encoder layer (produces pooled output)\n","   - A small classification head  \n","4. Train for a few epochs and evaluate on the test set\n"]},{"cell_type":"code","execution_count":3,"id":"1a7f7919","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a7f7919","executionInfo":{"status":"ok","timestamp":1767982993592,"user_tz":-420,"elapsed":9434,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"13ddd1ab-4410-4ef5-f1f9-0ebb141f50af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: scikit-learn 1.6.0\n","Uninstalling scikit-learn-1.6.0:\n","  Successfully uninstalled scikit-learn-1.6.0\n","Found existing installation: imbalanced-learn 0.13.0\n","Uninstalling imbalanced-learn-0.13.0:\n","  Successfully uninstalled imbalanced-learn-0.13.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# If you run this on Colab, install the required libraries.\n","# (If you already have them, this cell is safe to re-run.)\n","!pip -q install -U tensorflow_hub tensorflow_text\n","\n","# Aggressively uninstall existing scikit-learn and imbalanced-learn to avoid conflicts\n","!pip uninstall -y scikit-learn imbalanced-learn\n","\n","# Specify compatible versions for scikit-learn and imbalanced-learn\n","# imbalanced-learn 0.13.0 is compatible with scikit-learn >= 1.6\n","!pip -q install scikit-learn==1.6.0 imbalanced-learn==0.13.0"]},{"cell_type":"code","execution_count":4,"id":"5745bb8d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"5745bb8d","executionInfo":{"status":"ok","timestamp":1767982993632,"user_tz":-420,"elapsed":39,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"b665c9a3-43fe-4398-8cfa-7f862424207d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                                                text  label\n","0  Go until jurong point, crazy.. Available only ...      0\n","1                      Ok lar... Joking wif u oni...      0\n","2  Free entry in 2 a wkly comp to win FA Cup fina...      1\n","3  U dun say so early hor... U c already then say...      0\n","4  Nah I don't think he goes to usf, he lives aro...      0"],"text/html":["\n","  <div id=\"df-30629258-f881-47f4-8fb1-2febee5a3193\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30629258-f881-47f4-8fb1-2febee5a3193')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-30629258-f881-47f4-8fb1-2febee5a3193 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-30629258-f881-47f4-8fb1-2febee5a3193');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-aba06419-9275-4a79-9ab8-d8a4bdd52e0c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aba06419-9275-4a79-9ab8-d8a4bdd52e0c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-aba06419-9275-4a79-9ab8-d8a4bdd52e0c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"class counts:\\\\n\\\", df[\\\"label\\\"]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Ok lar... Joking wif u oni...\",\n          \"Nah I don't think he goes to usf, he lives around here though\",\n          \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dataset size: 5574\n","class counts:\n"," label\n","0    4827\n","1     747\n","Name: count, dtype: int64\n"]}],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","# Download the dataset (UCI mirror). If it fails, try changing the URL to another mirror.\n","DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n","zip_path = tf.keras.utils.get_file(\"smsspamcollection.zip\", DATA_URL, extract=True)\n","# The file `SMSSpamCollection` is inside the extracted directory, which `zip_path` now points to.\n","# So, we should join `zip_path` directly with the filename.\n","txt_path = os.path.join(zip_path, \"SMSSpamCollection\")\n","\n","texts, labels = [], []\n","with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        parts = line.split(\"\\t\", 1)\n","        if len(parts) != 2:\n","            continue\n","        lab, msg = parts[0], parts[1]\n","        if lab == \"ham\":\n","            labels.append(0)\n","        elif lab == \"spam\":\n","            labels.append(1)\n","        else:\n","            continue\n","        texts.append(msg)\n","\n","texts = np.array(texts, dtype=object)\n","labels = np.array(labels, dtype=np.int32)\n","\n","df = pd.DataFrame({\"text\": texts, \"label\": labels})\n","display(df.head())\n","print(\"dataset size:\", len(df))\n","print(\"class counts:\\n\", df[\"label\"].value_counts())"]},{"cell_type":"markdown","id":"81d6e77c","metadata":{"id":"81d6e77c"},"source":["### 4.4 Split data (balanced validation + test)\n","\n","The chapter emphasizes evaluating on balanced subsets (same number of examples per class).  \n","Here, I create:\n","\n","- test: `n_per_class` ham + `n_per_class` spam  \n","- validation: `n_per_class` ham + `n_per_class` spam (from remaining data)  \n","- training: everything else (can be imbalanced, then balanced for training)\n","\n","For training, the book discusses using undersampling (including NearMiss).  \n","To keep this notebook stable across Colab environments, I implement:\n","\n","- a **default random undersampling** for training (simple and reliable)\n","- an **optional NearMiss** path if imbalanced-learn supports extracting selected indices\n","\n","You can switch between them with a flag.\n"]},{"cell_type":"code","execution_count":5,"id":"8e8de0bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e8de0bd","executionInfo":{"status":"ok","timestamp":1767982994190,"user_tz":-420,"elapsed":550,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"c9b85ce6-2c63-4a47-ef27-34931ebb5703"},"outputs":[{"output_type":"stream","name":"stdout","text":["train size: 5174  | class counts: [4627  547]\n","valid size: 200  | class counts: [100 100]\n","test  size: 200  | class counts: [100 100]\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7a0958273ce0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n","    self._make_controller_from_path(filepath)\n","  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n","    lib_controller = controller_class(\n","                     ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/threadpoolctl.py\", line 114, in __init__\n","    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n","OSError: dlopen() error\n"]},{"output_type":"stream","name":"stdout","text":["Random undersampling: 1094  | class counts: [547 547]\n"]}],"source":["rng = np.random.default_rng(4321)\n","n_per_class = 100  # same idea as the book; you can increase if you have more compute\n","\n","idx_all = np.arange(len(labels))\n","idx_ham = idx_all[labels == 0]\n","idx_spam = idx_all[labels == 1]\n","\n","test_idx = np.concatenate([\n","    rng.choice(idx_ham, n_per_class, replace=False),\n","    rng.choice(idx_spam, n_per_class, replace=False),\n","])\n","remaining = np.setdiff1d(idx_all, test_idx)\n","\n","# validation from remaining\n","rem_ham = remaining[labels[remaining] == 0]\n","rem_spam = remaining[labels[remaining] == 1]\n","valid_idx = np.concatenate([\n","    rng.choice(rem_ham, n_per_class, replace=False),\n","    rng.choice(rem_spam, n_per_class, replace=False),\n","])\n","train_idx = np.setdiff1d(remaining, valid_idx)\n","\n","train_texts, train_y = texts[train_idx], labels[train_idx]\n","valid_texts, valid_y = texts[valid_idx], labels[valid_idx]\n","test_texts, test_y = texts[test_idx], labels[test_idx]\n","\n","print(\"train size:\", len(train_texts), \" | class counts:\", np.bincount(train_y))\n","print(\"valid size:\", len(valid_texts), \" | class counts:\", np.bincount(valid_y))\n","print(\"test  size:\", len(test_texts),  \" | class counts:\", np.bincount(test_y))\n","\n","# Optional: balance training set with undersampling\n","USE_NEARMISS = False\n","\n","if USE_NEARMISS:\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from imblearn.under_sampling import NearMiss\n","\n","    vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n","    X_tfidf = vec.fit_transform(train_texts)\n","\n","    nm = NearMiss()\n","    X_res, y_res = nm.fit_resample(X_tfidf, train_y)\n","\n","    if hasattr(nm, \"sample_indices_\"):\n","        sel = nm.sample_indices_\n","        train_texts_bal = train_texts[sel]\n","        train_y_bal = train_y[sel]\n","        print(\"NearMiss selected:\", len(sel), \" | class counts:\", np.bincount(train_y_bal))\n","    else:\n","        print(\"NearMiss does not expose sample indices in this environment. Falling back to random undersampling.\")\n","        USE_NEARMISS = False\n","\n","if not USE_NEARMISS:\n","    from imblearn.under_sampling import RandomUnderSampler\n","    rus = RandomUnderSampler(random_state=4321)\n","    # Use indices as dummy features so we can retrieve selected texts by index\n","    dummy_X = np.arange(len(train_texts)).reshape(-1, 1)\n","    X_res, y_res = rus.fit_resample(dummy_X, train_y)\n","    sel = X_res.flatten()\n","    train_texts_bal = train_texts[sel]\n","    train_y_bal = train_y[sel]\n","    print(\"Random undersampling:\", len(train_texts_bal), \" | class counts:\", np.bincount(train_y_bal))"]},{"cell_type":"markdown","id":"793dfa06","metadata":{"id":"793dfa06"},"source":["### 4.5 Build tf.data pipelines\n","\n","For BERT fine-tuning, it is convenient to feed raw strings and let the preprocessing layer handle tokenization and packing.\n","\n","- Input: a batch of strings\n","- Output: a batch of labels (0/1)\n"]},{"cell_type":"code","execution_count":6,"id":"4b6e1061","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b6e1061","executionInfo":{"status":"ok","timestamp":1767982995262,"user_tz":-420,"elapsed":1071,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"d1e3609e-b338-41eb-92a0-d70ffdf33d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["batch text dtype: <dtype: 'string'> shape: (16,)\n","batch labels: [1 0 1 1 0 0 1 1 1 1]\n"]}],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","BATCH_SIZE = 16\n","\n","def make_text_ds(x, y, shuffle=False):\n","    ds = tf.data.Dataset.from_tensor_slices((x, y))\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=min(len(x), 5000), seed=4321, reshuffle_each_iteration=True)\n","    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n","    return ds\n","\n","tr_ds = make_text_ds(train_texts_bal, train_y_bal, shuffle=True)\n","va_ds = make_text_ds(valid_texts, valid_y, shuffle=False)\n","te_ds = make_text_ds(test_texts, test_y, shuffle=False)\n","\n","for batch_x, batch_y in tr_ds.take(1):\n","    print(\"batch text dtype:\", batch_x.dtype, \"shape:\", batch_x.shape)\n","    print(\"batch labels:\", batch_y[:10].numpy())\n"]},{"cell_type":"markdown","id":"eedbadc4","metadata":{"id":"eedbadc4"},"source":["### 4.6 Define the BERT model (preprocess + encoder + classification head)\n","\n","To avoid handling `vocab.txt` manually, I use the official TF Hub preprocessing module that matches the encoder:\n","\n","- preprocess: `bert_en_uncased_preprocess`\n","- encoder: `bert_en_uncased_L-12_H-768_A-12`\n","\n","The model produces a pooled representation for the whole sequence, then a small dense layer predicts spam/ham.\n"]},{"cell_type":"code","execution_count":7,"id":"b53248fa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"id":"b53248fa","executionInfo":{"status":"error","timestamp":1767982995658,"user_tz":-420,"elapsed":396,"user":{"displayName":"Anom","userId":"07557807243162324433"}},"outputId":"81fdad2b-c65a-4402-d23e-af7153c4cecc"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Op type not registered 'CaseFoldUTF8' in binary running on 800ffd28f4d9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3084\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'CaseFoldUTF8'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4290065962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbert_preprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert_preprocess\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbert_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert_encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m           \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    124\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    125\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         loader = Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m   1043\u001b[0m                         ckpt_options, options, filters)\n\u001b[1;32m   1044\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     self._concrete_functions = (\n\u001b[0;32m--> 161\u001b[0;31m         function_deserialization.load_function_def_library(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0msaved_object_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;31m# import).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       func_graph = function_def_lib.function_def_to_graph(\n\u001b[0m\u001b[1;32m    457\u001b[0m           \u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m           \u001b[0mstructured_input_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructured_input_signature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0minput_shapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0mfdef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_library_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_library_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes, include_library_functions)\u001b[0m\n\u001b[1;32m    328\u001b[0m           \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_def\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_def_for_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mop_def_for_type\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m       self._op_def_cache[type] = op_def_pb2.OpDef.FromString(\n\u001b[0;32m-> 3087\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_for_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3088\u001b[0m       )\n\u001b[1;32m   3089\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op_def_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on 800ffd28f4d9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."]}],"source":["import tensorflow_hub as hub\n","\n","preprocess_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n","encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n","\n","bert_preprocess = hub.KerasLayer(preprocess_url, name=\"bert_preprocess\")\n","bert_encoder = hub.KerasLayer(encoder_url, trainable=True, name=\"bert_encoder\")\n","\n","text_inp = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n","enc_inputs = bert_preprocess(text_inp)\n","enc_outputs = bert_encoder(enc_inputs)\n","\n","# TF Hub BERT returns a dict-like output; pooled_output corresponds to [CLS]\n","pooled = enc_outputs[\"pooled_output\"]\n","x = tf.keras.layers.Dropout(0.1)(pooled)\n","logits = tf.keras.layers.Dense(1, name=\"classifier\")(x)\n","\n","spam_model = tf.keras.Model(inputs=text_inp, outputs=logits)\n","spam_model.summary()\n"]},{"cell_type":"markdown","id":"b7d86aa0","metadata":{"id":"b7d86aa0"},"source":["### 4.7 Train and evaluate\n","\n","The chapter uses a small learning rate and trains for a few epochs.  \n","Fine-tuning can be slow, so I keep the default batch size moderate.\n","\n","If you need a quicker run, reduce `EPOCHS` or set `bert_encoder.trainable = False` (feature extraction instead of fine-tuning).\n"]},{"cell_type":"code","execution_count":null,"id":"4818fcbf","metadata":{"id":"4818fcbf","executionInfo":{"status":"aborted","timestamp":1767982995657,"user_tz":-420,"elapsed":21671,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["EPOCHS = 3\n","lr = 3e-5  # common fine-tuning LR; the book uses an even smaller LR\n","\n","loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = [\n","    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.0),\n","    tf.keras.metrics.AUC(name=\"auc\")\n","]\n","\n","spam_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n","    loss=loss_fn,\n","    metrics=metrics\n",")\n","\n","history = spam_model.fit(tr_ds, validation_data=va_ds, epochs=EPOCHS)\n","\n","print(\"\\nTest evaluation:\")\n","spam_model.evaluate(te_ds, verbose=1)\n"]},{"cell_type":"markdown","id":"e129b63f","metadata":{"id":"e129b63f"},"source":["### 4.8 Quick inference check\n","\n","I test a few hand-written messages to see if the model behaves reasonably.\n"]},{"cell_type":"code","execution_count":null,"id":"288d8677","metadata":{"id":"288d8677","executionInfo":{"status":"aborted","timestamp":1767982995659,"user_tz":-420,"elapsed":21671,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["samples = [\n","    \"Congratulations! You have won a free ticket. Call now to claim.\",\n","    \"Are we still meeting at 7pm tonight?\",\n","    \"URGENT! Your account has been selected for a prize. Reply YES.\",\n","    \"Ok, I will be there in 10 minutes.\"\n","]\n","\n","probs = tf.sigmoid(spam_model.predict(samples, verbose=0)).numpy().reshape(-1)\n","pred = (probs >= 0.5).astype(int)\n","\n","for s, p, yhat in zip(samples, probs, pred):\n","    label = \"spam\" if yhat == 1 else \"ham\"\n","    print(f\"{label:4s} | p(spam)={p:.3f} | {s}\")\n"]},{"cell_type":"markdown","id":"a4fa89b1","metadata":{"id":"a4fa89b1"},"source":["## 5) Project 2 — Extractive question answering with Hugging Face Transformers (SQuAD v1)\n","\n","### 5.1 Goal\n","\n","Given a **question** and a **context paragraph**, predict the answer span inside the context.\n","\n","### 5.2 Main steps (mirroring the chapter)\n","\n","1. Load SQuAD v1 with `datasets`\n","2. Fix known alignment issues in answer character indices\n","3. Tokenize (question + context) with a fast DistilBERT tokenizer\n","4. Convert character indices → token indices\n","5. Build a tf.data pipeline\n","6. Fine-tune `TFDistilBertForQuestionAnswering`\n","7. Run a small qualitative test: ask a question and decode the predicted span\n"]},{"cell_type":"code","execution_count":null,"id":"724f7008","metadata":{"id":"724f7008","executionInfo":{"status":"aborted","timestamp":1767982995660,"user_tz":-420,"elapsed":21672,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["# Hugging Face libraries used in the chapter\n","!pip -q install -U datasets transformers\n"]},{"cell_type":"code","execution_count":null,"id":"0ead4439","metadata":{"id":"0ead4439","executionInfo":{"status":"aborted","timestamp":1767982995673,"user_tz":-420,"elapsed":21684,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from datasets import load_dataset\n","import numpy as np\n","import tensorflow as tf\n","\n","dataset = load_dataset(\"squad\")\n","print(dataset)\n","\n","# For Colab runtime, it is usually better to start with a subset.\n","TRAIN_SAMPLES = 4000\n","TEST_SAMPLES = 800\n","\n","train_subset = dataset[\"train\"].select(range(TRAIN_SAMPLES))\n","test_subset = dataset[\"validation\"].select(range(TEST_SAMPLES))\n","\n","print(\"train subset:\", len(train_subset), \" | test subset:\", len(test_subset))\n","\n","# Inspect a sample\n","i = 0\n","print(\"Question:\", train_subset[i][\"question\"])\n","print(\"Answer:\", train_subset[i][\"answers\"][\"text\"][0])\n","print(\"Answer start (char):\", train_subset[i][\"answers\"][\"answer_start\"][0])\n","print(\"Context snippet:\", train_subset[i][\"context\"][:200], \"...\")\n"]},{"cell_type":"markdown","id":"438075c8","metadata":{"id":"438075c8"},"source":["### 5.3 Fix alignment issues and compute answer end index\n","\n","SQuAD provides `answer_start` (character index), but some records have small offsets.\n","The chapter fixes this by checking the substring in the context and shifting the index if needed.\n","\n","I implement a correction that tries offsets of 0, -1, -2, +1, +2 (small local search).  \n","Then I store:\n","\n","- `answer_start` (corrected)\n","- `answer_end` (exclusive end position)\n"]},{"cell_type":"code","execution_count":null,"id":"7bd880ff","metadata":{"id":"7bd880ff","executionInfo":{"status":"aborted","timestamp":1767982995674,"user_tz":-420,"elapsed":21684,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["def correct_indices_add_end_idx(answers, contexts):\n","    \"\"\"Fix answer_start when it is slightly misaligned, and add answer_end.\"\"\"\n","    n_ok, n_fix, n_fail = 0, 0, 0\n","    fixed = []\n","    for ans, ctx in zip(answers, contexts):\n","        gold_text = ans[\"text\"][0]\n","        start = ans[\"answer_start\"][0]\n","        # try small shifts around the given start\n","        candidates = [0, -1, -2, 1, 2]\n","        found = None\n","        for off in candidates:\n","            s = start + off\n","            e = s + len(gold_text)\n","            if s >= 0 and e <= len(ctx) and ctx[s:e] == gold_text:\n","                found = s\n","                break\n","        if found is None:\n","            # keep original start, but mark as failed (rare)\n","            found = start\n","            n_fail += 1\n","        elif found == start:\n","            n_ok += 1\n","        else:\n","            n_fix += 1\n","\n","        fixed.append({\n","            \"text\": [gold_text],\n","            \"answer_start\": found,\n","            \"answer_end\": found + len(gold_text),\n","        })\n","    print(f\"alignment ok: {n_ok} | fixed: {n_fix} | failed-to-fix: {n_fail}\")\n","    return fixed\n","\n","train_questions = list(train_subset[\"question\"])\n","train_contexts  = list(train_subset[\"context\"])\n","train_answers   = correct_indices_add_end_idx(list(train_subset[\"answers\"]), train_contexts)\n","\n","test_questions = list(test_subset[\"question\"])\n","test_contexts  = list(test_subset[\"context\"])\n","test_answers   = correct_indices_add_end_idx(list(test_subset[\"answers\"]), test_contexts)\n"]},{"cell_type":"markdown","id":"e0e44e1f","metadata":{"id":"e0e44e1f"},"source":["### 5.4 Tokenize (DistilBERT)\n","\n","DistilBERT uses the same WordPiece-style tokenization family as BERT (sub-word tokens).  \n","The *fast* tokenizer provides the `char_to_token` helper, which is important for mapping answer character positions to token indices.\n"]},{"cell_type":"code","execution_count":null,"id":"ee1ce915","metadata":{"id":"ee1ce915","executionInfo":{"status":"aborted","timestamp":1767982995674,"user_tz":-420,"elapsed":21683,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from transformers import DistilBertTokenizerFast\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","\n","# Sanity check: how [CLS] + [SEP] are inserted\n","context_demo = \"This is the context\"\n","question_demo = \"This is the question\"\n","tok_demo = tokenizer(context_demo, question_demo, return_tensors=\"tf\")\n","print(\"input_ids:\", tok_demo[\"input_ids\"].shape)\n","print(\"tokens:\", tokenizer.convert_ids_to_tokens(tok_demo[\"input_ids\"][0].numpy()))\n"]},{"cell_type":"markdown","id":"073d55e4","metadata":{"id":"073d55e4"},"source":["### 5.5 Encode the subset and map character indices to token indices\n","\n","Tokenization returns padded/truncated sequences.  \n","Next, we compute token-level start/end positions using `char_to_token`.\n"]},{"cell_type":"code","execution_count":null,"id":"e3d692bf","metadata":{"id":"e3d692bf","executionInfo":{"status":"aborted","timestamp":1767982995675,"user_tz":-420,"elapsed":21683,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["MAX_LEN = 384  # 512 is standard, but 384 is often enough and faster for Colab\n","\n","train_enc = tokenizer(\n","    train_contexts,\n","    train_questions,\n","    truncation=True,\n","    padding=\"max_length\",\n","    max_length=MAX_LEN,\n","    return_tensors=\"tf\"\n",")\n","test_enc = tokenizer(\n","    test_contexts,\n","    test_questions,\n","    truncation=True,\n","    padding=\"max_length\",\n","    max_length=MAX_LEN,\n","    return_tensors=\"tf\"\n",")\n","\n","def add_token_positions(encodings, answers, tokenizer, max_len):\n","    start_positions = []\n","    end_positions = []\n","    n_truncated = 0\n","\n","    for i, ans in enumerate(answers):\n","        start_char = ans[\"answer_start\"]\n","        end_char = ans[\"answer_end\"] - 1  # inclusive end char for char_to_token\n","\n","        start_tok = encodings.char_to_token(i, start_char)\n","        end_tok = encodings.char_to_token(i, end_char)\n","\n","        # If the answer is truncated away, char_to_token returns None.\n","        # A stable fallback is to use 0 (the [CLS] position).\n","        if start_tok is None or end_tok is None:\n","            n_truncated += 1\n","            start_tok = 0\n","            end_tok = 0\n","\n","        start_positions.append(start_tok)\n","        end_positions.append(end_tok)\n","\n","    encodings.update({\n","        \"start_positions\": tf.convert_to_tensor(start_positions, dtype=tf.int32),\n","        \"end_positions\": tf.convert_to_tensor(end_positions, dtype=tf.int32),\n","    })\n","    print(\"answers truncated (mapped to [CLS]):\", n_truncated)\n","\n","add_token_positions(train_enc, train_answers, tokenizer, MAX_LEN)\n","add_token_positions(test_enc, test_answers, tokenizer, MAX_LEN)\n","\n","print(\"train input_ids:\", train_enc[\"input_ids\"].shape)\n","print(\"train start_positions:\", train_enc[\"start_positions\"].shape)\n"]},{"cell_type":"markdown","id":"f09004b0","metadata":{"id":"f09004b0"},"source":["### 5.6 tf.data pipeline\n","\n","The model expects:\n","\n","- inputs: `(input_ids, attention_mask)`\n","- outputs: `(start_positions, end_positions)`\n","\n","I keep the pipeline simple with `from_tensor_slices`.\n"]},{"cell_type":"code","execution_count":null,"id":"ecad9cbf","metadata":{"id":"ecad9cbf","executionInfo":{"status":"aborted","timestamp":1767982995676,"user_tz":-420,"elapsed":21683,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["BATCH_SIZE_QA = 8\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","def make_qa_ds(enc):\n","    x = (enc[\"input_ids\"], enc[\"attention_mask\"])\n","    y = (enc[\"start_positions\"], enc[\"end_positions\"])\n","    return tf.data.Dataset.from_tensor_slices((x, y))\n","\n","full_train_ds = make_qa_ds(train_enc).shuffle(2048, seed=4321)\n","\n","# split: last 10% as validation\n","n_train = int(0.9 * TRAIN_SAMPLES)\n","train_ds = full_train_ds.take(n_train).batch(BATCH_SIZE_QA).prefetch(AUTOTUNE)\n","valid_ds = full_train_ds.skip(n_train).batch(BATCH_SIZE_QA).prefetch(AUTOTUNE)\n","\n","test_ds = make_qa_ds(test_enc).batch(BATCH_SIZE_QA).prefetch(AUTOTUNE)\n","\n","for (x_ids, x_mask), (y_s, y_e) in train_ds.take(1):\n","    print(\"input_ids:\", x_ids.shape, \"attention_mask:\", x_mask.shape)\n","    print(\"start/end:\", y_s.shape, y_e.shape)\n"]},{"cell_type":"markdown","id":"e03f19dc","metadata":{"id":"e03f19dc"},"source":["### 5.7 Define and train the QA model\n","\n","The chapter uses `TFDistilBertForQuestionAnswering`.  \n","In TensorFlow/Keras training, it is convenient to make the model output a tuple of tensors (start_logits, end_logits).  \n","I wrap the Hugging Face model inside a small Keras Functional model so the outputs are plain tensors.\n"]},{"cell_type":"code","execution_count":null,"id":"a4eab33a","metadata":{"id":"a4eab33a","executionInfo":{"status":"aborted","timestamp":1767982995676,"user_tz":-420,"elapsed":21683,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["from transformers import TFDistilBertForQuestionAnswering\n","\n","hf_qa = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n","hf_qa.config.return_dict = False  # prefer tuple outputs where possible\n","\n","def tf_wrap_model(model, max_len):\n","    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n","    out = model([input_ids, attention_mask])  # (start_logits, end_logits, ...)\n","    start_logits, end_logits = out[0], out[1]\n","    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[start_logits, end_logits])\n","\n","qa_model = tf_wrap_model(hf_qa, MAX_LEN)\n","qa_model.summary()\n","\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n","\n","qa_model.compile(\n","    optimizer=opt,\n","    loss=[loss_fn, loss_fn],\n","    metrics=[\n","        [tf.keras.metrics.SparseCategoricalAccuracy(name=\"start_acc\")],\n","        [tf.keras.metrics.SparseCategoricalAccuracy(name=\"end_acc\")]\n","    ]\n",")\n","\n","EPOCHS_QA = 2\n","history_qa = qa_model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS_QA)\n"]},{"cell_type":"markdown","id":"f8b3558f","metadata":{"id":"f8b3558f"},"source":["### 5.8 Evaluate and ask the model a question\n","\n","Evaluation metrics here are token-index accuracies (start/end).  \n","A more realistic QA metric is Exact Match / F1, but the chapter focuses on the start/end heads directly.\n","\n","Then I decode one sample prediction into text.\n"]},{"cell_type":"code","execution_count":null,"id":"c595f420","metadata":{"id":"c595f420","executionInfo":{"status":"aborted","timestamp":1767982995677,"user_tz":-420,"elapsed":21683,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["print(\"\\nTest evaluation:\")\n","qa_model.evaluate(test_ds, verbose=1)\n","\n","def ask_bert(sample_input, tokenizer):\n","    \"\"\"Decode predicted start/end token indices into a text answer.\"\"\"\n","    start_logits, end_logits = qa_model.predict(sample_input, verbose=0)\n","    start_idx = int(np.argmax(start_logits[0]))\n","    end_idx = int(np.argmax(end_logits[0]))\n","\n","    # ensure valid ordering\n","    if end_idx < start_idx:\n","        end_idx = start_idx\n","\n","    input_ids = sample_input[0].numpy()[0]\n","    # decode span (inclusive)\n","    span_ids = input_ids[start_idx:end_idx+1]\n","    answer = tokenizer.decode(span_ids, skip_special_tokens=True).strip()\n","    return start_idx, end_idx, answer\n","\n","i = 5\n","sample_q = test_questions[i]\n","sample_c = test_contexts[i]\n","gold = test_answers[i][\"text\"][0]\n","\n","sample_input = (\n","    test_enc[\"input_ids\"][i:i+1],\n","    test_enc[\"attention_mask\"][i:i+1],\n",")\n","\n","s_idx, e_idx, pred_answer = ask_bert(sample_input, tokenizer)\n","\n","print(\"Question:\", sample_q)\n","print(\"\\nGold answer:\", gold)\n","print(\"\\nPredicted:\", pred_answer)\n","print(\"\\nContext snippet:\", sample_c[:400], \"...\")\n"]},{"cell_type":"markdown","id":"3130d642","metadata":{"id":"3130d642"},"source":["### 5.9 Saving the tokenizer and the pretrained QA model\n","\n","The wrapper `qa_model` is a standard Keras model, but it contains a Hugging Face model as a layer.\n","To keep saving simple, I save the underlying Hugging Face model and tokenizer using `save_pretrained()`.\n"]},{"cell_type":"code","execution_count":null,"id":"1b63fb4b","metadata":{"id":"1b63fb4b","executionInfo":{"status":"aborted","timestamp":1767982995688,"user_tz":-420,"elapsed":21693,"user":{"displayName":"Anom","userId":"07557807243162324433"}}},"outputs":[],"source":["import os\n","\n","save_dir = os.path.join(\"models\", \"distilbert_qa\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","tokenizer.save_pretrained(save_dir)\n","hf_qa.save_pretrained(save_dir)\n","\n","print(\"Saved tokenizer + model to:\", save_dir)\n"]},{"cell_type":"markdown","id":"cf2705e9","metadata":{"id":"cf2705e9"},"source":["## 6) Takeaways\n","\n","- Transformers replace recurrence with self-attention, so they can model long-range dependencies while staying highly parallelizable.\n","- Pretrained Transformer encoders (BERT / DistilBERT) drastically reduce the amount of task-specific training needed.\n","- For classification, a pooled sequence representation + a small dense head is usually enough.\n","- For extractive QA, the model predicts start/end positions. Even basic token-index accuracy can be informative, but qualitative checks are still necessary.\n","- Libraries matter: TF Hub makes BERT fine-tuning approachable in Keras; Hugging Face makes advanced Transformer workflows (like QA) accessible with minimal code.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}