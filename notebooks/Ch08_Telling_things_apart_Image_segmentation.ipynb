{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monaa48/TensorFlow-in-Action-starter/blob/main/notebooks/Ch08_Telling_things_apart_Image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c832b46",
      "metadata": {
        "id": "2c832b46"
      },
      "source": [
        "# Chapter 08 — Telling Things Apart: Image Segmentation\n",
        "\n",
        "This chapter moves from **image classification** to **image segmentation**, where the target is not a single label per image, but a **label for every pixel**.  \n",
        "Instead of asking “What is in the image?”, segmentation asks “Which pixels belong to which object category?”\n",
        "\n",
        "In the book, the task is built around **PASCAL VOC 2012** segmentation data and an advanced model, **DeepLab v3**, which uses:\n",
        "- a strong feature extractor (pretrained ResNet-50 backbone),\n",
        "- **atrous / dilated convolution** to preserve spatial detail while keeping a large receptive field,\n",
        "- an **ASPP (Atrous Spatial Pyramid Pooling)** module to combine context at multiple effective scales,\n",
        "- and a final upsampling step to produce a full-resolution segmentation mask.\n",
        "\n",
        "In this notebook, I reproduce the chapter workflow end-to-end:\n",
        "1) download + inspect the VOC2012 segmentation data,\n",
        "2) build a proper `tf.data` pipeline (with aligned augmentation for image + mask),\n",
        "3) implement a DeepLab v3–style model (ResNet50 + ASPP),\n",
        "4) compile with segmentation-aware loss/metrics (including masking the “void/border” label),\n",
        "5) train, evaluate, and visualize predictions (qualitative inspection).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea1dd52",
      "metadata": {
        "id": "0ea1dd52"
      },
      "source": [
        "## 1) Summary\n",
        "\n",
        "### 1.1 Segmentation vs classification\n",
        "Classification outputs a single label, but segmentation outputs a **dense grid** of labels.  \n",
        "For a model, this changes several things at once:\n",
        "\n",
        "- **Targets are images too.**  \n",
        "  In VOC2012, the target masks are stored as palettized PNGs: each pixel is an index into a fixed color palette. That index corresponds to a class such as *person*, *dog*, *chair*, etc.\n",
        "\n",
        "- **Augmentation must be coordinated.**  \n",
        "  If an input image is randomly cropped or flipped, the target mask must be cropped or flipped in exactly the same way. Otherwise the labels no longer correspond to the pixels.\n",
        "\n",
        "- **Metrics should reflect spatial correctness.**  \n",
        "  Pixel accuracy can be misleading when background dominates. The most common segmentation metric is **IoU (Intersection-over-Union)**, which penalizes both false positives and false negatives in a spatial way.\n",
        "\n",
        "### 1.2 VOC2012 segmentation label structure (important detail)\n",
        "VOC2012 segmentation masks typically contain:\n",
        "- class indices (0…20) for known categories + background,\n",
        "- a special value for “border/void” that should usually be excluded from loss/metrics.\n",
        "\n",
        "In this notebook, I map `255 → 21` and treat `21` as the **void** class id.  \n",
        "During training/evaluation, I ignore void pixels so that the model is not rewarded or punished for ambiguous boundary regions.\n",
        "\n",
        "### 1.3 DeepLab v3 in a practical way\n",
        "DeepLab v3 is a design that balances **semantic context** and **spatial precision**:\n",
        "\n",
        "- The backbone (ResNet) provides strong feature extraction.\n",
        "- Atrous convolution helps keep a large receptive field without aggressive downsampling.\n",
        "- ASPP uses multiple dilation rates + global pooling to gather information from different spatial extents.\n",
        "- A final upsampling step restores full resolution for per-pixel prediction.\n",
        "\n",
        "The final output is a tensor shaped `(H, W, num_classes)`, and training uses per-pixel categorical loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d07c696",
      "metadata": {
        "id": "2d07c696"
      },
      "source": [
        "## 2) Setup\n",
        "\n",
        "Imports, seed, and small utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74eb8b83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74eb8b83",
        "outputId": "bc03c577-8b63-4266-b2fc-0372789d92d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 4321\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8348f109",
      "metadata": {
        "id": "8348f109"
      },
      "source": [
        "## 3) Download and extract PASCAL VOC 2012\n",
        "\n",
        "The chapter uses PASCAL VOC 2012.  \n",
        "The official download is hosted by Oxford:\n",
        "- `http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar`\n",
        "\n",
        "This is a large archive, so the code below avoids re-downloading if the file already exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tarfile\n",
        "\n",
        "tar_path = Path(\"data/VOCtrainval_11-May-2012.tar\")\n",
        "\n",
        "print(\"Exists:\", tar_path.exists())\n",
        "if tar_path.exists():\n",
        "    size_mb = tar_path.stat().st_size / (1024 * 1024)\n",
        "    print(\"Size (MB):\", round(size_mb, 2))\n",
        "    print(\"is_tarfile:\", tarfile.is_tarfile(tar_path))\n",
        "\n",
        "    if size_mb < 500:  # ambang aman untuk mendeteksi file yang “kecil banget”\n",
        "        with open(tar_path, \"rb\") as f:\n",
        "            head = f.read(200)\n",
        "        print(\"First 200 bytes preview:\")\n",
        "        print(head)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMJ60Bs3KkR8",
        "outputId": "7fe45e0c-6c82-4696-ab3f-6d5dceacae1c"
      },
      "id": "dMJ60Bs3KkR8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: True\n",
            "Size (MB): 0.0\n",
            "is_tarfile: False\n",
            "First 200 bytes preview:\n",
            "b''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path(\"data\")\n",
        "bad_tar = DATA_ROOT / \"VOCtrainval_11-May-2012.tar\"\n",
        "bad_extract = DATA_ROOT / \"VOCdevkit\"\n",
        "\n",
        "if bad_tar.exists():\n",
        "    bad_tar.unlink()\n",
        "\n",
        "shutil.rmtree(bad_extract, ignore_errors=True)\n",
        "\n",
        "print(\"Cleaned old/corrupted VOC files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iZ6YYcPIpu9",
        "outputId": "027e8d56-938f-4f3b-815e-7710d8faa577"
      },
      "id": "4iZ6YYcPIpu9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned old/corrupted VOC files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5db9f19f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db9f19f",
        "outputId": "38d213be-5399-4fe6-d916-7fdf07b126af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: ['https://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar', 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar']\n",
            "Download failed: expected string or bytes-like object, got 'list'\n",
            "Skipping extraction (archive not found).\n",
            "VOC_DIR : data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012\n",
            "JPEG_DIR: data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages\n",
            "MASK_DIR: data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/SegmentationClass\n",
            "SET_DIR : data/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "DATA_ROOT = Path(\"data\")\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tar_name = \"VOCtrainval_11-May-2012.tar\"\n",
        "tar_path = DATA_ROOT / tar_name\n",
        "extract_dir = DATA_ROOT / \"VOCtrainval_11-May-2012\"\n",
        "\n",
        "# URL mirror sebelumnya error (404), gunakan server resmi Oxford\n",
        "url = [\n",
        "    \"https://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\",\n",
        "    \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\",\n",
        "]\n",
        "\n",
        "\n",
        "# 1. Clean up corrupted file if present\n",
        "if tar_path.exists():\n",
        "    try:\n",
        "        if not tarfile.is_tarfile(tar_path):\n",
        "            print(\"Existing file is not a valid tar (likely corrupted download). Deleting...\")\n",
        "            tar_path.unlink()\n",
        "        else:\n",
        "            print(\"Archive exists and seems valid:\", tar_path)\n",
        "    except Exception:\n",
        "        print(\"Error checking file. Deleting...\")\n",
        "        tar_path.unlink()\n",
        "\n",
        "# 2. Download if missing\n",
        "if not tar_path.exists():\n",
        "    print(\"Downloading:\", url)\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url, tar_path)\n",
        "        print(\"Saved to:\", tar_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        if tar_path.exists():\n",
        "            tar_path.unlink() # Hapus file parsial jika gagal\n",
        "\n",
        "# 3. Extract\n",
        "if not extract_dir.exists():\n",
        "    if tar_path.exists():\n",
        "        print(\"Extracting tar (this can take a while)...\")\n",
        "        try:\n",
        "            with tarfile.open(tar_path) as tf_tar:\n",
        "                tf_tar.extractall(extract_dir)\n",
        "            print(\"Extracted to:\", extract_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during extraction: {e}\")\n",
        "            print(\"You may need to delete the file and try again.\")\n",
        "    else:\n",
        "        print(\"Skipping extraction (archive not found).\")\n",
        "else:\n",
        "    print(\"Extracted folder already exists:\", extract_dir)\n",
        "\n",
        "# VOC folder structure\n",
        "VOC_DIR = extract_dir / \"VOCdevkit\" / \"VOC2012\"\n",
        "JPEG_DIR = VOC_DIR / \"JPEGImages\"\n",
        "MASK_DIR = VOC_DIR / \"SegmentationClass\"\n",
        "SET_DIR = VOC_DIR / \"ImageSets\" / \"Segmentation\"\n",
        "\n",
        "print(\"VOC_DIR :\", VOC_DIR)\n",
        "print(\"JPEG_DIR:\", JPEG_DIR)\n",
        "print(\"MASK_DIR:\", MASK_DIR)\n",
        "print(\"SET_DIR :\", SET_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tar exists?\", tar_path.exists())\n",
        "if tar_path.exists():\n",
        "    print(\"Size (MB):\", tar_path.stat().st_size / (1024 * 1024))\n",
        "else:\n",
        "    print(\"File is missing (likely deleted after failed validation).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JotuE2FDJCZF",
        "outputId": "90def193-775b-42fd-b143-67950a7d598d"
      },
      "id": "JotuE2FDJCZF",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tar exists? False\n",
            "File is missing (likely deleted after failed validation).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(tar_path, \"rb\") as f:\n",
        "    head = f.read(200)\n",
        "\n",
        "print(head[:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "wympG9WTJ9FR",
        "outputId": "97062ef7-0929-4f80-b7c6-e999a64a66c4"
      },
      "id": "wympG9WTJ9FR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/VOCtrainval_11-May-2012.tar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3632763010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/VOCtrainval_11-May-2012.tar'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624a6e7d",
      "metadata": {
        "id": "624a6e7d"
      },
      "source": [
        "## 4) Load split files and build file pairs (image, mask)\n",
        "\n",
        "VOC provides predefined train/val file lists in:\n",
        "- `ImageSets/Segmentation/train.txt`\n",
        "- `ImageSets/Segmentation/val.txt`\n",
        "\n",
        "The book also uses a train/val/test setup.  \n",
        "VOC2012 does not ship a labeled test set publicly, so a common approach is to split the provided `val.txt` list into:\n",
        "- validation subset\n",
        "- test subset\n",
        "\n",
        "I reproduce that idea by shuffling `val.txt` with a fixed seed and splitting it in half.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a7b38c",
      "metadata": {
        "id": "69a7b38c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup paths\n",
        "DATA_ROOT = Path(\"data\")\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "extract_dir = DATA_ROOT / \"VOCtrainval_11-May-2012\"\n",
        "VOC_DIR = extract_dir / \"VOCdevkit\" / \"VOC2012\"\n",
        "JPEG_DIR = VOC_DIR / \"JPEGImages\"\n",
        "MASK_DIR = VOC_DIR / \"SegmentationClass\"\n",
        "SET_DIR = VOC_DIR / \"ImageSets\" / \"Segmentation\"\n",
        "\n",
        "tar_name = \"VOCtrainval_11-May-2012.tar\"\n",
        "tar_path = DATA_ROOT / tar_name\n",
        "\n",
        "# --- Robust Download & Extraction Logic ---\n",
        "\n",
        "# List of mirrors to try if one fails\n",
        "mirrors = [\n",
        "    \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\", # Official\n",
        "    \"https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\",             # Mirror 1\n",
        "    \"http://data.lip6.fr/cadene/VOCtrainval_11-May-2012.tar\"                     # Mirror 2\n",
        "]\n",
        "\n",
        "def download_file(url, path):\n",
        "    print(f\"Attempting download from: {url}\")\n",
        "    try:\n",
        "        # User-agent to avoid 403 Forbidden on some servers\n",
        "        opener = urllib.request.build_opener()\n",
        "        opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)')]\n",
        "        urllib.request.install_opener(opener)\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "        print(\"Download finished.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def validate_and_extract(path, target_dir):\n",
        "    if not path.exists():\n",
        "        return False\n",
        "\n",
        "    print(\"Verifying file integrity...\")\n",
        "    try:\n",
        "        if not tarfile.is_tarfile(path):\n",
        "            print(\"Error: File is not a valid tar archive.\")\n",
        "            return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "    print(\"Extracting...\")\n",
        "    try:\n",
        "        with tarfile.open(path) as tf:\n",
        "            tf.extractall(target_dir)\n",
        "        print(\"Extraction successful.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Check if data is already ready\n",
        "train_txt_path = SET_DIR / \"train.txt\"\n",
        "\n",
        "if not train_txt_path.exists():\n",
        "    print(\"Dataset not ready. Starting setup...\")\n",
        "\n",
        "    # 1. If file exists, check if it works. If not, delete it.\n",
        "    if tar_path.exists():\n",
        "        if not validate_and_extract(tar_path, extract_dir):\n",
        "            print(\"Existing file corrupted. Deleting...\")\n",
        "            tar_path.unlink()\n",
        "\n",
        "    # 2. If we still don't have the data, loop through mirrors\n",
        "    if not train_txt_path.exists():\n",
        "        success = False\n",
        "        for url in mirrors:\n",
        "            if download_file(url, tar_path):\n",
        "                if validate_and_extract(tar_path, extract_dir):\n",
        "                    success = True\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Downloaded file invalid. Deleting and trying next mirror...\")\n",
        "                    if tar_path.exists():\n",
        "                        tar_path.unlink()\n",
        "\n",
        "        if not success:\n",
        "            raise RuntimeError(\"Failed to download and extract dataset from all available mirrors.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "# --- Load Data Helper ---\n",
        "\n",
        "def read_id_list(path: Path):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"{path} not found. Dataset download might have failed.\")\n",
        "    ids = path.read_text().strip().split()\n",
        "    return ids\n",
        "\n",
        "train_ids = read_id_list(SET_DIR / \"train.txt\")\n",
        "val_ids = read_id_list(SET_DIR / \"val.txt\")\n",
        "\n",
        "print(\"train ids:\", len(train_ids))\n",
        "print(\"val ids  :\", len(val_ids))\n",
        "\n",
        "# Split val into val/test\n",
        "random_seed = 4321 # explicit local seed if needed\n",
        "rng = np.random.RandomState(random_seed)\n",
        "val_ids_shuffled = val_ids.copy()\n",
        "rng.shuffle(val_ids_shuffled)\n",
        "\n",
        "mid = len(val_ids_shuffled) // 2\n",
        "valid_ids = val_ids_shuffled[:mid]\n",
        "test_ids  = val_ids_shuffled[mid:]\n",
        "\n",
        "print(\"valid ids:\", len(valid_ids))\n",
        "print(\"test ids :\", len(test_ids))\n",
        "\n",
        "def make_pairs(ids):\n",
        "    x_paths = [str(JPEG_DIR / f\"{i}.jpg\") for i in ids]\n",
        "    y_paths = [str(MASK_DIR / f\"{i}.png\") for i in ids]\n",
        "    return x_paths, y_paths\n",
        "\n",
        "train_x, train_y = make_pairs(train_ids)\n",
        "valid_x, valid_y = make_pairs(valid_ids)\n",
        "test_x,  test_y  = make_pairs(test_ids)\n",
        "\n",
        "if len(train_x) > 0:\n",
        "    print(\"Sample path:\", train_x[0])\n",
        "else:\n",
        "    print(\"Warning: No data loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf77e79",
      "metadata": {
        "id": "bcf77e79"
      },
      "source": [
        "## 5) Visual inspection: input image + mask\n",
        "\n",
        "VOC masks are stored as palettized PNGs.  \n",
        "If we load them with Pillow, `np.array(mask)` returns a 2D array of **class indices** (not RGB).  \n",
        "For visualization, it helps to convert those indices into an RGB image using the VOC colormap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910e67cd",
      "metadata": {
        "id": "910e67cd"
      },
      "outputs": [],
      "source": [
        "def voc_colormap():\n",
        "    # Standard PASCAL VOC color map (21 classes); we will add a \"void\" color for index 21.\n",
        "    # Source idea: common VOC colormap implementation (bit trick); kept here as a utility.\n",
        "    def bitget(byteval, idx):\n",
        "        return (byteval & (1 << idx)) != 0\n",
        "\n",
        "    cmap = np.zeros((256, 3), dtype=np.uint8)\n",
        "    for i in range(256):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r |= (bitget(c, 0) << (7 - j))\n",
        "            g |= (bitget(c, 1) << (7 - j))\n",
        "            b |= (bitget(c, 2) << (7 - j))\n",
        "            c >>= 3\n",
        "        cmap[i] = [r, g, b]\n",
        "    return cmap\n",
        "\n",
        "VOC_CMAP = voc_colormap()\n",
        "\n",
        "VOID_RAW_ID = 255   # typical \"border/void\" in VOC masks\n",
        "VOID_CLASS_ID = 21  # mapped void id in this notebook\n",
        "\n",
        "def mask_to_rgb(mask_index):\n",
        "    # mask_index: (H, W) int\n",
        "    # Map 255 to 21 for visualization consistency\n",
        "    m = mask_index.copy()\n",
        "    m[m == VOID_RAW_ID] = VOID_CLASS_ID\n",
        "    rgb = VOC_CMAP[m]\n",
        "    return rgb\n",
        "\n",
        "def show_image_and_mask(img_path, mask_path):\n",
        "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "    mask = np.array(Image.open(mask_path))\n",
        "    mask_rgb = mask_to_rgb(mask)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Input image\", fontsize=11)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Segmentation mask (colorized)\", fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show a few examples\n",
        "for i in range(3):\n",
        "    show_image_and_mask(train_x[i], train_y[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5851c2a",
      "metadata": {
        "id": "c5851c2a"
      },
      "source": [
        "## 6) Data preparation: a `tf.data` pipeline for segmentation\n",
        "\n",
        "### 6.1 Why `tf.data` is useful here\n",
        "Segmentation datasets can be large, and masks are images too.  \n",
        "A streaming pipeline helps because:\n",
        "- it avoids loading everything into RAM,\n",
        "- it enables parallelism and prefetching,\n",
        "- it keeps augmentation consistent and repeatable.\n",
        "\n",
        "### 6.2 Key requirements for segmentation pipelines\n",
        "1) input and mask must be transformed together (crop/flip),\n",
        "2) mask resizing must use **nearest-neighbor** (to keep class indices),\n",
        "3) normalize inputs (the chapter uses `(x - 128) / 255`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d5bc7c",
      "metadata": {
        "id": "d2d5bc7c"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (384, 384)              # the chapter uses a large input size\n",
        "RESIZE_BEFORE_CROP = (444, 444)    # for random crop augmentation\n",
        "BATCH_SIZE = 16                    # adjust based on GPU memory\n",
        "EPOCHS = 10                        # change for longer training\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_image_np(path):\n",
        "    # path is a bytes object when called via tf.numpy_function\n",
        "    path = path.decode(\"utf-8\")\n",
        "    img = np.array(Image.open(path).convert(\"RGB\"), dtype=np.uint8)\n",
        "    return img\n",
        "\n",
        "def load_mask_np(path):\n",
        "    path = path.decode(\"utf-8\")\n",
        "    m = np.array(Image.open(path), dtype=np.uint8)  # palettized indices\n",
        "    # Map VOC void border 255 -> 21\n",
        "    m[m == VOID_RAW_ID] = VOID_CLASS_ID\n",
        "    return m\n",
        "\n",
        "def tf_load_pair(img_path, mask_path):\n",
        "    img = tf.numpy_function(load_image_np, [img_path], Tout=tf.uint8)\n",
        "    mask = tf.numpy_function(load_mask_np, [mask_path], Tout=tf.uint8)\n",
        "    img.set_shape([None, None, 3])\n",
        "    mask.set_shape([None, None])\n",
        "    mask = mask[..., tf.newaxis]  # (H, W, 1)\n",
        "    return img, mask\n",
        "\n",
        "def random_resize_and_crop(img, mask, crop_size=IMG_SIZE, resize_to=RESIZE_BEFORE_CROP):\n",
        "    # Resize first, then random crop\n",
        "    img = tf.image.resize(img, resize_to, method=\"bilinear\")\n",
        "    mask = tf.image.resize(mask, resize_to, method=\"nearest\")\n",
        "\n",
        "    h = tf.shape(img)[0]\n",
        "    w = tf.shape(img)[1]\n",
        "    ch, cw = crop_size\n",
        "\n",
        "    offset_h = tf.random.uniform([], 0, h - ch + 1, dtype=tf.int32)\n",
        "    offset_w = tf.random.uniform([], 0, w - cw + 1, dtype=tf.int32)\n",
        "\n",
        "    img = tf.image.crop_to_bounding_box(img, offset_h, offset_w, ch, cw)\n",
        "    mask = tf.image.crop_to_bounding_box(mask, offset_h, offset_w, ch, cw)\n",
        "    return img, mask\n",
        "\n",
        "def resize_only(img, mask, size=IMG_SIZE):\n",
        "    img = tf.image.resize(img, size, method=\"bilinear\")\n",
        "    mask = tf.image.resize(mask, size, method=\"nearest\")\n",
        "    return img, mask\n",
        "\n",
        "def augment(img, mask):\n",
        "    # Horizontal flip\n",
        "    if tf.random.uniform([]) < 0.5:\n",
        "        img = tf.image.flip_left_right(img)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "\n",
        "    # Color jitter for image only\n",
        "    img = tf.image.random_hue(img, 0.08)\n",
        "    img = tf.image.random_brightness(img, 0.15)\n",
        "    img = tf.image.random_contrast(img, 0.8, 1.2)\n",
        "    img = tf.clip_by_value(img, 0.0, 255.0)\n",
        "    return img, mask\n",
        "\n",
        "def normalize(img, mask):\n",
        "    # Chapter normalization: (x - 128) / 255\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = (img - 128.0) / 255.0\n",
        "\n",
        "    mask = tf.cast(mask, tf.int32)  # keep integer labels\n",
        "    return img, mask\n",
        "\n",
        "def make_dataset(x_paths, y_paths, training, batch_size=BATCH_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_paths, y_paths))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=min(len(x_paths), 2048), seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    ds = ds.map(tf_load_pair, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        ds = ds.map(lambda x, y: random_resize_and_crop(x, y), num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(resize_only, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_x, train_y, training=True)\n",
        "valid_ds = make_dataset(valid_x, valid_y, training=False)\n",
        "test_ds  = make_dataset(test_x,  test_y,  training=False)\n",
        "\n",
        "# Sanity check shapes\n",
        "x_batch, y_batch = next(iter(train_ds))\n",
        "print(\"x batch:\", x_batch.shape, x_batch.dtype)\n",
        "print(\"y batch:\", y_batch.shape, y_batch.dtype)\n",
        "print(\"unique labels (sample):\", np.unique(y_batch.numpy())[:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765242a9",
      "metadata": {
        "id": "765242a9"
      },
      "source": [
        "## 7) Model: DeepLab v3 style (ResNet50 backbone + ASPP)\n",
        "\n",
        "### 7.1 Backbone feature map choice\n",
        "DeepLab v3 typically uses a backbone where the output stride is ~16.\n",
        "A simple practical way is to take the output of the ResNet50 **conv4** block as the feature map,\n",
        "then apply ASPP on top of that feature map and upsample back to the input resolution.\n",
        "\n",
        "### 7.2 ASPP (Atrous Spatial Pyramid Pooling)\n",
        "ASPP combines:\n",
        "- 1×1 conv (local features),\n",
        "- 3×3 conv with multiple dilation rates (multi-scale context),\n",
        "- image-level pooling branch (global context),\n",
        "then concatenates and projects the combined features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01acaaf2",
      "metadata": {
        "id": "01acaaf2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "NUM_CLASSES = 22  # 0..20 + void=21\n",
        "\n",
        "def conv_bn_relu(x, filters, k, rate=1):\n",
        "    x = layers.Conv2D(filters, k, padding=\"same\", dilation_rate=rate, use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def aspp(x, filters=256, rates=(6, 12, 18), dropout=0.1):\n",
        "    # 1x1\n",
        "    b0 = conv_bn_relu(x, filters, 1, rate=1)\n",
        "\n",
        "    # 3x3 atrous\n",
        "    b1 = conv_bn_relu(x, filters, 3, rate=rates[0])\n",
        "    b2 = conv_bn_relu(x, filters, 3, rate=rates[1])\n",
        "    b3 = conv_bn_relu(x, filters, 3, rate=rates[2])\n",
        "\n",
        "    # Image-level pooling\n",
        "    pool = layers.Lambda(lambda t: tf.reduce_mean(t, axis=[1, 2], keepdims=True))(x)\n",
        "    pool = conv_bn_relu(pool, filters, 1, rate=1)\n",
        "    pool = layers.Lambda(lambda t: tf.image.resize(t, tf.shape(x)[1:3], method=\"bilinear\"))(pool)\n",
        "\n",
        "    y = layers.Concatenate(axis=-1)([b0, b1, b2, b3, pool])\n",
        "    y = conv_bn_relu(y, filters, 1, rate=1)\n",
        "    y = layers.Dropout(dropout)(y)\n",
        "    return y\n",
        "\n",
        "def build_deeplab_v3(input_size=IMG_SIZE, num_classes=NUM_CLASSES):\n",
        "    inputs = layers.Input(shape=(input_size[0], input_size[1], 3))\n",
        "\n",
        "    base = ResNet50(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
        "    feat = base.get_layer(\"conv4_block6_out\").output  # stride ~16\n",
        "\n",
        "    x = aspp(feat, filters=256, rates=(6, 12, 18), dropout=0.2)\n",
        "    x = conv_bn_relu(x, 256, 3, rate=1)\n",
        "\n",
        "    # Upsample to input size (bilinear)\n",
        "    x = layers.Lambda(lambda t: tf.image.resize(t, input_size, method=\"bilinear\"))(x)\n",
        "\n",
        "    logits = layers.Conv2D(num_classes, 1, padding=\"same\")(x)\n",
        "    outputs = layers.Softmax(axis=-1)(logits)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"deeplabv3_resnet50_aspp\")\n",
        "    return model\n",
        "\n",
        "model = build_deeplab_v3()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa83b6f",
      "metadata": {
        "id": "eaa83b6f"
      },
      "source": [
        "## 8) Loss and metrics (masking void pixels)\n",
        "\n",
        "### 8.1 Why masking is necessary\n",
        "VOC masks contain a “void/border” label (mapped to class id `21` here).  \n",
        "These pixels are ambiguous boundaries and should not dominate optimization.\n",
        "\n",
        "So for loss and metrics:\n",
        "- create a boolean mask: `y_true != VOID_CLASS_ID`,\n",
        "- compute values only on valid pixels.\n",
        "\n",
        "### 8.2 Metrics included\n",
        "- masked pixel accuracy (useful for monitoring),\n",
        "- masked mean IoU (more meaningful for segmentation quality).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f934d3fd",
      "metadata": {
        "id": "f934d3fd"
      },
      "outputs": [],
      "source": [
        "def masked_sparse_cce(y_true, y_pred, void_id=VOID_CLASS_ID):\n",
        "    # y_true: (B, H, W, 1), y_pred: (B, H, W, C) probabilities\n",
        "    y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int32)\n",
        "    mask = tf.not_equal(y_true, void_id)\n",
        "\n",
        "    y_true_m = tf.boolean_mask(y_true, mask)\n",
        "    y_pred_m = tf.boolean_mask(y_pred, mask)\n",
        "\n",
        "    # sparse categorical cross-entropy expects y_true shape (N,) and y_pred shape (N, C)\n",
        "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true_m, y_pred_m))\n",
        "\n",
        "def masked_pixel_accuracy(y_true, y_pred, void_id=VOID_CLASS_ID):\n",
        "    y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int32)\n",
        "    y_hat = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
        "\n",
        "    mask = tf.not_equal(y_true, void_id)\n",
        "    y_true_m = tf.boolean_mask(y_true, mask)\n",
        "    y_hat_m  = tf.boolean_mask(y_hat, mask)\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(y_true_m, y_hat_m), tf.float32))\n",
        "\n",
        "def masked_mean_iou(y_true, y_pred, void_id=VOID_CLASS_ID, num_classes=NUM_CLASSES):\n",
        "    y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int32)\n",
        "    y_hat = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
        "\n",
        "    mask = tf.not_equal(y_true, void_id)\n",
        "    y_true_m = tf.boolean_mask(y_true, mask)\n",
        "    y_hat_m  = tf.boolean_mask(y_hat, mask)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(y_true_m, y_hat_m, num_classes=num_classes, dtype=tf.float32)\n",
        "\n",
        "    # Exclude void row/col for IoU averaging\n",
        "    cm = cm[:void_id, :void_id]  # classes 0..20\n",
        "\n",
        "    diag = tf.linalg.diag_part(cm)\n",
        "    rowsum = tf.reduce_sum(cm, axis=1)\n",
        "    colsum = tf.reduce_sum(cm, axis=0)\n",
        "    denom = rowsum + colsum - diag\n",
        "\n",
        "    iou = diag / (denom + 1e-8)\n",
        "\n",
        "    # Average only over classes that appear (denom > 0)\n",
        "    valid = tf.greater(denom, 0.0)\n",
        "    iou = tf.boolean_mask(iou, valid)\n",
        "    return tf.reduce_mean(iou)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=masked_sparse_cce,\n",
        "    metrics=[masked_pixel_accuracy, masked_mean_iou],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe34d8a",
      "metadata": {
        "id": "0fe34d8a"
      },
      "source": [
        "## 9) Training\n",
        "\n",
        "This is a compute-heavy task. The defaults are chosen to be runnable, but you can scale them:\n",
        "- increase `EPOCHS`,\n",
        "- increase `BATCH_SIZE` if GPU memory allows,\n",
        "- or reduce `IMG_SIZE` to speed up iterations.\n",
        "\n",
        "I also add callbacks to make training more controlled:\n",
        "- ModelCheckpoint to save best validation IoU,\n",
        "- EarlyStopping to reduce wasted epochs when validation stops improving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd750cbd",
      "metadata": {
        "id": "fd750cbd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "ckpt_path = \"deeplabv3_voc2012_best.keras\"\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(ckpt_path, monitor=\"val_masked_mean_iou\", mode=\"max\", save_best_only=True),\n",
        "    EarlyStopping(monitor=\"val_masked_mean_iou\", mode=\"max\", patience=4, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace37b4b",
      "metadata": {
        "id": "ace37b4b"
      },
      "source": [
        "### 9.1 Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6900b137",
      "metadata": {
        "id": "6900b137"
      },
      "outputs": [],
      "source": [
        "def plot_history(hist, keys):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for k in keys:\n",
        "        if k in hist.history:\n",
        "            plt.plot(hist.history[k], label=k)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history, [\"loss\", \"val_loss\"])\n",
        "plot_history(history, [\"masked_pixel_accuracy\", \"val_masked_pixel_accuracy\"])\n",
        "plot_history(history, [\"masked_mean_iou\", \"val_masked_mean_iou\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afaa723f",
      "metadata": {
        "id": "afaa723f"
      },
      "source": [
        "## 10) Evaluation on the held-out test split\n",
        "\n",
        "After training, evaluate on the split created from VOC `val.txt`.  \n",
        "The reported IoU is computed after excluding void pixels, which makes it more meaningful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16862367",
      "metadata": {
        "id": "16862367"
      },
      "outputs": [],
      "source": [
        "test_metrics = model.evaluate(test_ds, verbose=1)\n",
        "for name, value in zip(model.metrics_names, test_metrics):\n",
        "    print(f\"{name:25s}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ba6323",
      "metadata": {
        "id": "56ba6323"
      },
      "source": [
        "## 11) Qualitative results: visualize predictions\n",
        "\n",
        "Quantitative metrics summarize performance, but segmentation also benefits from direct inspection:\n",
        "- are object boundaries roughly correct?\n",
        "- does the model confuse similar categories?\n",
        "- does it mostly predict background?\n",
        "\n",
        "Below, I:\n",
        "1) run inference on a few test images,\n",
        "2) convert predicted class indices to RGB for visualization,\n",
        "3) show input / ground-truth / prediction side-by-side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f7bed9",
      "metadata": {
        "id": "b7f7bed9"
      },
      "outputs": [],
      "source": [
        "def predict_mask(model, img):\n",
        "    # img: (H, W, 3) float32 normalized\n",
        "    pred = model.predict(img[None, ...], verbose=0)[0]  # (H, W, C)\n",
        "    pred_idx = np.argmax(pred, axis=-1).astype(np.uint8)\n",
        "    return pred_idx\n",
        "\n",
        "def denormalize_for_display(x):\n",
        "    # x was (x - 128)/255; undo approximately for display\n",
        "    x_disp = (x * 255.0) + 128.0\n",
        "    x_disp = np.clip(x_disp, 0, 255).astype(np.uint8)\n",
        "    return x_disp\n",
        "\n",
        "def show_prediction_triplet(img, true_mask, pred_mask):\n",
        "    img_disp = denormalize_for_display(img)\n",
        "    true_rgb = VOC_CMAP[true_mask]\n",
        "    pred_rgb = VOC_CMAP[pred_mask]\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img_disp)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Input\", fontsize=11)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(true_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Ground truth\", fontsize=11)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(pred_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Prediction\", fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run on a few test batches\n",
        "for x_b, y_b in test_ds.take(1):\n",
        "    x_np = x_b.numpy()\n",
        "    y_np = y_b.numpy().squeeze(-1).astype(np.uint8)\n",
        "\n",
        "    for i in range(min(3, x_np.shape[0])):\n",
        "        pred = predict_mask(model, x_np[i])\n",
        "        show_prediction_triplet(x_np[i], y_np[i], pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3df1a6e",
      "metadata": {
        "id": "a3df1a6e"
      },
      "source": [
        "## 12) Takeaways\n",
        "\n",
        "- Segmentation requires treating labels as structured spatial data, not simple categories.\n",
        "- A good pipeline matters: resizing, cropping, and flipping must keep the input image and mask aligned.\n",
        "- Nearest-neighbor resizing for masks is non-negotiable, otherwise class labels get corrupted.\n",
        "- DeepLab v3’s ASPP is a practical way to combine local and global context without an encoder-decoder structure.\n",
        "- Masking void pixels makes training and metrics more stable and interpretable on VOC-style datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5b6641",
      "metadata": {
        "id": "df5b6641"
      },
      "source": [
        "## 13) References\n",
        "\n",
        "- Thushan Ganegedara, *TensorFlow in Action* (Chapter 8).\n",
        "- PASCAL VOC 2012 dataset (segmentation benchmark).\n",
        "- DeepLab v3: Chen et al., “Rethinking Atrous Convolution for Semantic Image Segmentation.”\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}