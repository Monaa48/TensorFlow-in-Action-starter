{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monaa48/TensorFlow-in-Action-starter/blob/main/notebooks/Ch05_State_of_the_art_in_deep_learning_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158faf1e",
      "metadata": {
        "id": "158faf1e"
      },
      "source": [
        "# Chapter 05 — State-of-the-art in Deep Learning: Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac89c41",
      "metadata": {
        "id": "eac89c41"
      },
      "source": [
        "## 1) Summary\n",
        "\n",
        "This chapter introduces the **Transformer** model and explains why it became a standard approach for many sequence problems.\n",
        "\n",
        "Key points that I extracted while working through the code:\n",
        "\n",
        "- **Text must be converted into numbers** before it can be used by a neural network. In practice, this usually means:\n",
        "  - mapping tokens to integer IDs,\n",
        "  - padding/truncating sequences to a fixed length for batching,\n",
        "  - using an **embedding layer** to convert IDs to vectors.\n",
        "\n",
        "- A Transformer is usually described as an **encoder–decoder** architecture:\n",
        "  - the **encoder** reads the source sequence and produces a sequence of contextual representations,\n",
        "  - the **decoder** generates the output sequence step by step, attending to both:\n",
        "    - previously generated tokens (masked self-attention),\n",
        "    - encoder outputs (encoder–decoder attention).\n",
        "\n",
        "- The main difference from RNN-based sequence models is the heavy use of **attention**:\n",
        "  - attention lets each token look at other tokens directly,\n",
        "  - the model can process all tokens in parallel (no recurrent hidden state dependency),\n",
        "  - position information is injected through **positional encoding**.\n",
        "\n",
        "- Internally, each encoder/decoder layer is mainly a combination of:\n",
        "  1. **(Masked) Multi-head self-attention**\n",
        "  2. **A position-wise feed-forward network**\n",
        "  3. Residual connections + layer normalization (for stable optimization)\n",
        "\n",
        "In this notebook, I implement the major components in TensorFlow/Keras:\n",
        "- scaled dot-product attention,\n",
        "- look-ahead and padding masks,\n",
        "- multi-head attention,\n",
        "- encoder/decoder layers,\n",
        "- a small Transformer model trained on a small translation-like dataset.\n",
        "\n",
        "The goal is not to obtain high translation quality (that requires more data and compute), but to reproduce the architecture and understand what each block contributes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520cd1cb",
      "metadata": {
        "id": "520cd1cb"
      },
      "source": [
        "## 2) Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d78258a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d78258a6",
        "outputId": "295dcc75-4504-4618-c2df-c9c0a3af28bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939e5efa",
      "metadata": {
        "id": "939e5efa"
      },
      "source": [
        "## 3) Representing text as numbers\n",
        "\n",
        "Neural networks operate on tensors. For text, I need a pipeline that turns sentences into:\n",
        "- integer sequences (token IDs),\n",
        "- padded batches with a consistent length.\n",
        "\n",
        "In this notebook I use `TextVectorization` because it is simple and integrates well with Keras.\n",
        "I build separate vectorizers for source and target text, which is a common practice in translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0d124516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d124516",
        "outputId": "1c37408f-1c3a-485f-ea74-0223daf47e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source vocab size: 38\n",
            "Target vocab size: 35\n",
            "Example source tokens: [[34  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [19  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "Example target-in tokens: [[ 3 25  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3 14  5  0  0  0  0  0  0  0  0  0]]\n",
            "Example target-out tokens: [[25  4  0  0  0  0  0  0  0  0  0  0]\n",
            " [14  5  4  0  0  0  0  0  0  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "def build_vectorizer(texts, vocab_size=8000, seq_len=20):\n",
        "    vec = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=seq_len,\n",
        "        standardize=\"lower_and_strip_punctuation\",\n",
        "        split=\"whitespace\"\n",
        "    )\n",
        "    vec.adapt(texts)\n",
        "    return vec\n",
        "\n",
        "# A small translation-like dataset (French -> English).\n",
        "# This is intentionally small so it runs quickly in Colab.\n",
        "pairs = [\n",
        "    (\"bonjour\", \"hello\"),\n",
        "    (\"merci\", \"thank you\"),\n",
        "    (\"je suis etudiant\", \"i am a student\"),\n",
        "    (\"je suis fatigué\", \"i am tired\"),\n",
        "    (\"où est la gare\", \"where is the station\"),\n",
        "    (\"je veux de l eau\", \"i want water\"),\n",
        "    (\"je veux un café\", \"i want coffee\"),\n",
        "    (\"je ne comprends pas\", \"i do not understand\"),\n",
        "    (\"pouvez vous aider\", \"can you help\"),\n",
        "    (\"bonne nuit\", \"good night\"),\n",
        "    (\"comment ça va\", \"how are you\"),\n",
        "    (\"je vais bien\", \"i am fine\"),\n",
        "    (\"je viens d indonésie\", \"i come from indonesia\"),\n",
        "    (\"j aime apprendre\", \"i like learning\"),\n",
        "    (\"j aime tensorflow\", \"i like tensorflow\"),\n",
        "]\n",
        "\n",
        "src_texts = [s for s, t in pairs]\n",
        "tgt_texts = [t for s, t in pairs]\n",
        "\n",
        "# Add explicit start/end tokens on the target side (common decoder setup)\n",
        "START, END = \"start\", \"end\"\n",
        "tgt_in_texts  = [f\"{START} {t}\" for t in tgt_texts]\n",
        "tgt_out_texts = [f\"{t} {END}\" for t in tgt_texts]\n",
        "\n",
        "SRC_SEQ_LEN = 12\n",
        "TGT_SEQ_LEN = 12\n",
        "VOCAB_SIZE  = 2000\n",
        "\n",
        "src_vec = build_vectorizer(src_texts, vocab_size=VOCAB_SIZE, seq_len=SRC_SEQ_LEN)\n",
        "tgt_vec = build_vectorizer(tgt_in_texts + tgt_out_texts, vocab_size=VOCAB_SIZE, seq_len=TGT_SEQ_LEN)\n",
        "\n",
        "# Peek at vocab\n",
        "print(\"Source vocab size:\", len(src_vec.get_vocabulary()))\n",
        "print(\"Target vocab size:\", len(tgt_vec.get_vocabulary()))\n",
        "print(\"Example source tokens:\", src_vec(src_texts[:2]).numpy())\n",
        "print(\"Example target-in tokens:\", tgt_vec(tgt_in_texts[:2]).numpy())\n",
        "print(\"Example target-out tokens:\", tgt_vec(tgt_out_texts[:2]).numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49320037",
      "metadata": {
        "id": "49320037"
      },
      "source": [
        "### 3.1 Build a `tf.data` pipeline\n",
        "\n",
        "For training a decoder, I need:\n",
        "- encoder input: source token IDs\n",
        "- decoder input: target sequence shifted right (starts with ``start``)\n",
        "- labels: target sequence shifted left (ends with ``end``)\n",
        "\n",
        "This is the standard teacher-forcing setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a6223f27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6223f27",
        "outputId": "472cbe1e-12af-4fbb-f451-d0b4f4a5571e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src batch: (8, 12)\n",
            "tgt_in batch: (8, 12)\n",
            "tgt_out batch: (8, 12)\n"
          ]
        }
      ],
      "source": [
        "def make_dataset(src_texts, tgt_in_texts, tgt_out_texts, batch_size=8, shuffle=True):\n",
        "    src_ids = src_vec(tf.constant(src_texts))\n",
        "    tgt_in_ids = tgt_vec(tf.constant(tgt_in_texts))\n",
        "    tgt_out_ids = tgt_vec(tf.constant(tgt_out_texts))\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((src_ids, tgt_in_ids), tgt_out_ids))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(src_texts), seed=SEED)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(src_texts, tgt_in_texts, tgt_out_texts, batch_size=8, shuffle=True)\n",
        "\n",
        "((src_b, tgt_in_b), tgt_out_b) = next(iter(train_ds))\n",
        "print(\"src batch:\", src_b.shape)\n",
        "print(\"tgt_in batch:\", tgt_in_b.shape)\n",
        "print(\"tgt_out batch:\", tgt_out_b.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cb0297",
      "metadata": {
        "id": "55cb0297"
      },
      "source": [
        "## 4) Transformer overview (encoder–decoder view)\n",
        "\n",
        "A Transformer can be seen as a stack of encoder layers and decoder layers.\n",
        "\n",
        "- Each **encoder layer** uses:\n",
        "  1) multi-head self-attention (tokens attend to all tokens in the source sequence),\n",
        "  2) a feed-forward network applied to each position independently.\n",
        "\n",
        "- Each **decoder layer** uses:\n",
        "  1) masked multi-head self-attention (a token can only attend to earlier tokens),\n",
        "  2) encoder–decoder attention (decoder attends to encoder outputs),\n",
        "  3) a feed-forward network.\n",
        "\n",
        "In practice, the model also needs:\n",
        "- positional encoding (to represent word order),\n",
        "- masking (to ignore padding tokens and to enforce autoregressive decoding).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7889357",
      "metadata": {
        "id": "b7889357"
      },
      "source": [
        "## 5) Scaled dot-product attention\n",
        "\n",
        "The basic attention computation can be written as:\n",
        "\n",
        "\\[\n",
        "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "\\]\n",
        "\n",
        "- `Q` (query), `K` (key), and `V` (value) are learned projections of embeddings.\n",
        "- dividing by \\(\\sqrt{d_k}\\) prevents dot products from growing too large and making softmax too peaky.\n",
        "\n",
        "I implement this function first because it is the core of the Transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bdd837c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdd837c8",
        "outputId": "2f1cd71a-22a3-4f0e-9a5b-a62edf6e44a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: (2, 4, 8) | weights: (2, 4, 5)\n"
          ]
        }
      ],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    # q, k, v: (..., seq_len, depth)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        # mask is broadcastable to (..., seq_len_q, seq_len_k)\n",
        "        scaled += (mask * -1e9)\n",
        "\n",
        "    weights = tf.nn.softmax(scaled, axis=-1)\n",
        "    output = tf.matmul(weights, v)\n",
        "    return output, weights\n",
        "\n",
        "# Toy check: random attention on small tensors\n",
        "q = tf.random.normal([2, 4, 8])  # batch=2, seq=4, depth=8\n",
        "k = tf.random.normal([2, 5, 8])\n",
        "v = tf.random.normal([2, 5, 8])\n",
        "\n",
        "out, w = scaled_dot_product_attention(q, k, v)\n",
        "print(\"output:\", out.shape, \"| weights:\", w.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f684c837",
      "metadata": {
        "id": "f684c837"
      },
      "source": [
        "## 6) Masks: padding mask and look-ahead mask\n",
        "\n",
        "There are two masks commonly used:\n",
        "\n",
        "1. **Padding mask**: ignore padded positions (token ID = 0)  \n",
        "2. **Look-ahead mask**: during decoding, prevent attention to future tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0822ffa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0822ffa",
        "outputId": "bbdd0645-4729-419b-95dd-8a5d637f9721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding mask shape: (2, 1, 1, 4)\n",
            "Look-ahead mask:\n",
            " [[0. 1. 1. 1.]\n",
            " [0. 0. 1. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "def create_padding_mask(seq):\n",
        "    # seq: (batch, seq_len) with 0 as padding\n",
        "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
        "    # shape -> (batch, 1, 1, seq_len) for broadcasting\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    # upper-triangular matrix of 1s (excluding diagonal)\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (size, size)\n",
        "\n",
        "# Example masks\n",
        "example_seq = tf.constant([[7, 6, 0, 0],\n",
        "                           [1, 2, 3, 0]], dtype=tf.int32)\n",
        "\n",
        "pad_mask = create_padding_mask(example_seq)\n",
        "la_mask  = create_look_ahead_mask(4)\n",
        "\n",
        "print(\"Padding mask shape:\", pad_mask.shape)\n",
        "print(\"Look-ahead mask:\\n\", la_mask.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71be058a",
      "metadata": {
        "id": "71be058a"
      },
      "source": [
        "## 7) Positional encoding\n",
        "\n",
        "Attention alone does not encode word order. Transformers inject position information by adding a positional encoding to token embeddings.\n",
        "\n",
        "A standard choice is the sinusoidal encoding used in the original Transformer paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3687acd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3687acd4",
        "outputId": "5c79cd17-b192-4b4c-82b6-643caaac89d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positional encoding: (1, 50, 32)\n"
          ]
        }
      ],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(max_position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(max_position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_enc = angle_rads[np.newaxis, ...].astype(\"float32\")\n",
        "    return tf.constant(pos_enc)\n",
        "\n",
        "# Sanity check\n",
        "pe = positional_encoding(50, 32)\n",
        "print(\"positional encoding:\", pe.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1a6663",
      "metadata": {
        "id": "6f1a6663"
      },
      "source": [
        "## 8) Multi-head attention\n",
        "\n",
        "Instead of computing a single attention output, Transformers use **multiple heads**.\n",
        "Each head learns attention in a different learned subspace.\n",
        "\n",
        "Implementation outline:\n",
        "- project inputs to Q, K, V\n",
        "- split into heads\n",
        "- run scaled dot-product attention per head\n",
        "- concatenate heads and project back\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4f497d63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f497d63",
        "outputId": "a3dfd1e6-fa7c-4654-f551-ffa0bb7a4ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mha output: (2, 10, 32) | weights: (2, 4, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        if d_model % num_heads != 0:\n",
        "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # x: (batch, seq_len, d_model) -> (batch, heads, seq_len, depth)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_dot_product_attention expects (..., seq, depth)\n",
        "        attn_out, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        # back to (batch, seq_len, d_model)\n",
        "        attn_out = tf.transpose(attn_out, perm=[0, 2, 1, 3])\n",
        "        concat = tf.reshape(attn_out, (batch_size, -1, self.d_model))\n",
        "\n",
        "        out = self.dense(concat)\n",
        "        return out, attn_weights\n",
        "\n",
        "# Quick check\n",
        "mha = MultiHeadAttention(d_model=32, num_heads=4)\n",
        "x_tmp = tf.random.normal([2, 10, 32])\n",
        "y_tmp, w_tmp = mha(x_tmp, x_tmp, x_tmp, mask=None)\n",
        "print(\"mha output:\", y_tmp.shape, \"| weights:\", w_tmp.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffeb9f59",
      "metadata": {
        "id": "ffeb9f59"
      },
      "source": [
        "## 9) Feed-forward network (position-wise)\n",
        "\n",
        "Each token position also goes through a small fully connected network:\n",
        "\n",
        "\\[\n",
        "\\mathrm{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "\\]\n",
        "\n",
        "This is applied independently to each time step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df1ff19a",
      "metadata": {
        "id": "df1ff19a"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3694f6d0",
      "metadata": {
        "id": "3694f6d0"
      },
      "source": [
        "## 10) Encoder and decoder layers\n",
        "\n",
        "Each layer uses:\n",
        "- attention + residual + layer norm\n",
        "- feed-forward + residual + layer norm\n",
        "\n",
        "Dropout is included to reduce overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e9dfc142",
      "metadata": {
        "id": "e9dfc142"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        attn_out, _ = self.mha(x, x, x, mask)\n",
        "        attn_out = self.dropout1(attn_out, training=training)\n",
        "        out1 = self.layernorm1(x + attn_out)\n",
        "\n",
        "        ffn_out = self.ffn(out1)\n",
        "        ffn_out = self.dropout2(ffn_out, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_out)\n",
        "        return out2\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # masked self-attn\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # enc-dec attn\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_out, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        # 1) masked self-attention\n",
        "        attn1, attn_weights1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        # 2) encoder-decoder attention (queries from decoder, keys/values from encoder)\n",
        "        attn2, attn_weights2 = self.mha2(enc_out, enc_out, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        # 3) feed-forward\n",
        "        ffn_out = self.ffn(out2)\n",
        "        ffn_out = self.dropout3(ffn_out, training=training)\n",
        "        out3 = self.layernorm3(out2 + ffn_out)\n",
        "\n",
        "        return out3, attn_weights1, attn_weights2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb35d51",
      "metadata": {
        "id": "dcb35d51"
      },
      "source": [
        "## 11) Encoder and decoder stacks\n",
        "\n",
        "The encoder and decoder are stacks of the above layers with:\n",
        "- token embedding,\n",
        "- positional encoding,\n",
        "- dropout.\n",
        "\n",
        "Note: I scale embeddings by \\(\\sqrt{d_{model}}\\), which is commonly used in Transformer implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "efeea648",
      "metadata": {
        "id": "efeea648"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)  # (batch, seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_out, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](\n",
        "                x, enc_out, training=training,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=padding_mask\n",
        "            )\n",
        "            attention_weights[f\"decoder_layer{i+1}_block1\"] = block1\n",
        "            attention_weights[f\"decoder_layer{i+1}_block2\"] = block2\n",
        "\n",
        "        return x, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7716ab2b",
      "metadata": {
        "id": "7716ab2b"
      },
      "source": [
        "## 12) Full Transformer model\n",
        "\n",
        "The final layer projects decoder outputs to vocabulary logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bc60e442",
      "metadata": {
        "id": "bc60e442"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 pe_input, pe_target, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, dropout_rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, dropout_rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # inputs: ((inp, tar_in),)\n",
        "        inp, tar_in = inputs\n",
        "\n",
        "        enc_padding_mask = create_padding_mask(inp)\n",
        "        dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar_in)[1])\n",
        "        dec_target_padding_mask = create_padding_mask(tar_in)\n",
        "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask[tf.newaxis, tf.newaxis, :, :])\n",
        "\n",
        "        enc_out = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "        dec_out, attn = self.decoder(tar_in, enc_out, training=training,\n",
        "                                     look_ahead_mask=combined_mask,\n",
        "                                     padding_mask=dec_padding_mask)\n",
        "\n",
        "        final_logits = self.final_layer(dec_out)\n",
        "        return final_logits, attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caec244",
      "metadata": {
        "id": "2caec244"
      },
      "source": [
        "## 13) Training setup\n",
        "\n",
        "### 13.1 Loss with padding masking\n",
        "\n",
        "When sequences are padded, I should not penalize the model for predicting padding.\n",
        "I compute the cross-entropy per token and mask out padding tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "357d9673",
      "metadata": {
        "id": "357d9673"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # real: (batch, seq_len), pred: (batch, seq_len, vocab)\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(tf.not_equal(real, 0), tf.float32)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c713fd",
      "metadata": {
        "id": "f6c713fd"
      },
      "source": [
        "### 13.2 Optimizer and learning rate schedule\n",
        "\n",
        "A common choice for Transformers is Adam with a warmup schedule.\n",
        "I implement a small learning-rate schedule similar to widely used Transformer examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "20cf25a0",
      "metadata": {
        "id": "20cf25a0"
      },
      "outputs": [],
      "source": [
        "D_MODEL = 64\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 4\n",
        "DFF = 128\n",
        "DROP_RATE = 0.1\n",
        "\n",
        "# For this small dataset, a constant learning rate is enough and easier to debug.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ae4337",
      "metadata": {
        "id": "23ae4337"
      },
      "source": [
        "### 13.3 Create the model and run training\n",
        "\n",
        "Because this dataset is small, training will converge quickly, but it will also overfit easily.\n",
        "I keep epochs low and mainly check that the model learns a sensible mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dd35acbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd35acbd",
        "outputId": "e2bfada0-d4b3-43cf-ae35-31bf5fec6ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training loss: 0.011037083342671394\n",
            "First 5 losses: [3.8343255519866943, 3.261533498764038, 3.0069315433502197, 2.955033302307129, 2.861940622329712]\n",
            "Last  5 losses: [0.02810092829167843, 0.010155181400477886, 0.011375500820577145, 0.011499704793095589, 0.011037083342671394]\n"
          ]
        }
      ],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dff=DFF,\n",
        "    input_vocab_size=len(src_vec.get_vocabulary()),\n",
        "    target_vocab_size=len(tgt_vec.get_vocabulary()),\n",
        "    pe_input=SRC_SEQ_LEN,\n",
        "    pe_target=TGT_SEQ_LEN,\n",
        "    dropout_rate=DROP_RATE\n",
        ")\n",
        "\n",
        "# Keras compile expects a tensor output; the Transformer returns (logits, attention).\n",
        "# This wrapper exposes logits only.\n",
        "class TransformerLogits(tf.keras.Model):\n",
        "    def __init__(self, transformer):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        logits, _ = self.transformer(inputs, training=training)\n",
        "        return logits\n",
        "\n",
        "logit_model = TransformerLogits(transformer)\n",
        "logit_model.compile(optimizer=optimizer, loss=loss_function)\n",
        "\n",
        "EPOCHS = 200\n",
        "history = logit_model.fit(train_ds, epochs=EPOCHS, verbose=0)\n",
        "\n",
        "# Print a quick training signal\n",
        "print(\"Final training loss:\", float(history.history[\"loss\"][-1]))\n",
        "print(\"First 5 losses:\", [float(x) for x in history.history[\"loss\"][:5]])\n",
        "print(\"Last  5 losses:\", [float(x) for x in history.history[\"loss\"][-5:]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b076a1",
      "metadata": {
        "id": "81b076a1"
      },
      "source": [
        "## 14) Simple decoding (greedy)\n",
        "\n",
        "To translate a sentence, I:\n",
        "1. encode the source\n",
        "2. start the target with ``start``\n",
        "3. iteratively predict the next token and append it\n",
        "4. stop if ``end`` is generated or max length is reached\n",
        "\n",
        "This is greedy decoding (no beam search).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1d48d5e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d48d5e6",
        "outputId": "27985907-b86c-453c-a445-8a04f7323c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start token: start | id: 3\n",
            "end token  : end | id: 4\n",
            "bonjour                   -> hello\n",
            "merci                     -> thank you\n",
            "je suis etudiant          -> i am a student\n",
            "je ne comprends pas       -> i do not understand\n",
            "où est la gare            -> where is the station\n"
          ]
        }
      ],
      "source": [
        "# Helper maps for decoding\n",
        "tgt_vocab = tgt_vec.get_vocabulary()\n",
        "tgt_id_to_token = {i: tok for i, tok in enumerate(tgt_vocab)}\n",
        "tgt_token_to_id = {tok: i for i, tok in enumerate(tgt_vocab)}\n",
        "\n",
        "start_id = tgt_token_to_id.get(START)\n",
        "end_id = tgt_token_to_id.get(END)\n",
        "\n",
        "print(\"start token:\", START, \"| id:\", start_id)\n",
        "print(\"end token  :\", END,   \"| id:\", end_id)\n",
        "\n",
        "def greedy_translate(src_sentence, max_len=TGT_SEQ_LEN):\n",
        "    # Vectorize source\n",
        "    src_ids = src_vec(tf.constant([src_sentence]))\n",
        "\n",
        "    if start_id is None or end_id is None:\n",
        "        return \"(start/end token not in vocabulary)\"\n",
        "\n",
        "    # Start with the start token id\n",
        "    out_ids = [start_id]\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        tar_in = tf.constant([out_ids], dtype=tf.int64)  # variable-length decoding\n",
        "        logits, _ = transformer((src_ids, tar_in), training=False)\n",
        "\n",
        "        # Next-token distribution is at the last position\n",
        "        next_id = int(tf.argmax(logits[0, -1]).numpy())\n",
        "        out_ids.append(next_id)\n",
        "\n",
        "        if next_id == end_id:\n",
        "            break\n",
        "\n",
        "    # Convert ids to tokens; drop start/end and padding\n",
        "    tokens = []\n",
        "    for i in out_ids:\n",
        "        if i in (0, start_id, end_id):\n",
        "            continue\n",
        "        tok = tgt_id_to_token.get(i, \"\")\n",
        "        if tok:\n",
        "            tokens.append(tok)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "tests = [\n",
        "    \"bonjour\",\n",
        "    \"merci\",\n",
        "    \"je suis etudiant\",\n",
        "    \"je ne comprends pas\",\n",
        "    \"où est la gare\",\n",
        "]\n",
        "\n",
        "for s in tests:\n",
        "    print(f\"{s:25s} -> {greedy_translate(s)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc41a3f",
      "metadata": {
        "id": "cdc41a3f"
      },
      "source": [
        "## 15) Takeaways\n",
        "\n",
        "- The Transformer replaces recurrence with attention, which enables parallel processing of tokens.\n",
        "- The two masks are essential: padding mask ignores padding; look-ahead mask enforces autoregressive decoding.\n",
        "- Multi-head attention is built from the same attention function repeated across several heads, then concatenated.\n",
        "- Even with a small dataset, the architecture can be assembled end-to-end in Keras once attention and masking are implemented carefully.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}